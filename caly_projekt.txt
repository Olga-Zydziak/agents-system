--- FILE: autogen_orchestrator.py ---

"""
Pełny orchestrator MOA używający AutoGen do zarządzania debatą agentów
"""
import json
from datetime import datetime
import config_api
from autogen import UserProxyAgent, ConversableAgent, GroupChat, GroupChatManager
from typing import Dict, List, Any, Optional
from datetime import datetime
import autogen
from google.cloud import secretmanager
from models_config import AgentRole
from moa_prompts import MOAPrompts
from memory_system import ContextMemory
# Używamy structured parsera zamiast heurystycznego response_parser
from structured_response_parser import StructuredResponseParser
from process_logger import log as process_log
import os, json, time, traceback
from typing import Any, Dict, Optional
from process_logger import log as process_log
import vertexai
from process_logger import log_exception as log_exc
from config_api import basic_config_agent
from exporter_missions_gcs import export_local_by_filename_date
from explainability_layer import ExplainabilityHooks
from memory_helpers import save_mission_to_gcs
EXPLAINABILITY = ExplainabilityHooks()

class AutoGenMOAOrchestrator:
    """
    Orchestrator systemu MOA używający AutoGen do wieloturowej debaty
    """
    
    def __init__(self, mission: str, node_library: Dict[str, Any], config_file: str = "agents_config.json"):
        self.mission = mission
        self.node_library = node_library
        self.memory = ContextMemory(
        max_episodes=50,
        gcs_bucket=None,   # ten sam bucket co w Memory_bank.ipynb
        gcs_prefix=""                   # opcjonalnie; usuń lub zostaw ""
        )
        # Parser oparty na Pydantic – oczekuje czystego JSON zgodnego ze schematem
        self.parser = StructuredResponseParser()
        
        # Wczytaj konfigurację
        self._load_config(config_file)
        
        # Stan debaty
        self.iteration_count = 0
        self.max_iterations = 5
        self.current_context = {}
        self.final_plan = None
        self._forced_speaker: Optional[str] = None
        # Inicjalizuj agentów AutoGen
        self.enable_sanity_ping = False
        process_log(f"[CFG] enable_sanity_ping={self.enable_sanity_ping}")
        self._initialize_autogen_agents()
        self._secret_cache = {}
        
        process_log(f"=== AutoGen MOA Orchestrator initialized for mission: {mission[:100]}... ===")
    
    
    #nowe helpery
    
    @staticmethod
    def _valid_memory_json(js: dict) -> bool:
        if not isinstance(js, dict):
            return False
        required = ("recommended_strategies", "common_pitfalls", "examples", "notes")
        if any(k not in js for k in required):
            return False
        return (
            isinstance(js["recommended_strategies"], list) and
            isinstance(js["common_pitfalls"], list) and
            isinstance(js["examples"], list) and
            isinstance(js["notes"], str)
        )

    
    def _extract_name_and_len(self, msg):
        """
        Zwraca (name, content_len, type_name) z wiadomości w wielu możliwych formatach:
        - dict z kluczami name/content/sender/role
        - list/tuple w stylu [role, name, content] lub [name, content]
        - obiekt z atrybutami .name/.content
        - plain string
        """
        try:
            tname = type(msg).__name__
            # 1) dict
            if isinstance(msg, dict):
                name = msg.get("name") or msg.get("sender") or msg.get("role") or None
                content = msg.get("content") or msg.get("text") or ""
                return name, len(content or ""), tname

            # 2) list/tuple – najczęstsze warianty
            if isinstance(msg, (list, tuple)):
                if len(msg) >= 3:
                    # [role, name, content]
                    name = str(msg[1])
                    content = str(msg[2])
                elif len(msg) == 2:
                    # [name, content] lub [role, content]
                    name = str(msg[0])
                    content = str(msg[1])
                elif len(msg) == 1:
                    name = None
                    content = str(msg[0])
                else:
                    name, content = None, ""
                return name, len(content or ""), tname

            # 3) obiekt z atrybutami
            if hasattr(msg, "name") or hasattr(msg, "content"):
                name = getattr(msg, "name", None)
                content = getattr(msg, "content", "") or ""
                return name, len(content or ""), tname

            # 4) fallback: traktuj jako string
            s = str(msg)
            return None, len(s), tname

        except Exception:
            # ostateczny fallback
            return None, 0, "unknown"

        
        
        
    #liczenie uzycia pamieci
    
    def _log_memory_alignment(self, plan_text: str, phase: str):
        """
        Miękka telemetria: sprawdza, na ile 'plan_text' (JSON/tekst Aggregatora)
        pokrywa się z seedowaną pamięcią (self._last_seeded_memory).
        Nie modyfikuje promptów ani toku debaty – tylko loguje.
        """
        import json, re

        mem = getattr(self, "_last_seeded_memory", None)
        if not mem or not plan_text:
            process_log(f"[MEMORY ALIGNMENT] skipped (mem_or_plan_missing) phase={phase}")
            return

        # --- 1) wyciągnij czysty tekst planu (jeśli to JSON w stringu)
        txt = plan_text
        # spróbuj znaleźć największy blok JSON (ostrożny regex)
        try:
            match = re.search(r"\{[\s\S]*\}", plan_text)
            if match:
                txt = match.group(0)
        except Exception:
            pass

        # --- 2) zbuduj 'work_text' do prostych dopasowań (lowercase)
        work_text = txt.lower()

        # --- 3) źródło pamięci
        strategies = [s.strip().lower() for s in (mem.get("recommended_strategies") or []) if s and isinstance(s, str)]
        pitfalls   = [p.strip().lower() for p in (mem.get("common_pitfalls")       or []) if p and isinstance(p, str)]
        examples   = mem.get("examples") or []

        # --- 4) proste dopasowania substring (bez magii – chodzi o telemetrię, nie scoring naukowy)
        def _hitcount(items, text):
            hits = []
            for it in items:
                if len(it) >= 5 and it in text:  # mini-próg, żeby „csv” itp. nie łapać
                    hits.append(it)
            return hits

        used_strats   = _hitcount(strategies, work_text)
        addressed_pts = _hitcount(pitfalls,   work_text)

        # --- 5) examples: sprawdzimy, czy wystąpiły ID lub URI w planie
        ex_total = len(examples)
        ex_hits = 0
        for ex in examples:
            mid = str(ex.get("mission_id","")).strip().lower()
            uri = str(ex.get("plan_uri","")).strip().lower()
            if (mid and mid in work_text) or (uri and uri in work_text):
                ex_hits += 1

        # --- 6) prosty score 0..1 (wagi możesz zmienić)
        import math
        def frac(num, den): 
            return (num/den) if den else 0.0

        s_frac = frac(len(used_strats),   len(strategies))
        p_frac = frac(len(addressed_pts), len(pitfalls))
        e_frac = frac(ex_hits,            ex_total)

        score = round(0.5 * s_frac + 0.3 * p_frac + 0.2 * e_frac, 3)

        # --- 7) log + opcjonalna integracja z EXPLAINABILITY (jeśli jest)
        process_log(
            "[MEMORY ALIGNMENT] phase=%s score=%.3f "
            "strategies_used=%d/%d pitfalls_addressed=%d/%d examples_ref=%d/%d" % (
                phase, score,
                len(used_strats), len(strategies),
                len(addressed_pts), len(pitfalls),
                ex_hits, ex_total
            )
        )
        try:
            if "EXPLAINABILITY" in globals() and hasattr(EXPLAINABILITY, "on_memory_alignment"):
                EXPLAINABILITY.on_memory_alignment({
                    "phase": phase, "score": score,
                    "strategies_used": used_strats,
                    "pitfalls_addressed": addressed_pts,
                    "examples_hits": ex_hits,
                    "examples_total": ex_total
                })
        except Exception as e:
            process_log(f"[MEMORY ALIGNMENT][WARN] explainability hook failed: {type(e).__name__}: {e}")

        # --- 8) (opcjonalnie) krótki, niewpływający podgląd w transkrypcie
        try:
            if True:
            # if getattr(self.config, "show_memory_alignment_preview", False):
                preview = {
                    "phase": phase, "score": score,
                    "strategies_used": len(used_strats), "strategies_total": len(strategies),
                    "pitfalls_addressed": len(addressed_pts), "pitfalls_total": len(pitfalls),
                    "examples_ref": ex_hits, "examples_total": ex_total
                }
                # zapisz do historii – jako wiadomość Orchestratora (nie zmienia mówcy)
                self.groupchat.messages.append({
                    "role": "assistant", "name": "Orchestrator",
                    "content": f"[MEMORY ALIGNMENT PREVIEW] {preview}"
                })
        except Exception as e:
            process_log(f"[MEMORY ALIGNMENT][DEBUG] preview append failed: {type(e).__name__}: {e}")
    
    
    #koniec liczenia
    
    
    def _make_memory_message_once(self):
        """LIGHT -> walidacja -> MID fallback -> lokalny fallback (bez LLM). Zwraca pojedynczą wiadomość do historii czatu."""
        user_msg = {"role":"user","content": f"mission={self.mission}"}

        # 1) LIGHT (PRIMARY)
        if self.memory_analyst_agent:
            try:
                out = self.memory_analyst_agent.generate_reply(messages=[user_msg])
                try: js = json.loads(out)
                except Exception: js = None
                if self._valid_memory_json(js) and (len(js["recommended_strategies"])>=2 or len(js["examples"])>=1):
                    self._last_seeded_memory = js
                    process_log("[MEMORY][LIGHT] Valid memory JSON produced")
                    return {"name":"Memory_Analyst","role":"assistant","content": out}
            except Exception as e:
                log_exc(f"[MEMORY] LIGHT failed:", e)

            # 2) MID (FALLBACK)
            if self.memory_analyst_fallback_model:
                try:
                    mid_llm = self._build_llm_config(self.memory_analyst_fallback_model)
                    tmp = autogen.ConversableAgent(
                        name="Memory_Analyst_MID",
                        llm_config=mid_llm,
                        system_message=MOAPrompts.get_memory_analyst_prompt(),
                        human_input_mode="NEVER"
                    )
                    out = tmp.generate_reply(messages=[user_msg])
                    try: js = json.loads(out)
                    except Exception: js = None
                    if self._valid_memory_json(js):
                        self._last_seeded_memory = js
                        process_log("[MEMORY][MID] Valid memory JSON produced")
                        return {"name":"Memory_Analyst","role":"assistant","content": out}
                except Exception as e:
                    log_exc(f"[MEMORY] MID failed:", e)

        # 3) Fallback bez LLM — lokalny + (opcjonalnie) Vertex
        try:
            ctx_local = self.memory.get_relevant_context(self.mission) if self.memory else {}
            get_v = getattr(self.memory, "get_vertex_context", None)
            ctx_vertex = get_v(self.mission) if callable(get_v) else {}
            tips = list(dict.fromkeys(
                (ctx_local.get("recommended_strategies") or []) +
                (ctx_vertex.get("recommended_strategies") or [])
            ))[:6]
            examples = (ctx_vertex.get("examples") or [])[:3]
            minimal = {
                "recommended_strategies": tips,
                "common_pitfalls": ctx_local.get("common_pitfalls") or [],
                "examples": [{"mission_id": e.get("mission_id",""), "plan_uri": e.get("plan_uri","")} for e in examples],
                "notes": ""
            }
            self._last_seeded_memory = minimal
            return {"name":"Memory_Analyst","role":"assistant","content": json.dumps(minimal, ensure_ascii=False)}
        except Exception as e:
            log_exc(f"[MEMORY] local fallback failed:",e)
            return {"name":"Memory_Analyst","role":"assistant","content": '{"recommended_strategies":[],"common_pitfalls":[],"examples":[],"notes":""}'}
    
    #koniec nowych helperow
    
    
    
    def reset(self):
        # ... resetuje liczniki ...
        # Używa wbudowanej metody .reset() do wyczyszczenia historii każdego agenta
        all_agents = [self.user_proxy, *self.proposer_agents, self.aggregator_agent, self.critic_agent]
        for agent in all_agents:
            if agent:
                agent.reset()
    
    def _get_api_key_from_gcp_secret_manager(self, model_cfg: Dict) -> str | None:
        """
        Czyta klucz z GCP Secret Manager.
        Oczekuje: model_cfg["secret_manager"] = {"project_id": "...", "secret_id": "...", "version": "latest"|"1"|...}
        Zwraca: string lub None (gdy brak/nieudane).
        """
        sm = model_cfg.get("secret_manager") or {}
        project_id = (sm.get("project_id") or "").strip()
        secret_id  = (sm.get("secret_id") or "").strip()
        version    = (sm.get("version") or "latest").strip()

        if not project_id or not secret_id:
            return None

        cache_key = (project_id, secret_id, version)
        if cache_key in self._secret_cache:
            return self._secret_cache[cache_key]

        try:
            client = secretmanager.SecretManagerServiceClient()
            name = f"projects/{project_id}/secrets/{secret_id}/versions/{version}"
            resp = client.access_secret_version(name=name)
            value = resp.payload.data.decode("utf-8")
            # cache in-memory (nie logujemy!)
            self._secret_cache[cache_key] = value
            return value
        except Exception as e:
            # Nie loguj wartości sekretu. Możesz zalogować TYLKO metadane.
            from process_logger import log as process_log
            process_log(f"[SECRETS] Failed to read {secret_id}@{project_id}/{version}: {type(e).__name__}: {e}")
            return None
    
    
    #pamiec:
    
    def _build_memory_analyst_message(self) -> str:
        ctx_local = self.memory.get_relevant_context(self.mission) if self.memory else {}
        get_v = getattr(self.memory, "get_vertex_context", None)
        ctx_vertex = get_v(self.mission) if callable(get_v) else {}


        tips = []
        tips += ctx_local.get("recommended_strategies", []) or []
        tips += ctx_vertex.get("recommended_strategies", []) or []


        seen, dedup = set(), []
        for t in tips:
            if t not in seen:
                seen.add(t)
                dedup.append(t)
        tips = dedup[:8]


        examples = (ctx_vertex.get("examples") or [])[:3]
        lines = ["# MemoryAnalyst Summary"]
        if tips:
            lines.append("## Recommended strategies:")
            lines += [f"- {t}" for t in tips]
        if examples:
            lines.append("\n## Example plans:")
            for e in examples:
                mid = e.get("mission_id") or "unknown"
                puri = e.get("plan_uri") or ""
                lines.append(f"- {mid}: {puri}")


        return "\n".join(lines) if len(lines) > 1 else "# MemoryAnalyst Summary\n(no relevant memory found)"
    
    
    #Raport:
    def _ensure_dir(self, path: str):
        os.makedirs(path, exist_ok=True)

    def _now_stamp(self) -> str:
        return time.strftime("%Y%m%d_%H%M%S", time.localtime())

    def _extract_llm_hint(self, text: str) -> Optional[str]:
        """Prosta heurystyka do rozpoznawania typowych problemów LLM-a."""
        if not text:
            return None
        
        #poprawka:
        if not isinstance(text, str):
            try:
                import json as _json
                text = _json.dumps(text, ensure_ascii=False)
            except Exception:
                text = str(text)
        
        
        #koniec_poprawki
        
        
        
        t = text.lower()
        hints = {
            "quota/rate_limit": ["rate limit", "too many requests", "quota", "insufficient_quota"],
            "context_length": ["maximum context length", "token limit", "context window", "too many tokens"],
            "safety": ["safety", "blocked", "content filter"],
            "auth/api": ["invalid api key", "unauthorized", "forbidden", "permission"],
            "timeout": ["timeout", "timed out", "deadline exceeded"]
        }
        for label, kws in hints.items():
            if any(k in t for k in kws):
                return label
        return None

    def _write_failure_report(
        self,
        reason: str,
        stage: str,
        aggregator_raw: Optional[str],
        critic_raw: Optional[str],
        exception: Optional[BaseException] = None,
        parsed_aggregator: Optional[Dict[str, Any]] = None
    ) -> str:
        
        
        #poprawka:
        def _safe_str(x):
            if x is None:
                return ""
            if isinstance(x, str):
                return x
            try:
                import json as _json
                return _json.dumps(x, ensure_ascii=False)
            except Exception:
                return str(x)
        
        
        
        #koniec poprawki:
        
        """Zapisuje raport awaryjny JSON + MD i zwraca ścieżkę do pliku JSON."""
        self._ensure_dir("reports")
        ts = self._now_stamp()
        jpath = f"reports/failure_report_{ts}.json"
        mpath = f"reports/failure_report_{ts}.md"

        agg_hint = self._extract_llm_hint(aggregator_raw or "")
        crit_hint = self._extract_llm_hint(critic_raw or "")

        report = {
            "timestamp": ts,
            "mission": self.mission,
            "stage": stage,  # np. "aggregator", "groupchat", "critic"
            "reason": reason,  # np. "AGGREGATOR_NO_VALID_JSON", "EXCEPTION_DURING_DEBATE"
            "aggregator_model": getattr(self, "aggregator_config", {}).get("model", {}),
            "critic_model": getattr(self, "critic_config", {}).get("model", {}),
            "aggregator_output_excerpt": _safe_str(aggregator_raw or "")[:4000],
            "critic_output_excerpt": _safe_str(critic_raw or "")[:4000],
            "aggregator_llm_hint": agg_hint,
            "critic_llm_hint": crit_hint,
            "parsed_aggregator": parsed_aggregator,
            "exception": None if not exception else {
                "type": type(exception).__name__,
                "message": str(exception),
                "traceback": traceback.format_exc()
            }
        }

        with open(jpath, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # krótkie MD dla ludzi
        with open(mpath, "w", encoding="utf-8") as f:
            f.write(f"# Failure Report ({ts})\n\n")
            f.write(f"**Mission:** {self.mission}\n\n")
            f.write(f"**Stage:** {stage}\n\n")
            f.write(f"**Reason:** {reason}\n\n")
            if agg_hint:
                f.write(f"**Aggregator LLM hint:** `{agg_hint}`\n\n")
            if crit_hint:
                f.write(f"**Critic LLM hint:** `{crit_hint}`\n\n")
            if exception:
                f.write(f"**Exception:** `{type(exception).__name__}: {exception}`\n\n")
            f.write("## Last Aggregator Output (excerpt)\n\n")
            f.write("```\n" +  _safe_str(aggregator_raw or "")[:4000] + "\n```\n\n")
            f.write("## Last Critic Output (excerpt)\n\n")
            f.write("```\n" +  _safe_str(critic_raw or "")[:4000] + "\n```\n")

        
        process_log(f"[FAILSAFE] Saved failure report: {jpath}")

        return jpath

    def _get_last_message_from(self, groupchat, agent_name: str) -> Optional[str]:
        """Zwraca tekst ostatniej wiadomości danego agenta z obiektu GroupChat."""
        try:
            msgs = getattr(groupchat, "messages", [])
            for m in reversed(msgs):
                if (m.get("name") or m.get("role")) == agent_name:
                    return m.get("content") or ""
        except Exception:
            pass
        return None
    
    
    # ========== UNIVERSAL JSON REPAIR ==========

    MAX_REPAIR_ATTEMPTS = 2
    REPAIR_JSON_SUFFIX = "\n\nZWRÓĆ TYLKO I WYŁĄCZNIE JSON, bez komentarzy, bez dodatkowego tekstu."

    def _schema_example_for(self, role: str) -> str:
        if role == "proposer":
            return (
                '{\n'
                '  "thought_process": ["Krok 1...", "Krok 2..."],\n'
                '  "plan": {\n'
                '    "entry_point": "Start_Node",\n'
                '    "nodes": [ {"name":"Start_Node","implementation":"load_data"} ],\n'
                '    "edges": [ {"from":"Start_Node","to":"Next_Node","condition":"on_success"} ]\n'
                '  },\n'
                '  "confidence": 0.80\n'
                '}'
            )
        if role == "aggregator":
            return (
                '{\n'
                '  "thought_process": ["Agreguję elementy X i Y..."],\n'
                '  "final_plan": {\n'
                '    "entry_point": "Start_Node",\n'
                '    "nodes": [ {"name":"Start_Node","implementation":"load_data"} ],\n'
                '    "edges": [ {"from":"Start_Node","to":"Next_Node","condition":"on_success"} ]\n'
                '  },\n'
                '  "confidence_score": 0.90\n'
                '}'
            )
        if role == "critic":
            return (
                '{\n'
                '  "critique_summary": {\n'
                '    "verdict": "ZATWIERDZONY",\n'
                '    "statement": "Uzasadnienie...",\n'
                '    "key_strengths": ["..."],\n'
                '    "identified_weaknesses": [{"weakness":"...", "severity":"Low", "description":"..."}]\n'
                '  },\n'
                '  "quality_metrics": {\n'
                '    "Complexity_Score_C": 3.1,\n'
                '    "Robustness_Score_R": 50,\n'
                '    "Innovation_Score_I": 100,\n'
                '    "Completeness_Score": 100,\n'
                '    "Overall_Quality_Q": 84.07\n'
                '  },\n'
                '  "final_synthesized_plan": {\n'
                '    "entry_point": "Start_Node",\n'
                '    "nodes": [ {"name":"Start_Node","implementation":"load_data"} ],\n'
                '    "edges": [ {"from":"Start_Node","to":"Next_Node","condition":"on_success"} ]\n'
                '  }\n'
                '}'
            )
        return "{}"

    def _try_parse_by_role(self, role: str, text: str):
        try:
            if role == "proposer":
                parsed = self.parser.parse_agent_response(text)
            elif role == "aggregator":
                parsed = self.parser.parse_aggregator_response(text)
            elif role == "critic":
                parsed = self.parser.parse_critic_response(text)
            else:
                return None, f"Unknown role: {role}"
            if parsed:
                return parsed, None
            return None, "Parser returned None"
        except Exception as e:
            return None, f"{type(e).__name__}: {e}"

    def _repair_prompt_for(self, role: str, err_msg: str) -> str:
        return (
            f"Twoja poprzednia odpowiedź NIE SPEŁNIA wymaganego schematu dla roli '{role}'.\n"
            f"Błąd/diagnoza parsera: {err_msg}\n\n"
            f"Wymagana struktura JSON (minimalny przykład):\n{self._schema_example_for(role)}\n"
            f"{REPAIR_JSON_SUFFIX}"
        )

    def _force_one_turn(self, agent, manager) -> str:
        self._forced_speaker = agent.name
        try:
            manager.step()  # jeśli Twoja wersja AG2 nie wspiera .step(), użyj run(max_round=1)
        except Exception:
            pass
        return self._get_last_message_from(manager.groupchat, agent.name) or ""

    def _auto_repair_and_parse(self, role: str, agent, manager, last_text: str):
        parsed, err = self._try_parse_by_role(role, last_text or "")
        if parsed:
            return parsed
        
        for attempt in range(1, MAX_REPAIR_ATTEMPTS + 1):
            repair_msg = self._repair_prompt_for(role, err or "Invalid JSON")
            manager.groupchat.messages.append({
                "role": "user",
                "name": "Orchestrator",
                "content": repair_msg
            })
            process_log(f"[REPAIR][{role}] attempt {attempt}: requesting strictly JSON output.")
            repaired_text = self._force_one_turn(agent, manager)
            parsed, err2 = self._try_parse_by_role(role, repaired_text or "")
            if parsed:
                return parsed
            err = err2
        return None
    
    
    
    def _load_config(self, config_file: str):
        """Wczytuje konfigurację agentów"""
        with open(config_file, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
    
    
    def _is_final_plan_message(self, m: dict) -> bool:
        """Kończymy TYLKO na odpowiedzi CRITICA, gdy kończy się markerem."""
        content = (m.get("content") or "").strip()
        name = (m.get("name") or "").lower()
        role = (m.get("role") or "").lower()
        return role == "assistant" and content.endswith("PLAN_ZATWIERDZONY") and "critic" in name
    
    

    def custom_speaker_selection_logic(self, last_speaker, groupchat):
        """
        Proposers → Aggregator → Critic. Jeśli Critic nie zatwierdzi, nowa iteracja od pierwszego Proposera.
        Porównujemy po NAZWACH z historii wiadomości (AutoGen może podawać inne instancje agentów).
        """
        msgs = groupchat.messages
        
        for msg in msgs:
            if "PLAN_ZATWIERDZONY" in msg.get("content", ""):
                raise StopIteration("Plan zatwierdzony - kończymy debatę")
        
        last_name = (msgs[-1].get("name") or "").lower() if msgs else ""
        last_content = (msgs[-1].get("content") or "")

        if last_name and last_content:
            # Znajdź prompt który wywołał tę odpowiedź
            # To będzie przedostatnia wiadomość lub początkowy bootstrap
            prompt_for_last = ""
            if len(msgs) >= 2:
                prompt_for_last = msgs[-2].get("content", "")
            elif len(msgs) == 1:
                # Pierwsza odpowiedź - prompt to bootstrap z run_full_debate_cycle
                prompt_for_last = getattr(self, '_initial_bootstrap', '')

            # Zapisz wyjaśnialność
            EXPLAINABILITY.on_response_received(
                prompt=prompt_for_last,
                response=last_content,
                agent_name=last_name
            )
        
        
        
        # --- [DODANE] Memory Analyst ma tylko jedną wypowiedź na starcie ---
        ma_name = (getattr(self, "memory_analyst_agent", None).name or "").lower() \
                  if getattr(self, "memory_analyst_agent", None) else ""
        if last_name == ma_name:
            # Po jednorazowym komunikacie pamięci przechodzimy do pierwszego Proposera
            return self.proposer_agents[0]

        # ❶ Po bootstrapie (ostatni był Orchestrator → wybieramy pierwszego proposera)
        if last_name == (self.user_proxy.name or "").lower() and last_content.strip():
            return self.proposer_agents[0]

        # ❷ Po Aggregatorze → czas na Critica
        if last_name == (self.aggregator_agent.name or "").lower():
            try:
                # zapamiętaj ostatni tekst planu do późniejszego użytku (np. post-approval)
                self._last_aggregated_plan_text = last_content
                # policz adhezję tuż po agregacji
                self._log_memory_alignment(last_content, phase="post_aggregation")
            except Exception as e:
                process_log(f"[MEMORY ALIGNMENT][WARN] post_aggregation failed: {type(e).__name__}: {e}")
            return self.critic_agent

        # ❸ Po Criticu → zatwierdzenie albo nowa iteracja
        if last_name == (self.critic_agent.name or "").lower():
            self._save_iteration_to_memory(last_content, self.iteration_count)
            if "PLAN_ZATWIERDZONY" in last_content:
                try:
                    plan_text = getattr(self, "_last_aggregated_plan_text", "") or last_content
                    self._log_memory_alignment(plan_text, phase="post_approval")
                except Exception as e:
                    process_log(f"[MEMORY ALIGNMENT][WARN] post_approval failed: {type(e).__name__}: {e}")
                return None
                
                
                
                
                # return None
            # nowa iteracja
            self.iteration_count += 1
            if self.iteration_count >= self.max_iterations:
                process_log(f"[FAILSAFE] Osiągnięto maksymalną liczbę iteracji ({self.max_iterations}). Koniec debaty.")
                return None
            process_log(f"===== ROZPOCZYNAM ITERACJĘ DEBATY NR {self.iteration_count + 1} =====")
            self._update_context_from_last_critique(last_content)
            return self.proposer_agents[0]

        # ❹ Wewnątrz puli proposerów – leć kolejno po nazwach
        proposer_names = [p.name.lower() for p in self.proposer_agents]
        if last_name in proposer_names:
            idx = proposer_names.index(last_name)
            if idx < len(self.proposer_agents) - 1:
                return self.proposer_agents[idx + 1]
            return self.aggregator_agent  # po ostatnim proposerze mówi Aggregator

        # ❺ Domyślnie – zacznij od pierwszego proposera
        return self.proposer_agents[0]
    
    
    def _save_iteration_to_memory(self, critic_response: str, iteration: int):
        """Zapisuje dane z iteracji do pamięci"""
        try:
            
            if not self.memory:
                return
            
            # Parse odpowiedzi krytyka
            parsed = self.parser.parse_critic_response(critic_response)
            if not parsed:
                process_log(f"[MEMORY] Nie mogę sparsować odpowiedzi krytyka w iteracji {iteration}")
                return

            # Wyciągnij kluczowe dane
            score = parsed.get("quality_metrics", {}).get("Overall_Quality_Q", 0)
            weaknesses = parsed.get("critique_summary", {}).get("identified_weaknesses", [])
            verdict = parsed.get("critique_summary", {}).get("verdict", "")

            # Stwórz feedback string
            feedback_data = {
                "score": score,
                "verdict": verdict,
                "weaknesses": [w.get("weakness", "") for w in weaknesses if isinstance(w, dict)],
                "iteration": iteration
            }

            # ZAPISZ DO PAMIĘCI
            self.memory.add_iteration_feedback(
                iteration=iteration,
                feedback=json.dumps(feedback_data),
                timestamp=datetime.now()
            )

            process_log(f"[MEMORY] Zapisano iterację {iteration}: score={score}, verdict={verdict}")

        except Exception as e:
            process_log(f"[MEMORY ERROR] Błąd zapisu iteracji {iteration}: {e}")

    
    def _initialize_autogen_agents(self):
        """Inicjalizuje agentów AutoGen dla debaty — minimalistycznie i niezawodnie."""
        # Atrybuty ZAWSZE istnieją
        self.proposer_agents = []
        self.aggregator = None
        self.critic = None
        self.aggregator_agent = None
        self.critic_agent = None
        self.memory_analyst_agent = None
        self.memory_analyst_fallback_model = None
        
        
        # User Proxy – nigdy nie kończy rozmowy
        self.user_proxy = autogen.ConversableAgent( # <-- POPRAWKA
        name="Orchestrator",
        human_input_mode="NEVER",
        llm_config=False,  # Ten agent nie potrzebuje LLM, tylko rozpoczyna rozmowę
        system_message="You are the orchestrator who starts the debate and then observes."
        )
        self.user_proxy.silent = False
        process_log("[INIT] UserProxy initialized")

        # Proposerzy
        for agent_config in self.config['agents']:
            rn = agent_config['role_name'].lower()
            if 'aggregator' in rn or 'critic' in rn or 'memory analyst' in rn:
                continue
            role = AgentRole(
                role_name=agent_config['role_name'],
                expertise_areas=agent_config['expertise_areas'],
                thinking_style=agent_config['thinking_style']
            )
            prompt = self._build_proposer_prompt(role)
            ag = autogen.ConversableAgent(
                name=agent_config['role_name'].replace(" ", "_"),
                llm_config=self._build_llm_config(agent_config['model']),
                system_message=prompt,
                human_input_mode="NEVER"
            )
            ag.silent = False
            self.proposer_agents.append(ag)
            process_log(f"[INIT] Proposer initialized: {ag.name}")

        #memory agent
        
        # --- Memory Analyst (lekki agent, mówi tylko raz na starcie; prompt z MOAPrompts) ---
        mem_cfg = next((a for a in self.config['agents'] if 'memory analyst' in a['role_name'].lower()), None)
        if mem_cfg:
            try:
                mem_llm = self._build_llm_config(mem_cfg['model'])
                self.memory_analyst_agent = autogen.ConversableAgent(
                    name="Memory_Analyst",
                    llm_config=mem_llm,
                    system_message=MOAPrompts.get_memory_analyst_prompt(),
                    human_input_mode="NEVER"
                )
                self.memory_analyst_agent.silent = False
                self.memory_analyst_fallback_model = mem_cfg.get("fallback")  # dict lub None
                process_log("[INIT] Memory Analyst initialized")
            except Exception as e:
                process_log(f"[INIT][WARN] Memory Analyst init failed: {e}")
        else:
            process_log("[INIT] Memory Analyst not configured")
        
        
        
        #koniec memory agent
        
            
        # Aggregator
        aggregator_config = next((a for a in self.config['agents'] if 'aggregator' in a['role_name'].lower()), None)
        if aggregator_config:
            self.aggregator = autogen.ConversableAgent(
                name="Master_Aggregator",
                llm_config=self._build_llm_config(aggregator_config['model']),
                system_message=MOAPrompts.get_aggregator_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=lambda m: False,
            )
        else:
            self.aggregator = autogen.ConversableAgent(
                name="Master_Aggregator",
                llm_config={"config_list": [{"model": "dummy", "api_type": "dummy"}]},
                system_message=MOAPrompts.get_aggregator_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=lambda m: False,
            )
        self.aggregator.silent = False
        self.aggregator_agent = self.aggregator
        process_log("[INIT] Aggregator initialized")

        # Critic
        critic_config = next((a for a in self.config['agents'] if 'critic' in a['role_name'].lower()), None)
        if critic_config:
            self.critic = autogen.ConversableAgent(
                name="Quality_Critic",
                llm_config=self._build_llm_config(critic_config['model']),
                system_message=self._build_critic_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=self._is_final_plan_message,
            )
        else:
            self.critic = autogen.ConversableAgent(
                name="Quality_Critic",
                llm_config={"config_list": [{"model": "dummy", "api_type": "dummy"}]},
                system_message=self._build_critic_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=self._is_final_plan_message,
            )
        self.critic.silent = False
        self.critic_agent = self.critic
        process_log("[INIT] Critic initialized")

        process_log(f"Initialized {len(self.proposer_agents)} proposers, 1 aggregator, 1 critic using AutoGen")
    
    
    
    def reset(self):
        """
        Minimalny reset: czyści historię agentów i liczniki sesji,
        bez dotykania cache'u LLM i bez zmian w konfiguracji.
        """
        agents = []

        # Zbierz agentów, jeśli istnieją (nie zakładamy, że wszystkie są zainicjalizowane)
        if getattr(self, "user_proxy", None):           agents.append(self.user_proxy)
        if getattr(self, "proposer_agents", None):      agents.extend(self.proposer_agents)
        if getattr(self, "aggregator_agent", None):     agents.append(self.aggregator_agent)
        if getattr(self, "critic_agent", None):         agents.append(self.critic_agent)
        if getattr(self, "memory_analyst_agent", None): agents.append(self.memory_analyst_agent)

        # Czyść historie czatu agentów (jeśli wspierają .reset())
        for ag in agents:
            try:
                if hasattr(ag, "reset") and callable(ag.reset):
                    ag.reset()
            except Exception as e:
                process_log(f"[RESET][WARN] {getattr(ag,'name','<agent>')}: {e}")

        # Zresetuj licznik iteracji i bootstrap
        self.iteration_count = 0
        self._initial_bootstrap = ""

        # (opcjonalnie) miękko wyczyść lokalny kontekst pamięci, jeśli ma takie API
        try:
            mem = getattr(self, "memory", None)
            if mem and hasattr(mem, "reset") and callable(mem.reset):
                mem.reset()
            elif mem and hasattr(mem, "clear") and callable(mem.clear):
                mem.clear()
        except Exception as e:
            process_log(f"[RESET][WARN] memory: {e}")

        process_log("[RESET] Orchestrator state cleared")
    
    
    
    
    def _runtime_env_snapshot(self) -> dict:
        # tylko presence, bez wartości
        def present(k): return bool(os.getenv(k))
        return {
            "VERTEXAI_PROJECT": present("VERTEXAI_PROJECT") or present("GOOGLE_CLOUD_PROJECT") or present("GCP_PROJECT"),
            "VERTEXAI_LOCATION": present("VERTEXAI_LOCATION") or present("GOOGLE_CLOUD_REGION"),
            "ANTHROPIC_API_KEY": present("ANTHROPIC_API_KEY"),
            "OPENAI_API_KEY": present("OPENAI_API_KEY"),
        }

    def _agent_signature(self, agent) -> dict:
        llm = getattr(agent, "llm_config", {})
        # wyciągamy pierwszy wpis z config_list dla krótkiego podpisu
        vendor = None; model = None
        try:
            entry = (llm.get("config_list") or [{}])[0]
            if "google" in entry:
                vendor = "google"; model = entry["google"].get("model")
            elif "anthropic" in entry:
                vendor = "anthropic"; model = entry["anthropic"].get("model")
            elif "openai" in entry:
                vendor = "openai"; model = entry["openai"].get("model")
            else:
                vendor = (entry.get("api_type") or "unknown")
                model = entry.get("model")
        except Exception:
            pass
        return {"name": getattr(agent, "name", "?"), "vendor": vendor, "model": model}

    def _sanity_ping_agent(self, agent) -> None:
        tmp_user = UserProxyAgent(
            "sanity_user", human_input_mode="NEVER", max_consecutive_auto_reply=1,
            is_termination_msg=lambda m: True, code_execution_config=False
        )
        try:
            tmp_user.initiate_chat(agent, message="Odpowiedz dokładnie słowem: PONG")
        except Exception as e:
            sig = self._agent_signature(agent)
            snap = self._runtime_env_snapshot()
            raise RuntimeError(
                f"[SANITY PING FAILED] agent={sig} | env={snap} | err={type(e).__name__}: {e}"
            ) from e
    
    
    
    
    
    
    def _build_llm_config(self, model_config: dict) -> dict:
        """
        Buduje llm_config dla AutoGen na bazie agents_config.json,
        używając config_api.basic_config_agent (ten sam format co w solo).
        Google => Vertex/ADC (bez api_key), Anthropic/OpenAI => klucze z ENV/SM.
        """
        from config_api import basic_config_agent, PROJECT_ID as DEFAULT_PROJECT_ID, LOCATION as DEFAULT_LOCATION

        # --- helpery ---
        DEFAULT_MODEL_BY_PROVIDER = {
            "google": "gemini-2.5-pro",
            "anthropic": "claude-3-7-sonnet",
            "openai": "gpt-4o-mini",
        }

        def _map_provider_to_api_type(provider: str) -> str:
            p = (provider or "google").strip().lower()
            return {
                "google": "google", "gemini": "google", "vertex": "google",
                "anthropic": "anthropic",
                "openai": "openai", "azure_openai": "openai",
            }.get(p, p)

        def _validate_provider_model_pair(api_type: str, model: str) -> None:
            m = (model or "").lower()
            if api_type == "google" and not m.startswith("gemini"):
                raise ValueError(f"Model '{model}' nie pasuje do providera 'google' (Vertex/Gemini).")
            if api_type == "anthropic" and not m.startswith("claude"):
                raise ValueError(f"Model '{model}' nie pasuje do 'anthropic'.")
            if api_type == "openai" and not ("gpt" in m or m.startswith("o")):
                raise ValueError(f"Model '{model}' nie wygląda na model OpenAI.")

        # 1) provider -> api_type
        api_type = _map_provider_to_api_type(model_config.get("provider"))

        # 2) model + sanity
        agent_name = model_config.get("model_name") or DEFAULT_MODEL_BY_PROVIDER.get(api_type, "gemini-2.5-pro")
        _validate_provider_model_pair(api_type, agent_name)

        # 3) projekt/region tylko dla Google/Vertex
        project_id = model_config.get("project_id") or DEFAULT_PROJECT_ID
        location   = model_config.get("location")   or DEFAULT_LOCATION
        if api_type == "google" and not project_id:
            raise RuntimeError("Vertex/Gemini: brak project_id. Ustaw VERTEXAI_PROJECT/GOOGLE_CLOUD_PROJECT albo podaj 'project_id' w agents_config.json.")

        # 4) api_key tylko dla nie-Google
        api_key_arg = None if api_type == "google" else model_config.get("api_key")

        # 5) wołamy Twój builder
        flat_list = basic_config_agent(
            agent_name = agent_name,
            api_type   = api_type,
            location   = (location if api_type == "google" else None),   # <-- KLUCZOWA ZMIANA
            project_id = (project_id if api_type == "google" else None), # <-- KLUCZOWA ZMIANA
            api_key    = api_key_arg,
        )
        if not isinstance(flat_list, list) or not flat_list:
            raise ValueError("basic_config_agent powinien zwrócić niepustą listę.")

        entry = dict(flat_list[0])  # kopia, żeby móc czyścić

        # 6) sanity: dla nie-Google WYTNJIJ project/location (gdyby kiedyś znów wpadły)
        if api_type != "google":
            entry.pop("project_id", None)
            entry.pop("location", None)

        # 7) finalny llm_config
        return {
            "config_list": [entry],
            "temperature": float(model_config.get("temperature", 0.0)),
            "seed": 42,
            "cache_seed": 42,
        }
       
    
    def _build_proposer_prompt(self, role: AgentRole) -> str:
        """Buduje prompt dla proposera z kontekstem"""
        base_prompt = MOAPrompts.get_proposer_prompt(role, self.mission, self.node_library)
        
        # Dodaj dynamiczny kontekst
        if self.current_context:
            context_injection = self._build_context_injection()
            base_prompt = base_prompt + "\n\n" + context_injection
            # return base_prompt + "\n\n" + context_injection
        
        base_prompt = EXPLAINABILITY.on_prompt_build(base_prompt, role.role_name)
        return base_prompt
    
    def _build_critic_prompt(self) -> str:
        """Buduje prompt dla krytyka"""
        base_prompt = MOAPrompts.get_critic_prompt()

        # Dodaj specjalną instrukcję o frazie kończącej i nowej strukturze JSON
        additional_instruction = """

        ## CRITICAL OUTPUT STRUCTURE
        - If you REJECT the plan, provide your standard critique with weaknesses and suggestions.
        - If you APPROVE the plan, your JSON response MUST contain a top-level key named `plan_approved`. Inside this key, you MUST place the complete, final, synthesized plan object. The other keys (like critique_summary) should still be present.

        Example of an APPROVED response structure:
        ```json
        {
          "critique_summary": {
            "verdict": "ZATWIERDZONY",
            "statement": "Plan jest doskonały, spełnia wszystkie wymagania.",
            ...
          },
          "plan_approved": {
            "entry_point": "Start_Node",
            "nodes": [ ... ],
            "edges": [ ... ]
          },
          ...
        }
        ```

        ## GOLDEN TERMINATION RULE
        If you approve the plan, you MUST end your ENTIRE response with the exact phrase on a new line, after the JSON block:
        PLAN_ZATWIERDZONY
        """
        return EXPLAINABILITY.on_prompt_build(base_prompt + additional_instruction, "Quality_Critic")
        # return base_prompt + additional_instruction
    
    def _build_context_injection(self) -> str:
        """Buduje wstrzyknięcie kontekstu"""
        parts = []
        
        if self.current_context.get('recommended_strategies'):
            parts.append("## 💡 RECOMMENDED STRATEGIES (from memory):")
            for strategy in self.current_context['recommended_strategies']:
                parts.append(f"• {strategy}")
        
        if self.current_context.get('common_pitfalls'):
            parts.append("\n## ⚠️ COMMON PITFALLS TO AVOID:")
            for pitfall in self.current_context['common_pitfalls']:
                parts.append(f"• {pitfall}")
        
        if self.current_context.get('last_feedback'):
            parts.append(f"\n## 📝 LAST FEEDBACK:\n{self.current_context['last_feedback']}")
        
        return "\n".join(parts)
    

    def run_full_debate_cycle(self):
        from autogen import GroupChat, GroupChatManager
        import json, os, traceback
        from datetime import datetime
        self.reset()
        # Lazy-guard: jeśli ktoś zawoła przed init
        for must in ("user_proxy", "proposer_agents", "aggregator_agent", "critic_agent"):
            if not hasattr(self, must) or getattr(self, must) is None:
                self._initialize_autogen_agents()
                break

        # Szybkie asserty z czytelnym komunikatem
        if not self.proposer_agents:
            raise RuntimeError("Brak proposerów. Sprawdź agents_config.json (role bez 'aggregator'/'critic').")
        if not self.aggregator_agent:
            raise RuntimeError("Brak agregatora. Sprawdź agents_config.json (rola 'Aggregator').")
        if not self.critic_agent:
            raise RuntimeError("Brak krytyka. Sprawdź agents_config.json (rola 'Critic').")

        max_rounds = len(self.proposer_agents) + 2

        # Bootstrap misji – bez 'PLAN_ZATWIERDZONY' w treści, żeby manager nie kończył po 1 msg
        bootstrap = (
            f"## MISJA\n{self.mission}\n\n"
    "Zaproponuj kompletny PLAN w formacie JSON {entry_point, nodes[], edges[]}.\n"
    "Rola: Proposerzy proponują swoje wersje planu. Następnie Aggregator scala je w jedną, spójną propozycję. "
    "Na końcu, Quality_Critic oceni finalny, zagregowany plan."
        )

        self._initial_bootstrap = bootstrap
        
        
        #nowe wczytywanie pamieci
        memory_msg = None
        try:
            memory_msg = self._make_memory_message_once()
            if memory_msg and isinstance(memory_msg, dict) and memory_msg.get("content"):
                process_log("[MEMORY] Seeded memory message into history")
            else:
                process_log("[MEMORY][WARN] No usable memory message produced; seeding skipped")
                memory_msg = None
        except Exception as e:
            process_log(f"[MEMORY][ERROR] Could not build memory message: {type(e).__name__}: {e}")
            memory_msg = None
        
        #koniec 
        
        
        
        
        agents = (
        ([self.memory_analyst_agent] if self.memory_analyst_agent else []) +
        [*self.proposer_agents, self.aggregator_agent, self.critic_agent]
        )
        
        # Uczestnicy – tylko agenci
        # agents = [*self.proposer_agents, self.aggregator_agent, self.critic_agent]

        turns_per_iteration = len(self.proposer_agents) + 2 
        max_rounds = self.max_iterations * turns_per_iteration + 5 # Dodajemy bufor bezpieczeństwa

        start_messages = []
        if memory_msg:
            start_messages.append(memory_msg)

        
        
        gc = GroupChat(
            agents=agents,
            messages = start_messages,
            max_round=max_rounds, # Używamy nowej, dynamicznie obliczonej wartości
            speaker_selection_method=self.custom_speaker_selection_logic)
        
        self.groupchat = gc
        
        try:
            for i, m in enumerate(gc.messages[:3]):  # pokaż do 3 pierwszych
                n, l, tn = self._extract_name_and_len(m)
                process_log(f"[MEMORY][DEBUG] m{i}: type={tn} name={n} len={l}")
        except Exception as e:
            process_log(f"[MEMORY][DEBUG] inspect failed: {type(e).__name__}: {e}")
        
        
        
        manager = GroupChatManager(
            groupchat=gc,
            llm_config=self.aggregator_agent.llm_config,
            human_input_mode="NEVER",
            system_message=MOAPrompts.get_aggregator_prompt(),
            is_termination_msg=self._is_final_plan_message
        )

        try:
            # Start rozmowy – to uruchamia całą maszynkę
            self.user_proxy.initiate_chat(manager, message=bootstrap, max_turns=max_rounds)
        except StopIteration:
            # To jest OK - plan został zatwierdzony
            process_log("[SUCCESS] Debata zakończona przez StopIteration - plan zatwierdzony")
        try:
            # Szukamy finalnej odpowiedzi
            final_plan_message_content = None
            messages = manager.groupchat.messages
            for msg in reversed(messages):
                
                content_str = str(msg.get("content", ""))
                # Używamy Twojej nowej, precyzyjnej funkcji sprawdzającej
                if "PLAN_ZATWIERDZONY" in content_str:
                    final_plan_message_content = msg.get("content")
                    break
                    

            # Jeśli znaleziono zatwierdzoną wiadomość, sparsuj ją
            if final_plan_message_content:
                process_log("[SUCCESS] Krytyk zatwierdził plan. Rozpoczynam parsowanie...")
                try:
                    parsed_critic_response = self.parser.parse_critic_response(final_plan_message_content)

                    # TUTAJ WKLEJ NOWY KOD (zamiast linii 67-75):
                    if parsed_critic_response:
                        # Szukaj planu w różnych możliwych miejscach
                        final_plan = None

                        # Lista możliwych kluczy
                        possible_keys = [
                            "plan_approved",
                            "final_synthesized_plan", 
                            "final_plan",
                            "synthesized_plan",
                            "approved_plan",
                            "plan"
                        ]

                        for key in possible_keys:
                            if key in parsed_critic_response:
                                candidate = parsed_critic_response[key]
                                # Sprawdź czy to wygląda jak plan (ma entry_point i nodes)
                                if isinstance(candidate, dict) and "entry_point" in candidate and "nodes" in candidate:
                                    final_plan = candidate
                                    process_log(f"[SUCCESS] Znaleziono plan pod kluczem: '{key}'")
                                    break

                        if final_plan:
                            self.final_plan = final_plan
                            self._save_successful_plan()
                            
                            
                            
                            # Zbierz stan orchestratora
                            orchestrator_state = {
                                "iteration_count": self.iteration_count,
                                "execution_time": (datetime.now() - start_time).total_seconds() if 'start_time' in locals() else 0,
                                "total_tokens": getattr(self, 'token_counter', 0),
                                "api_calls": getattr(self, 'api_call_counter', 0)
                            }
                            
                            # try:
                                # Jeśli masz obliczony final_score z jakości planu – możesz go tutaj podać, inaczej None
                            #     mission_id = save_mission_to_gcs(
                            #         bucket_name="memory_rag_for_agents",
                            #         base_prefix="missions",
                            #         mission=self.mission,
                            #         final_plan=self.final_plan,
                            #         all_messages=manager.groupchat.messages,
                            #         orchestrator_state=orchestrator_state,
                            #         approved=True,
                            #         final_score=None,
                            #     )
                            #     process_log(f"[ORCHESTRATOR] Mission completed and saved as: {mission_id}")
                            # except Exception as err:
                            #     # jedno wywołanie loguje zarówno nagłówek, jak i pełny traceback
                            #     log_exc("[MEMORY:GCS][ERROR] Nie udało się zapisać misji", err)
                            
                            if self.memory:
                                # Zapisz KOMPLETNĄ misję
                                mission_id = self.memory.save_complete_mission(
                                mission=self.mission,
                                final_plan=self.final_plan,
                                all_messages=manager.groupchat.messages,
                                orchestrator_state=orchestrator_state
                                )

                                process_log(f"[ORCHESTRATOR] Mission completed and saved as: {mission_id}")
    
                                try:
                                    ndjson_uris = export_local_by_filename_date(
                                        input_dir="memory/missions",                # folder z lokalnymi plikami mission_*.json
                                        output_root_gcs="gs://external_memory/missions",
                                        pattern="*.json",
                                    )
                                    process_log(f"[EXPORT] Wyeksportowano {len(ndjson_uris)} misji do GCS")
                                except Exception as e:
                                    # w razie błędu logujemy pełen traceback
                                    log_exc("[EXPORT][ERROR] Eksport misji do GCS nie powiódł się", e)
                            
                            return self.final_plan
                        else:
                            # Jeśli nie znaleziono planu w żadnym kluczu
                            raise RuntimeError(f"Nie znaleziono planu w odpowiedzi. Dostępne klucze: {list(parsed_critic_response.keys())}")
                    else:
                        raise RuntimeError("Parser zwrócił None - nie udało się sparsować JSON")
            
                except Exception as parse_error:
                    #poprawka
                    log_exc("[ERROR] Nie udało się sparsować odpowiedzi krytyka", parse_error)
                    
                    #koniec poprawki
                    
                    
                    # Sytuacja awaryjna: nie udało się sparsować odpowiedzi krytyka
                    # process_log(f"[ERROR] Nie udało się sparsować odpowiedzi krytyka: {parse_error}")
                    
                    
                    # Zapisz raport z surową odpowiedzią do analizy
                    self._write_failure_report(
                        reason="CRITIC_RESPONSE_PARSE_FAILURE",
                        stage="post-debate_parsing",
                        aggregator_raw=None, # Nieistotne na tym etapie
                        critic_raw=final_plan_message_content,
                        exception=parse_error
                    )
                    
                    return None # Zwracamy None w przypadku błędu parsowania
                
                
                explainability_report = EXPLAINABILITY.generate_final_report()
                process_log(f"[ORCHESTRATOR] Explainability report generated: {explainability_report['debate_id']}")
            else:
                explainability_report = EXPLAINABILITY.generate_final_report()
                process_log(f"[ORCHESTRATOR] Explainability report generated: {explainability_report['debate_id']}")
                # Jeśli pętla się zakończyła i nie znaleziono zatwierdzonej wiadomości
                raise RuntimeError("Debata zakończona, ale krytyk nigdy nie zwrócił wiadomości z 'PLAN_ZATWIERDZONY'.")

        except Exception as e:
            # Raport diagnostyczny
            tb = traceback.format_exc()
            os.makedirs("reports", exist_ok=True)
            path = f"reports/failure_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(path, "w", encoding="utf-8") as f:
                json.dump({"error_type": type(e).__name__,
                           "error_message": str(e),
                           "stacktrace": tb}, f, ensure_ascii=False, indent=2)
            process_log(f"[FAILSAFE] Saved failure report: {path}")
            process_log(tb)
            
            explainability_report = EXPLAINABILITY.generate_final_report()
            process_log(f"[ORCHESTRATOR] Explainability report generated: {explainability_report['debate_id']}")
            
            return None
    
   

    
        
    
    def _update_context_from_last_critique(self, critique_message: str):
        """Aktualizuje kontekst na podstawie krytyki"""
        # Parsuj krytykę
        if not self.memory:
            return
        
        
        parsed = self.parser.parse_critic_response(critique_message)
        
        if parsed:
            feedback = f"Score: {parsed.get('score', 'N/A')}. "
            feedback += f"Weaknesses: {', '.join(parsed.get('weaknesses', []))}. "
            feedback += f"Improvements: {', '.join(parsed.get('improvements', []))}"
            
            self.current_context['last_feedback'] = feedback
            
            # Zapisz do pamięci
            self.memory.add_iteration_feedback(
                iteration=self.iteration_count,
                feedback=feedback,
                timestamp=datetime.now()
            )
        
        # Odśwież kontekst z pamięci
        self.current_context = self.memory.get_relevant_context(self.mission)
        
        process_log(f"Context updated for iteration {self.iteration_count}")
    
    def _extract_final_plan(self, messages: List[Dict]):
        """Wyodrębnia zatwierdzony plan z historii wiadomości"""
        # Szukaj od końca
        for msg in reversed(messages):
            content = msg.get("content", "")
            name = msg.get("name", "")
            
            # Jeśli krytyk zatwierdził
            if name == "Quality_Critic" and "PLAN_ZATWIERDZONY" in content:
                # Znajdź ostatni plan od agregatora
                for prev_msg in reversed(messages):
                    if prev_msg.get("name") == "Master_Aggregator":
                        parsed = self.parser.parse_agent_response(prev_msg.get("content", ""))
                        if parsed:
                            self.final_plan = parsed.get("final_plan", parsed.get("plan"))
                            break
                break
        
        process_log(f"Final plan extracted: {self.final_plan is not None}")
    
    def _save_successful_plan(self):
        """Zapisuje udany plan do pamięci i pliku"""
        if not self.final_plan:
            return
        if self.memory:
           
        # Zapisz do pamięci
            self.memory.add_successful_plan(
                plan=self.final_plan,
                mission=self.mission,
                metadata={
                    'iterations': self.iteration_count,
                    'agents_count': len(self.proposer_agents)
                }
            )
        
        # Zapisz do pliku
        output = {
            "mission": self.mission,
            "final_plan": self.final_plan,
            "metadata": {
                "iterations": self.iteration_count,
                "timestamp": datetime.now().isoformat(),
                "autogen_debate": True
            }
        }
        
        os.makedirs("outputs", exist_ok=True)
        output_file = f"outputs/autogen_plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"💾 Plan saved to: {output_file}")
        process_log(f"Successful plan saved to {output_file}")
        
        
    def _debug_dump_transcript(self, groupchat, tail: int = 30):
        """Wypisz ostatnie ~N wiadomości debaty, żeby było je widać w notebooku."""
        from process_logger import log as process_log
        try:
            msgs = getattr(groupchat, "messages", [])[-tail:]
            process_log("----- TRANSCRIPT (tail) -----")
            for m in msgs:
                role = m.get("role") or m.get("name") or "?"
                name = m.get("name") or ""
                content = m.get("content") or ""
                head = (content[:400] + "...") if len(content) > 400 else content
                process_log(f"{role} {name}: {head}")
            process_log("----- END TRANSCRIPT -----")
        except Exception as e:
            process_log(f"[TRANSCRIPT_DUMP_FAIL] {type(e).__name__}: {e}")


--- FILE: autogen_vertex_mcp_system_claude.py ---

# autogen_vertex_mcp_system.py
"""
AutoGen Multi-Agent System with Vertex AI Search as MCP Tool
Agents learn from mission memory without prompt injection
"""

import asyncio
import json
import logging
from typing import Any, Dict, List, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime

import autogen
from autogen import AssistantAgent, UserProxyAgent
from google.cloud import discoveryengine_v1 as de
from google.cloud import storage
from google.api_core.client_options import ClientOptions

# ======================== Configuration ========================

TOOL_DEFINITIONS = [
    {
        "type": "function",
        "function": {
            "name": "search_mission_memory",
            "description": "Search the Vertex AI mission memory for past missions based on a query.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The search query (e.g., 'CSV pipeline error handling').",
                    },
                    "filters": {
                        "type": "string",
                        "description": "Optional filters (e.g., 'status:SUCCESS').",
                    },
                    "top_k": {
                        "type": "integer",
                        "description": "Number of results to return (default 5).",
                    },
                },
                "required": ["query"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_mission_plan",
            "description": "Retrieve the complete execution plan (nodes and edges) for a specific mission ID.",
            "parameters": {
                "type": "object",
                "properties": {
                    "mission_id": {
                        "type": "string",
                        "description": "The unique ID of the mission.",
                    }
                },
                "required": ["mission_id"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "analyze_patterns",
            "description": "Analyze successful patterns across high-scoring missions.",
            "parameters": {
                "type": "object",
                "properties": {
                    "min_score": {
                        "type": "number",
                        "description": "Minimum score threshold (default 90.0).",
                    }
                },
                "required": [],
            },
        },
    },
]


@dataclass
class SystemConfig:
    """Central configuration"""

    vertex_search_config: str = (
        "projects/815755318672/locations/us/collections/default_collection/dataStores/external-memory-connector_1756845276280_gcs_store/servingConfigs/default_config"
    )
    vertex_location: str = "us"

    # AutoGen config - możesz zmienić na Gemini jeśli masz setup
    autogen_config: Dict[str, Any] = field(
        default_factory=lambda: {
            "config_list": [
                {
                    "model": "gpt-4-turbo-preview",  # lub "gemini-1.5-pro" jeśli używasz Vertex AI
                    "api_key": "your-api-key",  # lub vertex ai credentials
                    "temperature": 0.7,
                }
            ],
            "timeout": 120,
            "cache_seed": 42,
        }
    )

    def get_llm_config_with_tools(self) -> Dict[str, Any]:
        config = self.autogen_config.copy()
        # Wstrzykujemy definicje narzędzi tutaj (użyj "tools" dla nowoczesnych API)
        config["tools"] = TOOL_DEFINITIONS
        return config

    max_search_results: int = 5
    enable_learning: bool = True


# ======================== Vertex AI Search Tool ========================


class VertexSearchTool:
    """Vertex AI Search as a tool for AutoGen agents"""

    def __init__(self, config: SystemConfig):
        self.config = config
        self.storage_client = storage.Client()
        self.search_client = de.SearchServiceClient(
            client_options=ClientOptions(
                api_endpoint=f"{config.vertex_location}-discoveryengine.googleapis.com"
            )
        )
        self._cache = {}

    def search_mission_memory(
        self, query: str, filters: Optional[str] = None, top_k: int = 5
    ) -> str:
        """
        Search mission memories. Returns JSON string.
        This is the function that AutoGen agents will call.
        """
        try:
            # Check cache
            cache_key = f"{query}_{filters}_{top_k}"
            if cache_key in self._cache:
                return json.dumps(self._cache[cache_key])

            # Build request
            req = de.SearchRequest(
                serving_config=self.config.vertex_search_config,
                query=query,
                page_size=min(top_k, self.config.max_search_results),
                filter=filters if filters else None,
            )

            # Execute search
            results = []
            for r in self.search_client.search(request=req):
                doc = r.document
                sdata = doc.struct_data or {}

                results.append(
                    {
                        "mission_id": sdata.get("mission_id", ""),
                        "score": sdata.get("final_score", 0),
                        "approved": sdata.get("approved", False),
                        "nodes_count": sdata.get("nodes_count", 0),
                        "edges_count": sdata.get("edges_count", 0),
                        "has_optimization": sdata.get("has_optimization", False),
                        "has_rollback": sdata.get("has_rollback", False),
                        "has_retry": sdata.get("has_retry", False),
                        # KLUCZOWA POPRAWKA: Konwertujemy RepeatedComposite na listę Pythona
                        "tags": list(sdata.get("tags", [])),
                        # KLUCZOWA POPRAWKA: Konwertujemy Struct/Map na słownik Pythona
                        "links": dict(sdata.get("links", {})),
                    }
                )

            response = {
                "status": "success",
                "query": query,
                "results": results,
                "count": len(results),
            }

            # Cache result
            self._cache[cache_key] = response

            return json.dumps(response, ensure_ascii=False)

        except Exception as e:
            error_response = {
                "status": "error",
                "error": str(e),
                "query": query,
                "results": [],
            }
            return json.dumps(error_response)

    def get_mission_plan(self, mission_id: str) -> str:
        """
        Get complete plan for a mission. Returns JSON string.
        """
        try:
            # First find the mission
            search_result = self.search_mission_memory(
                query=f"mission_id:{mission_id}", top_k=1
            )
            search_data = json.loads(search_result)

            if not search_data.get("results"):
                return json.dumps({"status": "not_found", "mission_id": mission_id})

            links = search_data["results"][0].get("links", {})
            plan_uri = links.get("plan_uri")

            if not plan_uri or not plan_uri.startswith("gs://"):
                return json.dumps({"status": "no_plan", "mission_id": mission_id})

            # Fetch from GCS
            bucket_name, _, path = plan_uri[5:].partition("/")
            blob = self.storage_client.bucket(bucket_name).blob(path)
            plan_content = blob.download_as_text(encoding="utf-8")
            plan_data = json.loads(plan_content)

            return json.dumps(
                {"status": "success", "mission_id": mission_id, "plan": plan_data}
            )

        except Exception as e:
            return json.dumps(
                {"status": "error", "error": str(e), "mission_id": mission_id}
            )

    def analyze_patterns(self, min_score: float = 90.0) -> str:
        """
        Analyze successful patterns. Returns JSON string.
        """
        try:
            # Search for high-scoring approved missions
            results_json = self.search_mission_memory(
                query="approved:true", filters=f"final_score >= {min_score}", top_k=20
            )
            results_data = json.loads(results_json)

            if not results_data.get("results"):
                return json.dumps({"status": "no_data", "patterns": {}})

            results = results_data["results"]
            total = len(results)

            patterns = {
                "optimization_rate": sum(
                    1 for r in results if r.get("has_optimization")
                )
                / total,
                "rollback_rate": sum(1 for r in results if r.get("has_rollback"))
                / total,
                "retry_rate": sum(1 for r in results if r.get("has_retry")) / total,
                "avg_nodes": sum(r.get("nodes_count", 0) for r in results) / total,
                "avg_edges": sum(r.get("edges_count", 0) for r in results) / total,
                "avg_score": sum(r.get("score", 0) for r in results) / total,
                "total_analyzed": total,
            }

            # Find common tags
            tag_counts = {}
            for r in results:
                for tag in r.get("tags", []):
                    tag_counts[tag] = tag_counts.get(tag, 0) + 1

            patterns["common_tags"] = sorted(
                tag_counts.items(), key=lambda x: x[1], reverse=True
            )[:5]

            return json.dumps({"status": "success", "patterns": patterns})

        except Exception as e:
            return json.dumps({"status": "error", "error": str(e), "patterns": {}})


# ======================== AutoGen Agents Setup ========================


class MissionPlanningTeam:
    """AutoGen multi-agent team for mission planning"""

    def __init__(self, config: SystemConfig):
        self.config = config
        self.vertex_tool = VertexSearchTool(config)
        self.setup_agents()

    def setup_agents(self):
        """Setup AutoGen agents with tools"""

        llm_config_with_tools = self.config.get_llm_config_with_tools()

        tool_implementation_map = {
            "search_mission_memory": self.vertex_tool.search_mission_memory,
            "get_mission_plan": self.vertex_tool.get_mission_plan,
            "analyze_patterns": self.vertex_tool.analyze_patterns,
        }

        # Memory Analyst Agent - analizuje pamięć misji
        self.memory_analyst = AssistantAgent(
            name="MemoryAnalyst",
            system_message="""You are a memory analyst. Your ONLY function is to retrieve data using tools.
            You MUST immediately call a tool. 
            Do NOT respond with text, explanations, thought processes, or acknowledgments. 
            
            Instructions:
            1. Call 'search_mission_memory' immediately.
            
            RESPOND ONLY WITH A TOOL CALL.
            """,
            llm_config=llm_config_with_tools,
        )

        # Graph Designer Agent - projektuje grafy wykonania
        self.graph_designer = AssistantAgent(
            name="GraphDesigner",
            system_message="""You are a graph execution plan designer. Your role is to:
            1. Design execution graphs with nodes and edges
            2. Incorporate patterns learned from successful missions
            3. Ensure robustness with error handling, rollback, and optimization
            
            Create plans in this JSON format:
            {
                "entry_point": "StartNode",
                "nodes": [
                    {"name": "NodeName", "implementation": "function", "params": {}}
                ],
                "edges": [
                    {"from": "Node1", "to": "Node2", "condition": "on_success"}
                ]
            }
            
            Use insights from MemoryAnalyst to improve your designs.""",
            llm_config=self.config.autogen_config,
        )

        # Quality Critic Agent - ocenia i ulepsza plany
        self.quality_critic = AssistantAgent(
            name="QualityCritic",
            system_message="""You are a quality critic. Your role is to:
            1. Evaluate proposed plans against historical success patterns
            2. Identify missing robustness features (rollback, retry, optimization)
            3. Suggest improvements based on data from mission memory
            4. Calculate confidence scores
            
            Be constructive but critical. Use data to support your assessments.""",
            llm_config=llm_config_with_tools,
        )

        self.executor = UserProxyAgent(
            name="Executor",
            system_message="Execute the tools requested by agents and report the results verbatim.",
            # WŁĄCZAMY wykonanie kodu (wymagane do uruchomienia funkcji Python)
            code_execution_config={
                "use_docker": False,
                "work_dir": "tool_execution_logs",
            },
            human_input_mode="NEVER",
            # Rejestrujemy implementacje funkcji TUTAJ
            function_map=tool_implementation_map,
            llm_config=False,  # Wykonawca nie potrzebuje LLM do myślenia
        )

    async def create_mission_plan(self, mission_prompt: str) -> Dict[str, Any]:
        """
        Create a mission plan using the multi-agent team
        """
        logging.info(f"Starting mission planning for: {mission_prompt}")

        # Create group chat
        groupchat = autogen.GroupChat(
            agents=[
                self.executor,  # Używamy Executora zamiast Coordinatora
                self.memory_analyst,
                self.graph_designer,
                self.quality_critic,
            ],
            messages=[],
            max_round=15,
            speaker_selection_method="auto",
        )

        manager = autogen.GroupChatManager(
            groupchat=groupchat, llm_config=self.config.autogen_config
        )

        # Start the planning process
        initial_message = f"""
        Create an execution plan for this mission: {mission_prompt}
        
        Process:
        1. MemoryAnalyst: Search for similar successful missions
        2. MemoryAnalyst: Get FULL DETAILS (complete plans) of top 2-3 similar missions
        3. MemoryAnalyst: Analyze the exact structure - what nodes, edges, conditions they use
        4. GraphDesigner: Create a plan based on ACTUAL successful plan structures
        5. QualityCritic: Compare new plan with successful ones and suggest improvements
        
        MemoryAnalyst, start by finding similar missions and then GET THEIR COMPLETE PLANS.
        """

        # Initiate chat
        await self.executor.a_initiate_chat(
            manager, message=initial_message, clear_history=True
        )

        # Extract the final plan from conversation
        final_plan = self._extract_plan_from_messages(groupchat.messages)

        # Get learning context
        learning_context = self._extract_learning_context(groupchat.messages)

        return {
            "mission_prompt": mission_prompt,
            "final_plan": final_plan,
            "learning_context": learning_context,
            "conversation_rounds": len(groupchat.messages),
            "timestamp": datetime.now().isoformat(),
        }

    def _extract_plan_from_messages(self, messages: List[Dict]) -> Dict[str, Any]:
        """Extract the final plan from agent messages"""
        # Look for JSON plans in reverse order (latest first)
        for msg in reversed(messages):
            if msg.get("name") == "GraphDesigner":
                content = msg.get("content", "")
                try:
                    # Try to find JSON in the content
                    import re

                    json_match = re.search(
                        r'\{.*"entry_point".*"nodes".*"edges".*\}', content, re.DOTALL
                    )
                    if json_match:
                        return json.loads(json_match.group())
                except:
                    continue

        # Return empty plan if not found
        return {"entry_point": "", "nodes": [], "edges": []}

    def _extract_learning_context(self, messages: List[Dict]) -> Dict[str, Any]:
        """Extract what was learned from memory"""
        context = {
            "searched_queries": [],
            "analyzed_missions": [],
            "identified_patterns": {},
            "applied_improvements": [],
        }

        for msg in messages:
            if msg.get("name") == "MemoryAnalyst":
                content = msg.get("content", "")
                # Extract search queries and results
                if "search_mission_memory" in content:
                    context["searched_queries"].append(content[:100])
                if "mission_id" in content:
                    # Extract mission IDs mentioned
                    import re

                    ids = re.findall(r"mission_\w+", content)
                    context["analyzed_missions"].extend(ids[:5])

            elif msg.get("name") == "QualityCritic":
                if "improvement" in msg.get("content", "").lower():
                    context["applied_improvements"].append(msg.get("content", "")[:200])

        return context


# ======================== Main Execution ========================


class MissionExecutor:
    """Main system orchestrator"""

    def __init__(self, config: Optional[SystemConfig] = None):
        self.config = config or SystemConfig()
        self.team = MissionPlanningTeam(self.config)
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )

    async def execute_mission(self, mission_prompt: str) -> Dict[str, Any]:
        """Execute a mission with learning from memory"""

        logging.info("=" * 60)
        logging.info(f"MISSION: {mission_prompt}")
        logging.info("=" * 60)

        # Create plan using multi-agent team
        result = await self.team.create_mission_plan(mission_prompt)

        # Log what was learned
        if result.get("learning_context", {}).get("analyzed_missions"):
            logging.info(
                f"Learned from missions: {result['learning_context']['analyzed_missions']}"
            )

        logging.info(
            f"Plan created with {len(result['final_plan'].get('nodes', []))} nodes"
        )

        return result

    def run(self, mission_prompt: str) -> Dict[str, Any]:
        """Synchronous wrapper for execute_mission"""
        return asyncio.run(self.execute_mission(mission_prompt))


# ======================== Usage Example ========================



--- FILE: config_api.py ---

import os
import logging
from enum import Enum
from google.cloud import secretmanager
import langchain
from langchain.cache import SQLiteCache


def get_secret(project_id: str, secret_id: str, version_id: str = "latest") -> str:
    """Pobiera wartość sekretu z Google Secret Manager."""
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"
    response = client.access_secret_version(request={"name": name})

    return response.payload.data.decode("UTF-8")


class ApiType(Enum):
    GOOGLE = "google"
    ANTHROPIC = "anthropic"
    OPENAI = "openai"

    def __str__(self):
        return self.value


LOCATION = "us-central1"
PROJECT_ID = "dark-data-discovery"

# ---------AGENTS--------:
MAIN_AGENT = "gemini-2.5-pro"
API_TYPE_GEMINI = str(ApiType.GOOGLE)

CRITIC_MODEL = "claude-3-7-sonnet-20250219"
ARCHITECT_MODEL = "claude-opus-4-1-20250805"
CODE_MODEL = "claude-sonnet-4-20250514"
QUICK_SMART_MODEL = "gemini-2.5-flash"

GPT_MODEL = "gpt-4o"  # Używamy gpt-4o jako odpowiednika "gpt-5"
API_TYPE_OPENAI = str(ApiType.OPENAI)

API_TYPE_SONNET = str(ApiType.ANTHROPIC)

LANGCHAIN_API_KEY = get_secret(PROJECT_ID, "LANGCHAIN_API_KEY")
ANTHROPIC_API_KEY = get_secret(PROJECT_ID, "ANTHROPIC_API_KEY")
TAVILY_API_KEY = get_secret(PROJECT_ID, "TAVILY_API_KEY")
OPENAI_API_KEY = get_secret(PROJECT_ID, "OPENAI_API_KEY")

MEMORY_ENGINE_DISPLAY_NAME = "memory-gamma-way"

INPUT_FILE_PATH = "gs://super_model/data/structural_data/synthetic_fraud_dataset.csv"

MAX_CORRECTION_ATTEMPTS = 5


os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_PROJECT"] = "Projekt Multi-Agent-System Dynamic-graphs"
os.environ["ANTHROPIC_API_KEY"] = ANTHROPIC_API_KEY
os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY
# os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

os.environ.setdefault("MOA_SANITY_PING", "0")
# ---cache-------
langchain.llm_cache = SQLiteCache(database_path=".langchain.db")


# FUNKCJA KONFIGURACYJNA AGENTOW AUTOGEN
def basic_config_agent(
    agent_name: str,
    api_type: str,
    location: str = None,
    project_id: str = None,
    api_key: str = None,
):
    try:
        configuration = {"model": agent_name}
        configuration.update({"api_type": api_type})
        if api_key:
            configuration["api_key"] = api_key
        if project_id:
            configuration["project_id"] = project_id
        if location:
            configuration["location"] = location

        logging.info(f"Model configuration: {configuration}")
        return [configuration]

    except Exception as e:
        logging.error(f"Failed to initialize Vertex AI or configure LLM: {e}")
        print(
            f"Error: Failed to initialize Vertex AI or configure LLM. Please check your project ID, region, and permissions. Details: {e}"
        )
        exit()



--- FILE: explainability_layer.py ---

import json
import re
from datetime import datetime
from typing import Any, Dict, List, Optional

from process_logger import log as process_log


class ExplainabilityLayer:
    """
    Dodaje wyjaśnialność do istniejącego systemu MOA bez większych zmian
    """

    @staticmethod
    def inject_explainability_prompt(base_prompt: str, agent_role: str) -> str:
        """
        Dodaje instrukcje wyjaśnialności do promptu - MODEL PRZEZ API TO ZROZUMIE
        """

        explainability_suffix = """

## EXPLAINABILITY REQUIREMENTS (MANDATORY)
You must include an additional JSON field "cognitive_trace" in your response with:

```json
"cognitive_trace": {
    "trigger_words": [/* actual words from prompt that influenced you */],
    "reasoning_chain": [/* your actual reasoning steps */],
    "confidence_per_decision": {
        /* your actual confidence values between 0-1 for each decision type */
        "node_selection": /* your confidence */,
        "edge_conditions": /* your confidence */,
        "overall_structure": /* your confidence */
    },
    "alternatives_considered": {
        "rejected_nodes": [/* nodes you considered but rejected with reasons */],
        "rejected_patterns": [/* patterns you didn't use and why */]
    },
    "key_influences": {
        "from_prompt": /* what specific instruction shaped this most */,
        "from_context": /* what contextual factor was crucial */,
        "from_role": /* how your assigned role affected this */
    },
    "uncertainty_points": [/* where you're least certain and why */],
    "word_choices": {
        /* actual words you chose and why */
    }
}
This is REQUIRED - include it after your main plan/response.
"""
        return base_prompt + explainability_suffix

    @staticmethod
    def extract_cognitive_trace(response: str) -> Optional[Dict]:
        """
        Wyciąga cognitive_trace z odpowiedzi modelu
        """
        try:
            # Szukaj cognitive_trace w odpowiedzi
            if isinstance(response, dict):
                return response.get("cognitive_trace")

            # Jeśli string, parsuj JSON
            if isinstance(response, str):
                # Usuń markdown jeśli jest
                cleaned = re.sub(r"```json?\s*|\s*```", "", response)
                parsed = json.loads(cleaned)
                return parsed.get("cognitive_trace")
        except:
            # Fallback - spróbuj znaleźć wzorzec
            pattern = r'"cognitive_trace":\s*\{[^}]+\}'
            match = re.search(pattern, str(response))
            if match:
                try:
                    return json.loads("{" + match.group() + "}")["cognitive_trace"]
                except:
                    pass
            return None

    @staticmethod
    def analyze_response_semantics(prompt: str, response: str, agent_name: str) -> Dict:
        """
        Analizuje związki semantyczne między promptem a odpowiedzią
        """
        analysis = {
            "agent": agent_name,
            "timestamp": datetime.now().isoformat(),
            "prompt_length": len(prompt),
            "response_length": len(response),
            "semantic_markers": {},
        }

        # Kluczowe słowa z promptu które pojawiły się w odpowiedzi
        prompt_keywords = set(re.findall(r"\b[A-Za-z_]+\b", prompt.lower()))
        response_keywords = set(re.findall(r"\b[A-Za-z_]+\b", response.lower()))

        analysis["semantic_markers"]["keyword_overlap"] = list(prompt_keywords & response_keywords)[
            :20
        ]
        analysis["semantic_markers"]["new_concepts"] = list(response_keywords - prompt_keywords)[
            :20
        ]

        # Wykryj wzorce decyzyjne
        if "error" in response.lower():
            analysis["semantic_markers"]["error_handling_focus"] = True
        if "rollback" in response.lower():
            analysis["semantic_markers"]["rollback_strategy"] = True
        if "optimiz" in response.lower():
            analysis["semantic_markers"]["optimization_focus"] = True

        return analysis

    @staticmethod
    def create_explainability_report(all_analyses: List[Dict]) -> Dict:
        """
        Tworzy raport wyjaśnialności dla całej debaty
        """
        report = {
            "debate_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "total_turns": len(all_analyses),
            "agents_involved": list(set(a["agent"] for a in all_analyses if "agent" in a)),
            "cognitive_patterns": {},
            "decision_evolution": [],
            "key_influences": {},
            "uncertainty_map": {},
        }

        # Agreguj cognitive traces
        for analysis in all_analyses:
            if "cognitive_trace" in analysis:
                trace = analysis["cognitive_trace"]
                agent = analysis.get("agent", "unknown")

                if agent not in report["cognitive_patterns"]:
                    report["cognitive_patterns"][agent] = []

                report["cognitive_patterns"][agent].append(
                    {
                        "triggers": trace.get("trigger_words", []),
                        "confidence": trace.get("confidence_per_decision", {}),
                        "alternatives": trace.get("alternatives_considered", {}),
                    }
                )

                # Mapuj niepewności
                for uncertainty in trace.get("uncertainty_points", []):
                    if uncertainty not in report["uncertainty_map"]:
                        report["uncertainty_map"][uncertainty] = []
                    report["uncertainty_map"][uncertainty].append(agent)

        return report


class ExplainabilityHooks:
    """
    Hooki do wstrzyknięcia w istniejący kod z minimalną ingerencją
    """

    def __init__(self):
        self.layer = ExplainabilityLayer()
        self.session_analyses = []

    def on_prompt_build(self, base_prompt: str, agent_role: str) -> str:
        """
        Hook wywoływany przy budowaniu promptu - DODAJ TO DO TWOJEJ METODY BUDOWANIA PROMPTÓW
        """
        return self.layer.inject_explainability_prompt(base_prompt, agent_role)

    def on_response_received(self, prompt: str, response: str, agent_name: str) -> Dict:
        """
        Hook wywoływany po otrzymaniu odpowiedzi - DODAJ TO PO OTRZYMANIU ODPOWIEDZI Z API
        """
        # Ekstraktuj cognitive trace
        cognitive_trace = self.layer.extract_cognitive_trace(response)

        # Analiza semantyczna
        semantic_analysis = self.layer.analyze_response_semantics(prompt, response, agent_name)

        # Połącz analizy
        full_analysis = {
            **semantic_analysis,
            "cognitive_trace": cognitive_trace,
            "raw_response_sample": (
                response[:500] if isinstance(response, str) else str(response)[:500]
            ),
        }

        # Zapisz do sesji
        self.session_analyses.append(full_analysis)

        # Loguj kluczowe informacje
        if cognitive_trace:
            process_log(
                f"[EXPLAINABILITY] {agent_name} confidence: {cognitive_trace.get('confidence_per_decision', {})}"
            )
            process_log(
                f"[EXPLAINABILITY] Key triggers: {cognitive_trace.get('trigger_words', [])[:5]}"
            )

        return full_analysis

    def generate_final_report(self) -> Dict:
        """
        Generuje końcowy raport wyjaśnialności
        """
        report = self.layer.create_explainability_report(self.session_analyses)

        # Zapisz do pliku
        report_file = f"explainability_reports/report_{report['debate_id']}.json"
        import os

        os.makedirs("explainability_reports", exist_ok=True)

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        process_log(f"[EXPLAINABILITY] Report saved to: {report_file}")
        return report



--- FILE: exporter_missions_gcs.py ---

# exporter_missions_gcs.py
from __future__ import annotations
import json, os, glob
from pathlib import Path
from typing import List, Tuple
from datetime import datetime

from google.cloud import storage
from datetime import datetime, timezone
# Re-use logiki z Twojej biblioteki (identyczne wyliczenia i pola!)
from exporter_missions_lib import (
    _to_str_content,
    _extract_plan,
    _build_txt,
    _build_transcript,
    _build_metrics,
    _ndjson_line,
)

def _parse_gs_uri(uri: str) -> Tuple[str, str]:
    if not uri.startswith("gs://"):
        raise ValueError(f"output_root_gcs must start with gs://, got: {uri}")
    rest = uri[5:]
    bucket, _, prefix = rest.partition("/")
    return bucket, prefix.strip("/")

def _mission_id_from_snapshot(snap: dict, fallback_path: Path) -> str:
    mid = snap.get("memory_id") or snap.get("mission_id")
    if isinstance(mid, str) and mid:
        return mid
    base = fallback_path.stem
    if base.lower().startswith("mission_"):
        return base
    # ostateczny fallback
    stamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    return f"mission_{stamp}"

def export_local_by_filename_date(
    input_dir: str,
    output_root_gcs: str,
    pattern: str = "*.json",
    skip_existing: bool = True,
) -> List[str]:
    """
    Eksportuje lokalne misje do GCS w formacie zgodnym z exporter_missions_lib:
      missions/<mission_id>/<mission_id>.txt
      missions/<mission_id>/<mission_id>.plan.json
      missions/<mission_id>/<mission_id>.transcript.json
      missions/<mission_id>/<mission_id>.metrics.json
      missions/<mission_id>/<mission_id>.ndjson   (1 linia na misję)

    Zwraca listę GCS URI do *.ndjson (po jednym na misję).
    """
    input_dir_p = Path(input_dir).resolve()
    files = sorted(
        glob.glob(os.path.join(str(input_dir_p), pattern)),
        key=lambda p: os.path.basename(p).split("_")[1:3]  # sort: YYYYMMDD, HHMMSS
    )

    bucket_name, root_prefix = _parse_gs_uri(output_root_gcs)
    client = storage.Client()
    bucket = client.bucket(bucket_name)

    ndjson_uris: List[str] = []

    for p in files:
        src = Path(p)
        try:
            snap = json.loads(src.read_text(encoding="utf-8"))
        except Exception as e:
            print(f"[EXPORT][SKIP] {src.name}: cannot load JSON ({e})")
            continue

        mission_id = _mission_id_from_snapshot(snap, src)
        plan = _extract_plan(snap)
        txt = _build_txt(snap, plan)
        transcript = _build_transcript(snap)
        metrics = _build_metrics(snap, plan)

        # Ścieżki w GCS – jak w Twojej bibliotece (katalog per misja)
        base_prefix = f"{root_prefix}/{mission_id}".strip("/")
        txt_path     = f"{base_prefix}/{mission_id}.txt"
        plan_path    = f"{base_prefix}/{mission_id}.plan.json"
        trans_path   = f"{base_prefix}/{mission_id}.transcript.json"
        metrics_path = f"{base_prefix}/{mission_id}.metrics.json"
        ndjson_path  = f"{base_prefix}/{mission_id}.ndjson"

        txt_uri     = f"gs://{bucket_name}/{txt_path}"
        plan_uri    = f"gs://{bucket_name}/{plan_path}"
        trans_uri   = f"gs://{bucket_name}/{trans_path}"
        metrics_uri = f"gs://{bucket_name}/{metrics_path}"
        ndjson_uri  = f"gs://{bucket_name}/{ndjson_path}"

        
        if skip_existing:
            
            metrics_blob = bucket.blob(metrics_path)
            ndjson_blob  = bucket.blob(ndjson_path)
            if metrics_blob.exists(client) or ndjson_blob.exists(client):
                print(f"[EXPORT][SKIP] {mission_id} już istnieje w GCS (metrics/ndjson).")
                continue
        
        
        # 1) Upload artefaktów
        bucket.blob(txt_path).upload_from_string(txt, content_type="text/plain; charset=utf-8")
        bucket.blob(plan_path).upload_from_string(
            json.dumps(plan, ensure_ascii=False, indent=2),
            content_type="application/json; charset=utf-8",
        )
        bucket.blob(trans_path).upload_from_string(
            json.dumps(transcript, ensure_ascii=False, indent=2),
            content_type="application/json; charset=utf-8",
        )
        bucket.blob(metrics_path).upload_from_string(
            json.dumps(metrics, ensure_ascii=False, indent=2),
            content_type="application/json; charset=utf-8",
        )

        # 2) Zbuduj linię NDJSON (identyczna z Twoją funkcją _ndjson_line)
        ndjson_line = _ndjson_line(
            mission_id=mission_id,
            txt_uri=txt_uri,
            plan_uri=plan_uri,
            transcript_uri=trans_uri,
            metrics_uri=metrics_uri,
            metrics=metrics,
        )

        # 3) Upload pojedynczego pliku NDJSON dla misji
        bucket.blob(ndjson_path).upload_from_string(
            ndjson_line + "\n",
            content_type="application/x-ndjson; charset=utf-8",
        )
        
        
        #zapisywanie indeksow
        
        
         # helper do slugów (zachowuje PL znaki, ogranicza długość)
        def _slug_u(text: str) -> str:
            t = (text or "").strip().lower()
            t = re.sub(r"\s+", "-", t)                         # spacje -> '-'
            t = re.sub(r"[^0-9A-Za-zĄĆĘŁŃÓŚŹŻąćęłńóśźż\-]+", "-", t)  # tylko sensowne znaki
            t = re.sub(r"-{2,}", "-", t).strip("-")
            return t[:120]

        # 1) Timestamp i identyfikatory
        ts_dt   = datetime.now(timezone.utc)                   # jeśli wolisz bez timezone: datetime.utcnow()
        ts_file = ts_dt.strftime("%Y%m%d_%H%M%S")              # do nazwy pliku
        ts_iso  = ts_dt.strftime("%Y-%m-%dT%H:%M:%SZ")         # do pola timestamp (ISO8601Z)

        # mission_id pełny vs skrócony (jak w Twoim załączniku)
        mid_full  = mission_id                                 # np. "mission_20250829_212413_92ed8ebc"
        mid_short = mission_id.replace("mission_", "")         # np. "20250829_212413_92ed8ebc"
        tail8     = mid_short[-8:] if len(mid_short) >= 8 else mid_short

        
        
        # --- SAFE BINDINGS: używaj *_val zamiast gołych nazw ---
        _locals = locals()
        _md = _locals.get("metadata") if isinstance(_locals.get("metadata"), dict) else {}

        approved_val     = _locals.get("approved", _md.get("approved", True))
        final_score_val  = _locals.get("final_score", _md.get("final_score"))
        nodes_count_val  = _locals.get("nodes_count", _md.get("nodes_count"))
        edges_count_val  = _locals.get("edges_count", _md.get("edges_count"))
        mission_type_val = _locals.get("mission_type", _md.get("mission_type", "general"))
        lang_val         = _locals.get("lang", _md.get("lang", "pl"))

        # tags jako lista
        _tags = _locals.get("tags", _md.get("tags", []))
        tags_list = list(_tags) if isinstance(_tags, (list, tuple)) else ([] if _tags is None else [str(_tags)])

        # flags jako dict + fallback z metadanych (has_* bywa trzymane płasko)
        flags_val = _locals.get("flags", {})
        if not isinstance(flags_val, dict):
            flags_val = {}
        flags_val = {
            "has_retry":        bool(flags_val.get("has_retry",        _md.get("has_retry", False))),
            "has_rollback":     bool(flags_val.get("has_rollback",     _md.get("has_rollback", False))),
            "has_optimization": bool(flags_val.get("has_optimization", _md.get("has_optimization", False))),
        }
        
        
        
        
        
        
        # 2) Źródło tytułu do display_id z metadanych (BEZ użycia 'mission')
        _display_src = ""
        try:
            if isinstance(metadata, dict):
                _display_src = (
                    metadata.get("mission_prompt")
                    or metadata.get("mission")
                    or metadata.get("title")
                    or ""
                )
        except NameError:
            _display_src = ""

        display_base = _slug_u(_display_src) if isinstance(_display_src, str) and _display_src.strip() else ""
        display_id   = f"{ts_file}-{display_base}-{tail8}" if display_base else f"{ts_file}-{tail8}"

        # 3) Lekki .txt jako content.uri (tak jak w załączniku)
        txt_name = f"{display_id}.txt"
        # UWAGA: 'root_prefix' to katalog dnia (ten sam, w którym lądują artefakty tej misji)
        txt_path = f"{root_prefix}/{txt_name}"
        
        txt_body = (
        f"mission_id: {mid_full}\n"
        f"timestamp:  {ts_iso}\n"
        f"approved:   {bool(approved_val)}\n"
        f"final_score:{final_score_val if final_score_val is not None else 'null'}\n"
        f"tags:       {', '.join(tags_list) if tags_list else ''}\n"
    )
        
        
        
        bucket.blob(txt_path).upload_from_string(txt_body, content_type="text/plain; charset=utf-8")
        txt_uri = f"gs://{bucket_name}/{txt_path}"

        # 4) Składamy dokument NDJSON 1:1 jak w załączniku
        tags_list = list(tags) if isinstance(tags, (list, tuple)) else ([] if tags is None else [str(tags)])
        has_retry = bool(flags.get("has_retry")) if isinstance(flags, dict) else False
        has_rb    = bool(flags.get("has_rollback")) if isinstance(flags, dict) else False
        has_opt   = bool(flags.get("has_optimization")) if isinstance(flags, dict) else False

        
        
        doc = {
        "id": mid_short,
        "structData": {
            "mission_id": mid_full,
            "timestamp": ts_iso,
            "mission_type": mission_type_val,
            "tags": tags_list,
            "outcome": "Success" if approved_val else ("Partial" if (final_score_val not in (None, 0)) else "Failure"),
            "final_score": float(final_score_val) if final_score_val is not None else None,
            "approved": bool(approved_val),
            "nodes_count": int(nodes_count_val) if nodes_count_val is not None else None,
            "edges_count": int(edges_count_val) if edges_count_val is not None else None,
            "has_retry": flags_val["has_retry"],
            "has_rollback": flags_val["has_rollback"],
            "has_optimization": flags_val["has_optimization"],
            "lang": lang_val,
            "display_id": display_id,
            "links": {
                "txt_uri": txt_uri,
                "plan_uri": plan_uri,
                "transcript_uri": transcript_uri,
                "metrics_uri": meta_uri,
            },
        },
        "content": {
            "mimeType": "text/plain",
            "uri": txt_uri,
        },
        }
        
        

        # 5) Jednowierszowy plik NDJSON do folderu index/
        index_dir  = f"{root_prefix}/index"    # jeśli chcesz top-level: index_dir = "index"
        index_path = f"{index_dir}/metadata_{ts_file}.ndjson"
        bucket.blob(index_path).upload_from_string(
            json.dumps(doc, ensure_ascii=False) + "\n",
            content_type="application/x-ndjson; charset=utf-8",
        )
        print(f"[INDEX] NDJSON -> gs://{bucket_name}/{index_path}")
        
        
        
        #koniec zapisu indeksu

        print(f"[EXPORT] {mission_id} -> {ndjson_uri}")
        ndjson_uris.append(ndjson_uri)

    return ndjson_uris



--- FILE: exporter_missions_lib.py ---

# exporter_missions_lib.py
# Eksporter misji – czysta funkcja do użycia w notebooku lub pipeline

from __future__ import annotations
import json, re, hashlib
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

FENCE_RE = re.compile(r"^```(?:json)?\s*|\s*```$", flags=re.IGNORECASE | re.MULTILINE)


def _strip_fences(text: str) -> str:
    return FENCE_RE.sub("", text or "").strip()


def _to_str_content(content: Any) -> str:
    if content is None:
        return ""
    s = (
        json.dumps(content, ensure_ascii=False)
        if isinstance(content, (dict, list))
        else str(content)
    )
    return _strip_fences(s)


def _iso_utc(ts: str | None) -> str:
    if not ts:
        return datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
    t = ts.replace(" ", "T")
    if "." in t:
        t = t.split(".")[0]
    return t if t.endswith("Z") else t + "Z"


def _hash(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()[:12]


def _ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def _extract_plan(snapshot: Dict[str, Any]) -> Dict[str, Any]:
    if isinstance(snapshot.get("final_plan"), dict):
        return snapshot["final_plan"]
    for it in snapshot.get("iterations") or []:
        for key in ("aggregator", "critic"):
            block = it.get(key, {}) or {}
            try:
                data = json.loads(_to_str_content(block.get("content", "")))
                if isinstance(data, dict) and "final_plan" in data:
                    return data["final_plan"]
                if isinstance(data, dict) and "plan_approved" in data:
                    return data["plan_approved"]
            except Exception:
                pass
    return {"entry_point": "", "nodes": [], "edges": []}


def _count_nodes_edges(plan: Dict[str, Any]) -> Tuple[int, int]:
    return len(plan.get("nodes") or []), len(plan.get("edges") or [])


def _approved(snapshot: Dict[str, Any]) -> bool:
    verdicts = [str(snapshot.get("verdict", ""))]
    for it in snapshot.get("iterations") or []:
        critic = it.get("critic", {}) or {}
        verdicts.append(str(critic.get("verdict", "")))
        if "zatwierdzony" in _to_str_content(critic.get("content", "")).lower():
            verdicts.append("ZATWIERDZONY")
    if "plan_zatwierdzony" in str(snapshot.get("decision_marker", "")).lower():
        verdicts.append("ZATWIERDZONY")
    return any("zatwierdzony" in v.lower() for v in verdicts)


def _derive_flags(snapshot: Dict[str, Any], plan: Dict[str, Any]) -> Dict[str, bool]:
    blob = "\n".join(
        [
            _to_str_content(snapshot.get("mission_prompt", "")),
            _to_str_content(snapshot.get("llm_generated_summary", "")),
            _to_str_content(snapshot.get("aggregator_reasoning", "")),
            _to_str_content(snapshot.get("identified_patterns", "")),
            json.dumps(plan, ensure_ascii=False),
        ]
    ).lower()
    return {
        "has_retry": ("retry" in blob or "ponow" in blob),
        "has_rollback": ("rollback" in blob or "wycof" in blob),
        "has_optimization": ("optimiz" in blob or "optymal" in blob),
    }


def _build_txt(snapshot: Dict[str, Any], plan: Dict[str, Any]) -> str:
    mission_id = (
        snapshot.get("memory_id")
        or snapshot.get("mission_id")
        or f"mission_{_hash(json.dumps(snapshot, ensure_ascii=False))}"
    )
    timestamp = _iso_utc(snapshot.get("timestamp"))
    mtype = snapshot.get("mission_type", "")
    tags = ", ".join(snapshot.get("tags") or [])
    outcome = str(snapshot.get("outcome", ""))
    score = snapshot.get("final_score", snapshot.get("score", ""))
    verdict = "ZATWIERDZONY" if _approved(snapshot) else ""
    nodes_cnt, edges_cnt = _count_nodes_edges(plan)
    prompt = _to_str_content(snapshot.get("mission_prompt", ""))
    llm_summary = _to_str_content(snapshot.get("llm_generated_summary", ""))

    txt = []
    txt.append(f"# Mission: {prompt[:80] or '—'}")
    txt.append(f"ID: {mission_id}")
    txt.append(f"Timestamp: {timestamp}")
    txt.append(f"Type: {mtype}")
    txt.append(f"Tags: {tags}")
    txt.append(f"Outcome | Score | Verdict: {outcome} | {score} | {verdict}\n")
    txt.append("## Executive Summary")
    txt.append(llm_summary or "Brak skrótu; patrz szczegóły planu i ryzyka.\n")
    txt.append("## Final Plan (skrót)")
    txt.append(f'Entry: {plan.get("entry_point","")}')
    node_names = [n.get("name") for n in plan.get("nodes", []) if isinstance(n, dict)]
    txt.append(f"Węzły ({nodes_cnt}): " + ", ".join(node_names))
    return "\n".join(txt).strip() + "\n"


def _build_transcript(snapshot: Dict[str, Any]) -> Dict[str, Any]:
    def norm(m: Dict[str, Any]) -> Dict[str, Any]:
        m2 = dict(m)
        m2["content"] = _to_str_content(m.get("content", ""))
        return m2

    out = {
        "mission_id": snapshot.get("memory_id") or snapshot.get("mission_id"),
        "iterations": [],
        "full_transcript": [],
    }
    for it in snapshot.get("iterations") or []:
        it_out = {}
        for k, v in it.items():
            if k == "proposers":
                it_out[k] = [
                    {
                        "agent": p.get("agent"),
                        "content": _to_str_content(p.get("content", "")),
                    }
                    for p in (v or [])
                ]
            elif k in ("aggregator", "critic"):
                block = v or {}
                it_out[k] = {"content": _to_str_content(block.get("content", ""))}
            else:
                it_out[k] = v
        out["iterations"].append(it_out)
    out["full_transcript"] = [norm(m) for m in (snapshot.get("full_transcript") or [])]
    return out


def _build_metrics(snapshot: Dict[str, Any], plan: Dict[str, Any]) -> Dict[str, Any]:
    mission_id = snapshot.get("memory_id") or snapshot.get("mission_id")
    flags = _derive_flags(snapshot, plan)
    nodes_cnt, edges_cnt = _count_nodes_edges(plan)
    return {
        "mission_id": mission_id,
        "timestamp": _iso_utc(snapshot.get("timestamp")),
        "mission_type": snapshot.get("mission_type"),
        "tags": snapshot.get("tags") or [],
        "outcome": snapshot.get("outcome"),
        "final_score": snapshot.get("final_score", snapshot.get("score")),
        "approved": _approved(snapshot),
        "nodes_count": nodes_cnt,
        "edges_count": edges_cnt,
        **flags,
        "lang": snapshot.get("lang", "pl"),
    }


def _ndjson_line(
    mission_id: str,
    txt_uri: str,
    plan_uri: str,
    transcript_uri: str,
    metrics_uri: str,
    metrics: Dict[str, Any],
) -> str:
    return json.dumps(
        {
            "id": mission_id,
            "structData": {
                **metrics,
                "links": {
                    "plan_uri": plan_uri,
                    "transcript_uri": transcript_uri,
                    "metrics_uri": metrics_uri,
                },
            },
            "content": {"mimeType": "text/plain", "uri": txt_uri},
        },
        ensure_ascii=False,
    )


def process_file(
    src_json: Path, input_dir: Path, out_dir: Path, gcs_prefix: str
) -> List[str]:
    rel = src_json.relative_to(input_dir)
    snap = json.loads(src_json.read_text(encoding="utf-8"))
    mission_id = (
        snap.get("memory_id")
        or snap.get("mission_id")
        or f"mission_{_hash(json.dumps(snap, ensure_ascii=False))}"
    )
    plan = _extract_plan(snap)
    txt = _build_txt(snap, plan)
    transcript = _build_transcript(snap)
    metrics = _build_metrics(snap, plan)
    rel_dir = rel.parent
    base = out_dir / rel_dir / mission_id
    _ensure_dir(base)
    (base / f"{mission_id}.txt").write_text(txt, encoding="utf-8")
    (base / f"{mission_id}.plan.json").write_text(
        json.dumps(plan, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    (base / f"{mission_id}.transcript.json").write_text(
        json.dumps(transcript, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    (base / f"{mission_id}.metrics.json").write_text(
        json.dumps(metrics, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    gcs_base = f"{gcs_prefix}/{rel_dir.as_posix()}/{mission_id}"
    return [
        _ndjson_line(
            mission_id,
            f"{gcs_base}/{mission_id}.txt",
            f"{gcs_base}/{mission_id}.plan.json",
            f"{gcs_base}/{mission_id}.transcript.json",
            f"{gcs_base}/{mission_id}.metrics.json",
            metrics,
        )
    ]


def export_missions(
    input_dir: str,
    out_dir: str,
    gcs_prefix: str,
    pattern="**/*.json",
    ndjson_out="metadata.ndjson",
):
    input_dir = Path(input_dir).resolve()
    out_dir = Path(out_dir).resolve()
    _ensure_dir(out_dir)
    files = sorted(input_dir.glob(pattern))
    if not files:
        print("Brak plików JSON do przetworzenia.")
        return
    lines: List[str] = []
    for f in files:
        try:
            lines.extend(process_file(f, input_dir, out_dir, gcs_prefix))
        except Exception as e:
            print(f"[WARN] Pomiń {f}: {e}")
    ndjson_path = out_dir / ndjson_out
    ndjson_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
    print(f"OK. Artefakty w: {out_dir}\nNDJSON: {ndjson_path}")



--- FILE: extended_llm_wrapper.py ---

"""
Rozszerzony wrapper LLM z bardziej realistycznymi dummy responses dla różnych ról
"""

import json
import random
from typing import Dict, Any


class ExtendedLLMWrapper:
    """
    Rozszerzona wersja wrappera z różnorodnymi odpowiedziami dla demo
    """

    @staticmethod
    def generate_dummy_response(model_name: str, prompt: str) -> str:
        """Generuje różne odpowiedzi w zależności od typu agenta"""

        # Sprawdź typ agenta na podstawie nazwy modelu lub promptu
        if "causal" in model_name.lower() or "Causal" in prompt:
            return ExtendedLLMWrapper._causal_analyst_response()
        elif "creative" in model_name.lower() or "Creative" in prompt:
            return ExtendedLLMWrapper._creative_planner_response()
        elif "risk" in model_name.lower() or "Risk" in prompt:
            return ExtendedLLMWrapper._risk_analyst_response()
        elif "aggregator" in model_name.lower() or "Aggregator" in prompt:
            return ExtendedLLMWrapper._aggregator_response(prompt)
        elif "critic" in model_name.lower() or "Critic" in prompt:
            return ExtendedLLMWrapper._critic_response(prompt)
        else:
            return ExtendedLLMWrapper._default_response()

    @staticmethod
    def _causal_analyst_response() -> str:
        """Odpowiedź analityka przyczynowego"""
        response = {
            "thought_process": [
                "Analizuję potencjalne relacje przyczynowe w przepływie danych",
                "Identyfikuję zmienne confounding i mediatory",
                "Projektuję DAG (Directed Acyclic Graph) dla workflow",
            ],
            "plan": {
                "entry_point": "validate_data",
                "nodes": [
                    {"name": "validate_data", "implementation": "validate_data"},
                    {"name": "check_quality", "implementation": "check_quality"},
                    {
                        "name": "discover_causality",
                        "implementation": "discover_causality",
                    },
                    {"name": "error_handler", "implementation": "error_handler"},
                    {"name": "validate_model", "implementation": "validate_model"},
                    {"name": "generate_report", "implementation": "generate_report"},
                ],
                "edges": [
                    {"from": "validate_data", "to": "check_quality"},
                    {"from": "check_quality", "to": "discover_causality"},
                    {
                        "from": "discover_causality",
                        "to": "validate_model",
                        "condition": "check_success",
                    },
                    {
                        "from": "discover_causality",
                        "to": "error_handler",
                        "condition": "check_error",
                    },
                    {"from": "error_handler", "to": "discover_causality"},
                    {"from": "validate_model", "to": "generate_report"},
                ],
            },
            "confidence": 0.85,
            "key_innovations": [
                "Dodanie pętli retry dla discover_causality",
                "Walidacja jakości przed analizą przyczynową",
            ],
            "risk_mitigation": {
                "data_quality": "Podwójna walidacja przed analizą",
                "algorithm_failure": "Error handler z retry mechanism",
            },
        }
        return json.dumps(response)

    @staticmethod
    def _creative_planner_response() -> str:
        """Odpowiedź kreatywnego planera"""
        response = {
            "thought_process": [
                "Myślę nieszablonowo - co gdyby pipeline sam się optymalizował?",
                "Inspiracja z natury: mrówki znajdują optymalną ścieżkę",
                "Dodaję element adaptacyjności i uczenia się",
            ],
            "plan": {
                "entry_point": "load_data",
                "nodes": [
                    {"name": "load_data", "implementation": "load_data"},
                    {"name": "clean_data", "implementation": "clean_data"},
                    {
                        "name": "optimize_performance",
                        "implementation": "optimize_performance",
                    },
                    {
                        "name": "discover_causality",
                        "implementation": "discover_causality",
                    },
                    {"name": "train_model", "implementation": "train_model"},
                    {"name": "notify_user", "implementation": "notify_user"},
                ],
                "edges": [
                    {"from": "load_data", "to": "clean_data"},
                    {"from": "clean_data", "to": "optimize_performance"},
                    {"from": "optimize_performance", "to": "discover_causality"},
                    {"from": "discover_causality", "to": "train_model"},
                    {"from": "train_model", "to": "notify_user"},
                ],
            },
            "confidence": 0.75,
            "key_innovations": [
                "Samooptymalizacja pipeline'u",
                "Proaktywne powiadomienia użytkownika",
                "Adaptacyjne dostosowanie do typu danych",
            ],
            "risk_mitigation": {
                "performance": "Continuous optimization",
                "user_experience": "Real-time notifications",
            },
        }
        return json.dumps(response)

    @staticmethod
    def _risk_analyst_response() -> str:
        """Odpowiedź analityka ryzyka"""
        response = {
            "thought_process": [
                "Identyfikuję wszystkie możliwe punkty awarii",
                "Analizuję cascading failures",
                "Projektuję redundancję i fallback paths",
            ],
            "plan": {
                "entry_point": "validate_data",
                "nodes": [
                    {"name": "validate_data", "implementation": "validate_data"},
                    {"name": "clean_data", "implementation": "clean_data"},
                    {"name": "check_quality", "implementation": "check_quality"},
                    {
                        "name": "discover_causality",
                        "implementation": "discover_causality",
                    },
                    {"name": "error_handler", "implementation": "error_handler"},
                    {"name": "rollback", "implementation": "rollback"},
                    {"name": "validate_model", "implementation": "validate_model"},
                    {"name": "generate_report", "implementation": "generate_report"},
                ],
                "edges": [
                    {"from": "validate_data", "to": "clean_data"},
                    {"from": "clean_data", "to": "check_quality"},
                    {
                        "from": "check_quality",
                        "to": "discover_causality",
                        "condition": "quality_ok",
                    },
                    {
                        "from": "check_quality",
                        "to": "rollback",
                        "condition": "quality_fail",
                    },
                    {
                        "from": "discover_causality",
                        "to": "validate_model",
                        "condition": "success",
                    },
                    {
                        "from": "discover_causality",
                        "to": "error_handler",
                        "condition": "error",
                    },
                    {
                        "from": "error_handler",
                        "to": "rollback",
                        "condition": "cannot_recover",
                    },
                    {
                        "from": "error_handler",
                        "to": "discover_causality",
                        "condition": "can_retry",
                    },
                    {"from": "validate_model", "to": "generate_report"},
                    {"from": "rollback", "to": "generate_report"},
                ],
            },
            "confidence": 0.90,
            "key_innovations": [
                "Comprehensive error handling",
                "Multiple fallback paths",
                "Quality gates at critical points",
            ],
            "risk_mitigation": {
                "data_corruption": "Rollback mechanism",
                "algorithm_failure": "Multiple retry with degradation",
                "quality_issues": "Early detection and abort",
            },
        }
        return json.dumps(response)

    @staticmethod
    def _aggregator_response(prompt: str) -> str:
        """Odpowiedź agregatora - synteza propozycji"""
        # Sprawdź iterację jeśli jest w prompcie
        iteration = 1
        if "ITERATION:" in prompt:
            try:
                iteration = int(prompt.split("ITERATION:")[1].split("/")[0].strip())
            except:
                pass

        response = {
            "thought_process": [
                "Analizuję siły każdej propozycji",
                "Identyfikuję synergie między podejściami",
                "Łączę najlepsze elementy w spójną całość",
            ],
            "final_plan": {
                "entry_point": "validate_data",
                "nodes": [
                    {"name": "validate_data", "implementation": "validate_data"},
                    {"name": "clean_data", "implementation": "clean_data"},
                    {"name": "check_quality", "implementation": "check_quality"},
                    {
                        "name": "optimize_performance",
                        "implementation": "optimize_performance",
                    },
                    {
                        "name": "discover_causality",
                        "implementation": "discover_causality",
                    },
                    {"name": "error_handler", "implementation": "error_handler"},
                    {"name": "rollback", "implementation": "rollback"},
                    {"name": "train_model", "implementation": "train_model"},
                    {"name": "validate_model", "implementation": "validate_model"},
                    {"name": "generate_report", "implementation": "generate_report"},
                    {"name": "notify_user", "implementation": "notify_user"},
                ],
                "edges": [
                    {"from": "validate_data", "to": "clean_data"},
                    {"from": "clean_data", "to": "check_quality"},
                    {
                        "from": "check_quality",
                        "to": "optimize_performance",
                        "condition": "quality_ok",
                    },
                    {
                        "from": "check_quality",
                        "to": "rollback",
                        "condition": "quality_fail",
                    },
                    {"from": "optimize_performance", "to": "discover_causality"},
                    {
                        "from": "discover_causality",
                        "to": "train_model",
                        "condition": "success",
                    },
                    {
                        "from": "discover_causality",
                        "to": "error_handler",
                        "condition": "error",
                    },
                    {
                        "from": "error_handler",
                        "to": "rollback",
                        "condition": "max_retries",
                    },
                    {
                        "from": "error_handler",
                        "to": "discover_causality",
                        "condition": "can_retry",
                    },
                    {"from": "train_model", "to": "validate_model"},
                    {"from": "validate_model", "to": "generate_report"},
                    {"from": "generate_report", "to": "notify_user"},
                ],
            },
            "synthesis_reasoning": "Połączyłem solidną obsługę błędów od Risk Analyst, innowacyjną optymalizację od Creative Planner, i rygorystyczną walidację od Causal Analyst",
            "component_sources": {
                "Causal Analyst": ["validate_data", "check_quality", "validate_model"],
                "Creative Planner": ["optimize_performance", "notify_user"],
                "Risk Analyst": ["error_handler", "rollback", "conditional_edges"],
            },
            "confidence_score": 0.80 + iteration * 0.05,  # Rośnie z iteracjami
            "improvements": [
                "Dodanie cache dla powtarzalnych operacji",
                "Implementacja progressive enhancement",
                "Monitoring w czasie rzeczywistym",
            ],
        }
        return json.dumps(response)

    @staticmethod
    def _critic_response(prompt: str) -> str:
        """Odpowiedź krytyka - ocena planu"""
        # Sprawdź iterację
        iteration = 1
        if "ITERATION:" in prompt:
            try:
                iteration = int(prompt.split("ITERATION:")[1].split("/")[0].strip())
            except:
                pass

        # Dostosuj ocenę do iteracji
        base_score = 60 + iteration * 8
        approved = base_score >= 75 or iteration >= 4

        response = {
            "approved": approved,
            "score": min(base_score + random.randint(-5, 10), 95),
            "strengths": [
                "Comprehensive error handling",
                "Good balance between robustness and efficiency",
                "Clear separation of concerns",
                "Innovative optimization approach",
            ][
                : 2 + iteration
            ],  # Więcej mocnych stron w późniejszych iteracjach
            "weaknesses": [
                "Missing parallelization opportunities",
                "No caching mechanism",
                "Limited monitoring capabilities",
                "Could benefit from more granular error types",
            ][
                iteration - 1 :
            ],  # Mniej słabości w późniejszych iteracjach
            "feedback": f"Plan shows {'significant' if iteration > 2 else 'good'} improvement. {'Ready for deployment.' if approved else 'Further refinement needed.'}",
            "improvements": (
                [
                    "Add parallel processing for independent steps",
                    "Implement result caching",
                    "Add detailed logging and monitoring",
                    "Consider adding A/B testing capability",
                ][iteration - 1 :]
                if not approved
                else []
            ),
        }

        # KLUCZOWA ZMIANA: Dodaj frazę "PLAN_ZATWIERDZONY" jeśli zatwierdzamy
        response_json = json.dumps(response)

        if approved:
            # Dodaj magiczną frazę PO JSONie
            response_json += "\n\nPLAN_ZATWIERDZONY"

        return response_json

    @staticmethod
    def _default_response() -> str:
        """Domyślna odpowiedź"""
        response = {
            "thought_process": ["Analyzing task", "Creating plan"],
            "plan": {
                "entry_point": "load_data",
                "nodes": [
                    {"name": "load_data", "implementation": "load_data"},
                    {"name": "process", "implementation": "clean_data"},
                    {"name": "output", "implementation": "generate_report"},
                ],
                "edges": [
                    {"from": "load_data", "to": "process"},
                    {"from": "process", "to": "output"},
                ],
            },
            "confidence": 0.7,
        }
        return json.dumps(response)


# Zastąp oryginalną klasę LLMWrapper
import llm_wrapper

original_call = llm_wrapper.LLMWrapper.__call__


def enhanced_call(self, prompt: str) -> str:
    """Rozszerzone wywołanie z lepszymi dummy responses"""
    if self.provider == "dummy":
        return ExtendedLLMWrapper.generate_dummy_response(self.model_name, prompt)
    else:
        return original_call(self, prompt)


# Monkey-patch oryginalnej klasy
llm_wrapper.LLMWrapper.__call__ = enhanced_call



--- FILE: llm_wrapper.py ---

"""
Wrapper modeli LLM. Umożliwia łatwą zamianę źródła modelu (np. OpenAI, lokalny model itp.).
W tym przykładzie implementujemy klasę `LLMWrapper`, która w trybie demonstracyjnym
generuje sztuczną odpowiedź. Aby użyć prawdziwego modelu (np. GPT‑5), należy
uzupełnić implementację wywołania API w metodzie `__call__`.
"""

import os
import json


class LLMWrapper:
    def __init__(
        self,
        provider: str,
        model_name: str,
        api_key_env: str = None,
        temperature: float = 0.5,
    ):
        """
        :param provider: dostawca modelu, np. "openai" lub "dummy" dla demonstracji
        :param model_name: nazwa modelu u dostawcy
        :param api_key_env: nazwa zmiennej środowiskowej z kluczem API
        :param temperature: parametr kreatywności dla modeli typu GPT
        """
        self.provider = provider
        self.model_name = model_name
        self.temperature = temperature
        self.api_key = os.environ.get(api_key_env) if api_key_env else None

    def __call__(self, prompt: str) -> str:
        """
        Zwraca odpowiedź modelu na dany prompt. W wersji demonstracyjnej,
        jeśli provider to "dummy", generuje prosty plan w formacie JSON.
        W przeciwnym razie wymaga zaimplementowania wywołania API.
        """
        if self.provider == "dummy":
            # Zwróć przykładowy JSON jako ciąg znaków
            response = {
                "thought_process": ["Analiza zadania", "Propozycja rozwiązania"],
                "plan": {
                    "entry_point": "start",
                    "nodes": [
                        {"name": "start", "implementation": "init_task"},
                        {"name": "finish", "implementation": "end_task"},
                    ],
                    "edges": [{"from": "start", "to": "finish"}],
                },
                "confidence": 0.85,
            }
            return json.dumps(response)
        elif self.provider == "openai":
            # Przykład wywołania OpenAI ChatCompletion – wymaga biblioteki openai i klucza API
            try:
                import openai  # zaimportuj wewnątrz, aby uniknąć zależności dla dummy
            except ImportError:
                raise RuntimeError(
                    "Biblioteka openai nie jest zainstalowana. Zainstaluj ją lub użyj provider='dummy'."
                )
            if not self.api_key:
                raise RuntimeError(
                    "Brak klucza API. Ustaw zmienną środowiskową lub przekaż api_key_env."
                )
            openai.api_key = self.api_key
            # Buduj listę wiadomości zgodnie z API ChatCompletion
            messages = [
                {"role": "system", "content": "You are an advanced planning agent."},
                {"role": "user", "content": prompt},
            ]
            response = openai.ChatCompletion.create(
                model=self.model_name, messages=messages, temperature=self.temperature
            )
            return response.choices[0].message["content"]
        else:
            raise NotImplementedError(
                f"Provider '{self.provider}' nie jest obsługiwany."
            )



--- FILE: memory_helpers.py ---

# memory_helpers.py

from datetime import datetime
import json
import uuid
from typing import Any, Dict, List, Optional, Tuple

from google.cloud import storage


# ---------- Generatory i serializacja ----------

def gen_mission_id() -> str:
    """Generuje unikalny identyfikator misji."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    short = uuid.uuid4().hex[:6]
    return f"mission_{timestamp}_{short}"


def safe_json_dumps(obj: Any) -> str:
    """Bezpieczne serializowanie do JSON; w razie błędów serializuje repr."""
    try:
        return json.dumps(obj, ensure_ascii=False, indent=2)
    except Exception:
        return json.dumps(str(obj), ensure_ascii=False, indent=2)


def count_nodes_edges(plan: Dict) -> Tuple[int, int]:
    """Zlicza liczbę węzłów i krawędzi w finalnym planie."""
    nodes = plan.get("nodes", []) if isinstance(plan, dict) else []
    edges = plan.get("edges", []) if isinstance(plan, dict) else []
    return (len(nodes), len(edges))


def infer_flags_and_tags(plan: Dict, transcript: List[str]) -> Tuple[Dict[str, bool], List[str]]:
    """Wykrywa flagi (retry, rollback, optimize) i generuje listę tagów."""
    joined = " ".join(str(x).lower() for x in transcript)[:50000]
    flags = {
        "has_retry": any(k in joined for k in ["retry", "ponów", "ponowienie", "backoff"]),
        "has_rollback": "rollback" in joined,
        "has_optimization": any(k in joined for k in ["optimiz", "optymaliz"]),
    }
    tags: List[str] = []
    if flags["has_retry"]:
        tags.append("retry")
    if flags["has_rollback"]:
        tags.append("rollback")
    if flags["has_optimization"]:
        tags.append("optimize")
    return flags, tags


# ---------- Główna funkcja zapisu ----------

def save_mission_to_gcs(
    bucket_name: str,
    base_prefix: str,
    mission: str,
    final_plan: Dict,
    all_messages: List[Dict],
    orchestrator_state: Dict,
    approved: bool = True,
    final_score: Optional[float] = None
) -> str:
    
    
    print(">>> save_mission_to_gcs CALLED")
    print(">>> module:", __name__)
    print(">>> file:  ", __file__)
    logging.warning("save_mission_to_gcs CALLED from %s", __file__)
    """
    Zapisuje plan, transkrypt i metadane do Google Cloud Storage w strukturze:
      gs://{bucket}/{base_prefix}/{mission_id}/plan.json
      gs://{bucket}/{base_prefix}/{mission_id}/transcript.jsonl
      gs://{bucket}/{base_prefix}/{mission_id}/metadata.json

    Zwraca mission_id.
    """
    # 1. Id i ścieżki
    mission_id = gen_mission_id()
    base_path = f"{base_prefix}/{mission_id}"
    plan_path = f"{base_path}/plan.json"
    transcript_path = f"{base_path}/transcript.jsonl"
    meta_path = f"{base_path}/metadata.json"

    plan_uri = f"gs://{bucket_name}/{plan_path}"
    transcript_uri = f"gs://{bucket_name}/{transcript_path}"
    meta_uri = f"gs://{bucket_name}/{meta_path}"

    # 2. Przygotuj dane
    nodes_count, edges_count = count_nodes_edges(final_plan)

    # Zamień transkrypt na JSONL (po jednej linii na wiadomość)
    transcript_lines: List[str] = []
    transcript_texts: List[str] = []
    for m in all_messages:
        mm = dict(m)
        c = mm.get("content")
        transcript_texts.append(c if isinstance(c, str) else safe_json_dumps(c))
        # Serializacja do JSONL – konwertuj content na string jeśli to np. dict
        if not isinstance(c, (str, dict)):
            mm["content"] = str(c)
        transcript_lines.append(safe_json_dumps(mm))

    flags, tags = infer_flags_and_tags(final_plan, transcript_texts)

    metadata = {
        "mission_id": mission_id,
        "mission_prompt": mission,
        "approved": approved,
        "final_score": float(final_score) if final_score is not None else None,
        "nodes_count": nodes_count,
        "edges_count": edges_count,
        "has_optimization": flags["has_optimization"],
        "has_rollback": flags["has_rollback"],
        "has_retry": flags["has_retry"],
        "tags": tags,
        "orchestrator_state": orchestrator_state or {},
        "timestamp": datetime.now().isoformat(),
        "links": {
            "plan_uri": plan_uri,
            "transcript_uri": transcript_uri,
            "metadata_uri": meta_uri,
        },
        "preview": {
            "entry_point": (final_plan or {}).get("entry_point"),
        },
    }

    # 3. Zapis do GCS
    client = storage.Client()
    bucket = client.bucket(bucket_name)

    bucket.blob(plan_path).upload_from_string(
        safe_json_dumps(final_plan), content_type="application/json; charset=utf-8"
    )
    bucket.blob(transcript_path).upload_from_string(
        "\n".join(transcript_lines), content_type="application/x-ndjson; charset=utf-8"
    )
    bucket.blob(meta_path).upload_from_string(
        safe_json_dumps(metadata), content_type="application/json; charset=utf-8"
    )
    
    
    
    #zapisywanie indeksow
    ts_dt = datetime.now(timezone.utc)
    ts_file = ts_dt.strftime("%Y%m%d_%H%M%S")
    ts_iso  = ts_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

    # preview.txt (content.uri)
    preview_txt = (
        f"mission_id: {mission_id}\n"
        f"timestamp:  {ts_iso}\n"
        f"approved:   {bool(approved)}\n"
        f"final_score:{final_score if final_score is not None else 'null'}\n"
        f"tags:       {', '.join(tags) if tags else ''}\n"
    )
    preview_path = f"{base_path}/preview.txt"
    bucket.blob(preview_path).upload_from_string(
        preview_txt, content_type="text/plain; charset=utf-8"
    )
    preview_uri = f"gs://{bucket_name}/{preview_path}"

    # dokument NDJSON (1 linia)
    ndjson_doc = {
        "id": f"{ts_file}_{(mission_id[-8:] if len(mission_id) >= 8 else mission_id)}",
        "structData": {
            "mission_id": mission_id,
            "timestamp": ts_iso,
            "mission_type": "general",
            "tags": tags,
            "outcome": "Success" if approved else "Partial" if final_score else "Failure",
            "final_score": float(final_score) if final_score is not None else None,
            "approved": bool(approved),
            "nodes_count": int(nodes_count),
            "edges_count": int(edges_count),
            "has_retry": bool(flags.get("has_retry")) if isinstance(flags, dict) else False,
            "has_rollback": bool(flags.get("has_rollback")) if isinstance(flags, dict) else False,
            "has_optimization": bool(flags.get("has_optimization")) if isinstance(flags, dict) else False,
            "lang": "pl",
            "display_id": f"{ts_file}-{mission_id}",
            "links": {
                "txt_uri": preview_uri,
                "plan_uri": plan_uri,
                "transcript_uri": transcript_uri,
                "metrics_uri": meta_uri,
                "metadata_uri": meta_uri,
            },
        },
        "content": {
            "mimeType": "text/plain",
            "uri": preview_uri,
        },
    }

    # zapis NDJSON pod {base_prefix}/index/
    index_dir  = f"{base_prefix}/index"
    index_path = f"{index_dir}/metadata_{ts_file}.ndjson"
    bucket.blob(index_path).upload_from_string(
        json.dumps(ndjson_doc, ensure_ascii=False) + "\n",
        content_type="application/x-ndjson; charset=utf-8",
    )

    # twardy log z pełnym URI
    ndjson_uri = f"gs://{bucket_name}/{index_path}"
    print("NDJSON ->", ndjson_uri)
    logging.warning("NDJSON wrote to %s", ndjson_uri)

    # mały kanarek, żeby łatwo złapać prefiks (ten sam katalog co NDJSON)
    canary_path = f"{index_dir}/_canary_{ts_file}.txt"
    bucket.blob(canary_path).upload_from_string(
        f"ok {ts_iso} mission_id={mission_id}",
        content_type="text/plain; charset=utf-8",
    )
    print("CANARY ->", f"gs://{bucket_name}/{canary_path}")
    
    #koniec zapisu indeksow

    logging.getLogger(__name__).info(f"[MEMORY:GCS] Saved mission {mission_id} at {plan_uri}")
    return mission_id



--- FILE: memory_system.py ---

"""
System pamięci kontekstowej z uczeniem się z poprzednich iteracji
"""

from typing import Dict, List, Any, Optional
from datetime import datetime
import json
import numpy as np
from collections import deque
import os

# Zewnętrzne biblioteki do obliczania podobieństwa tekstu
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Lokalny logger procesu
from process_logger import log as process_log


import re, unicodedata
from autogen_vertex_mcp_system_claude import SystemConfig, VertexSearchTool


def _to_text_safe(x) -> str:
    if x is None:
        return ""
    if isinstance(x, str):
        return x
    try:
        return json.dumps(x, ensure_ascii=False)
    except Exception:
        return str(x)


def _slugify(text: str, maxlen: int = 60) -> str:
    text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode()
    text = re.sub(r"[^a-zA-Z0-9]+", "-", text).strip("-").lower()
    return text[:maxlen] or "mission"


def _gcs_upload_json(bucket_name: str, blob_name: str, obj: dict):
    """Wrzuca JSON do GCS pod gs://{bucket_name}/{blob_name}"""
    try:
        from google.cloud import storage
    except ImportError:
        raise RuntimeError(
            "Brak pakietu google-cloud-storage. Zainstaluj: pip install google-cloud-storage"
        )
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    blob.cache_control = "no-cache"
    blob.upload_from_string(
        data=json.dumps(obj, ensure_ascii=False, indent=2),
        content_type="application/json",
    )


class ContextMemory:
    def __init__(
        self,
        max_episodes: int = 100,
        gcs_bucket: str | None = None,
        gcs_prefix: str = "",
    ):
        # Existing
        self.episodes = deque(maxlen=max_episodes)
        self.learned_patterns = {}
        self.successful_strategies = []

        # NOWE - Pełne dane misji
        self.full_mission_records = []  # Bez limitu - wszystko zapisujemy
        self.mission_index = {}  # Szybkie wyszukiwanie po ID

        self._load_persistent_memory()
        self.gcs_bucket = gcs_bucket
        self.gcs_prefix = (gcs_prefix or "").strip().strip("/")
        self.use_gcs = bool(self.gcs_bucket)

        self.iteration_feedback = []
        self.last_feedback = ""
        
        
    #poprawki
    
        
    def add_iteration_feedback(self, iteration: int, feedback: str, timestamp: datetime):
        """
        Zapisuje feedback z iteracji do pamięci operacyjnej + lekki zapis lokalny.
        Orchestrator woła to po każdej odpowiedzi Critica.
        """
        try:
            entry = {
                "iteration": int(iteration),
                "feedback": str(feedback),
                "timestamp": timestamp.isoformat(),
            }
            # w RAM
            if not hasattr(self, "iteration_feedback"):
                self.iteration_feedback = []
            self.iteration_feedback.append(entry)
            # do szybkiego kontekstu
            self.last_feedback = entry["feedback"]

            # lekki zapis lokalny (bez zależności od pełnego mission_id)
            os.makedirs("memory/iterations", exist_ok=True)
            with open(f"memory/iterations/iter_{iteration:03d}.json", "w", encoding="utf-8") as f:
                json.dump(entry, f, ensure_ascii=False, indent=2)

            process_log(f"[MEMORY] add_iteration_feedback(iter={iteration}) ok")
        except Exception as e:
            process_log(f"[MEMORY ERROR] add_iteration_feedback failed: {e}")

    def get_relevant_context(self, mission: str) -> Dict[str, Any]:
        """
        Zwraca wstrzyknięcie kontekstu dla promptów (to woła orchestrator).
        Minimalnie: rekomendacje, pułapki, ostatni feedback.
        Wersja bez TF-IDF: heurystyki + ostatnie sukcesy.
        """
        context = {
            "recommended_strategies": [],
            "common_pitfalls": [],
            "last_feedback": getattr(self, "last_feedback", ""),
        }

        # Heurystyka po treści misji (szybkie tagi)
        m = (mission or "").lower()
        if "csv" in m or "etl" in m or "pipeline" in m:
            context["recommended_strategies"].append("Add robust error handling with retry & dead-letter queue.")
            context["common_pitfalls"].append("Schema drift unchecked; missing data validation gates.")
        if "continuous" in m or "online" in m or "adapt" in m:
            context["recommended_strategies"].append("Introduce drift detection + gated retraining with rollback.")
            context["common_pitfalls"].append("No cap on retraining loops; missing convergence/abort criteria.")
        if "causal" in m or "przyczyn" in m:
            context["recommended_strategies"].append("Add causal shift analysis before blind retraining.")

        # Na bazie ostatnich udanych planów zapamiętanych w tym procesie
        best_practices = []
        if hasattr(self, "successful_strategies") and self.successful_strategies:
            best_practices = self.successful_strategies[-5:]
        for bp in best_practices:
            tip = bp.get("tip") or bp.get("note")
            if tip:
                context["recommended_strategies"].append(tip)

        # Deduplicate i przytnij, żeby prompt był zwięzły
        context["recommended_strategies"] = list(dict.fromkeys(context["recommended_strategies"]))[:6]
        context["common_pitfalls"] = list(dict.fromkeys(context["common_pitfalls"]))[:6]

        return context

    
    
    #przeszukiwanie pamieci
    
    def get_vertex_context(self, mission: str, min_score: float = 80.0, top_k: int = 5) -> dict:
        ctx = {"recommended_strategies": [], "common_pitfalls": [], "examples": []}
        if VertexSearchTool is None or SystemConfig is None:
            return ctx
        try:
            cfg = SystemConfig()
            vst = VertexSearchTool(cfg)
            raw = vst.search_mission_memory(query=mission, top_k=top_k)
            results = (json.loads(raw) or {}).get("results", [])
            for r in results:
                try:
                    score = float(r.get("score") or 0)
                except Exception:
                    score = 0.0
                if score < min_score:
                    continue
                tags = r.get("tags") or []
                links = r.get("links") or {}
                if "retry" in tags:
                    ctx["recommended_strategies"].append("Use retry with backoff + DLQ.")
                if "rollback" in tags:
                    ctx["recommended_strategies"].append("Add rollback path for irreversible ops.")
                if "optimize" in tags:
                    ctx["recommended_strategies"].append("Add optimize_performance guarded loop.")
                if links.get("plan_uri"):
                    ctx["examples"].append({
                    "mission_id": r.get("mission_id"),
                    "plan_uri": links["plan_uri"],
                    })
            # dedup + limit
            dedup = []
            seen = set()
            for t in ctx["recommended_strategies"]:
                if t not in seen:
                    seen.add(t)
                    dedup.append(t)
            ctx["recommended_strategies"] = dedup[:6]
            return ctx
        except Exception as e:
            process_log(f"[MEMORY] Vertex ctx skipped: {e}")
            return {"recommended_strategies": [], "common_pitfalls": [], "examples": []}
    
    #koniec poprawki
    
    
    
    def add_successful_plan(self, plan: Dict[str, Any], mission: str, metadata: Dict[str, Any]):
        """
        Zapisuje „udany plan” lokalnie (folder per misja) oraz aktualizuje proste „best practices”.
        To woła orchestrator po wyciągnięciu finalnego planu.
        """
        try:
            os.makedirs("memory/success", exist_ok=True)

            # id misji z misji (spójne z save_complete_mission)
            from datetime import datetime as _dt
            import hashlib as _h
            ts = _dt.now().strftime("%Y%m%d_%H%M%S")
            h = _h.md5(mission.encode("utf-8")).hexdigest()[:8]
            mission_id = f"mission_{ts}_{h}"

            # folder per misja
            mission_dir = os.path.join("memory", "success", mission_id)
            os.makedirs(mission_dir, exist_ok=True)

            # zapis planu i meta
            with open(os.path.join(mission_dir, f"{mission_id}.plan.json"), "w", encoding="utf-8") as f:
                json.dump(plan, f, ensure_ascii=False, indent=2)
            payload = {
                "mission": mission,
                "metadata": metadata or {},
                "saved_at": _dt.now().isoformat(),
            }
            with open(os.path.join(mission_dir, f"{mission_id}.meta.json"), "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)

            # bardzo proste „best practices” (do wykorzystania przez get_relevant_context)
            node_types = {str(n.get("implementation") or n.get("name") or "").lower()
                          for n in (plan.get("nodes") or [])}
            tiplist = []
            if "error_handler" in node_types:
                tiplist.append("Always include a dedicated error_handler with routing to notify/report.")
            if "rollback" in node_types:
                tiplist.append("Keep rollback path for any irreversible change.")
            if "validate_data" in node_types:
                tiplist.append("Gate the main path by validate_data with measurable thresholds.")
            if "validate_model" in node_types and "train_model" in node_types:
                tiplist.append("Retrain only after drift detection; compare and rollback on regression.")
            if "optimize_performance" in node_types:
                tiplist.append("Use optimize_performance with loop guards to avoid infinite cycles.")

            if tiplist:
                self.successful_strategies.append({"mission_id": mission_id, "tip": tiplist[0]})

            process_log(f"[MEMORY] add_successful_plan saved under {mission_dir}")
        except Exception as e:
            process_log(f"[MEMORY ERROR] add_successful_plan failed: {e}")

    
    
    #koniec poprawek
        
        
        
        
    def _clean_agent_content(
        self, content: str
    ) -> any:  # Zmieniamy typ zwracany na 'any'
        """
        Usuwa bloki kodu markdown i parsuje wewnętrzny JSON,
        jeśli to możliwe.
        """
        if not isinstance(content, str):
            return content

        cleaned_content = content.strip()

        # Krok 1: Usuń bloki kodu markdown (tak jak poprzednio)
        pattern = r"```(?:json)?\s*(.*?)\s*```"
        match = re.search(pattern, cleaned_content, re.DOTALL)
        if match:
            cleaned_content = match.group(1).strip()

        # === NOWY, KLUCZOWY KROK ===
        # Krok 2: Spróbuj sparsować string jako JSON
        try:
            # Jeśli się uda, zwróć prawdziwy obiekt (słownik)
            return json.loads(cleaned_content)
        except json.JSONDecodeError:
            # Jeśli to nie jest JSON, zwróć po prostu oczyszczony tekst
            return cleaned_content

    def _gcs_path(self, relative: str) -> str:
        """Buduje pełną ścieżkę do pliku w GCS z uwzględnieniem prefiksu"""
        if self.gcs_prefix:
            return f"{self.gcs_prefix}/{relative}"
        return relative

    def _learn_from_success(self, mission_record: Dict):
        """Ekstraktuje i zapisuje PRAWDZIWE wzorce z udanej misji"""

        # 1. Zapisz wzorzec sukcesu dla tego typu misji
        pattern_key = f"success_pattern_{mission_record['mission_type']}"

        if pattern_key not in self.learned_patterns:
            self.learned_patterns[pattern_key] = {
                "occurrences": 0,
                "examples": [],
                "common_elements": {},
                "avg_score": 0,
                "best_practices": [],
            }

        # 2. Aktualizuj statystyki
        pattern = self.learned_patterns[pattern_key]
        pattern["occurrences"] += 1
        current_score = mission_record.get("final_score", 0)
        pattern["avg_score"] = (
            pattern["avg_score"] * (pattern["occurrences"] - 1) + current_score
        ) / pattern["occurrences"]

        # 3. Znajdź kluczowe elementy sukcesu
        success_elements = []

        # Sprawdź co było w tym planie
        plan = mission_record.get("final_plan", {})
        nodes = plan.get("nodes", [])

        # Zapisz które węzły były użyte
        node_types = [n.get("implementation") for n in nodes]

        if "error_handler" in node_types:
            success_elements.append("comprehensive_error_handling")
        if "rollback" in node_types:
            success_elements.append("rollback_mechanism")
        if "validate_data" in node_types:
            success_elements.append("data_validation")
        if "optimize_performance" in node_types:
            success_elements.append("performance_optimization")

        # 4. Znajdź unikalne innowacje z tej misji
        if "Adaptive_Router" in str(nodes):
            success_elements.append("adaptive_routing")

        # 5. Zapisz jako best practice jeśli score > 90
        if current_score > 90:
            best_practice = {
                "mission_id": mission_record["memory_id"],
                "score": current_score,
                "key_success_factors": success_elements,
                "node_count": len(nodes),
                "complexity": mission_record["performance_metrics"].get(
                    "convergence_rate", 0
                ),
            }
            pattern["best_practices"].append(best_practice)

        # 6. Zaktualizuj common_elements (co występuje najczęściej)
        for element in success_elements:
            if element not in pattern["common_elements"]:
                pattern["common_elements"][element] = 0
            pattern["common_elements"][element] += 1

        # 7. Dodaj przykład
        pattern["examples"].append(
            {
                "mission_prompt": mission_record["mission_prompt"],
                "success_factors": success_elements,
                "score": current_score,
            }
        )

        process_log(
            f"[MEMORY] Learned from success: {pattern_key}, "
            f"occurrences={pattern['occurrences']}, "
            f"avg_score={pattern['avg_score']:.2f}"
        )

    def export_temporal_report(self, filepath: str = "memory/temporal_patterns.json"):
        """Eksportuje raport wzorców czasowych"""
        patterns = self.analyze_temporal_patterns()

        report = {
            "generated_at": datetime.now().isoformat(),
            "total_missions": len(self.full_mission_records),
            "patterns": patterns,
            "insights": [],
        }

        # Znajdź najlepszy/najgorszy czas
        best_day = max(
            patterns["by_weekday"].items(), key=lambda x: x[1].get("avg_score", 0)
        )
        worst_day = min(
            patterns["by_weekday"].items(), key=lambda x: x[1].get("avg_score", 100)
        )

        report["insights"].append(
            f"Best day: {best_day[0]} (avg: {best_day[1]['avg_score']:.1f})"
        )
        report["insights"].append(
            f"Worst day: {worst_day[0]} (avg: {worst_day[1]['avg_score']:.1f})"
        )

        with open(filepath, "w") as f:
            json.dump(report, f, indent=2)

        return report

    # ------
    def save_complete_mission(
        self,
        mission: str,
        final_plan: Dict,
        all_messages: List[Dict],
        orchestrator_state: Dict,
    ) -> str:
        """
        Zapisuje KOMPLETNY rekord misji do OSOBNEGO pliku JSON
        """
        from datetime import datetime
        import hashlib

        # Generuj unikalne ID
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        mission_hash = hashlib.md5(mission.encode()).hexdigest()[:8]
        mission_id = f"mission_{timestamp}_{mission_hash}"

        cleaned_messages = []
        for msg in all_messages:
            new_msg = msg.copy()  # Kopiujemy, aby nie modyfikować oryginału
            if "content" in new_msg:
                new_msg["content"] = self._clean_agent_content(new_msg["content"])
            cleaned_messages.append(new_msg)

        # Ekstraktuj kluczowe informacje z transcript
        iterations_data = self._extract_iterations_from_transcript(cleaned_messages)

        # Klasyfikuj misję i tagi
        mission_type = self._classify_mission(mission)
        tags = self._extract_tags(mission, final_plan)

        # Znajdź krytyczne momenty w debacie
        critical_moments = self._identify_critical_moments(all_messages)

        # Przygotuj pełny rekord
        mission_record = {
            # === METADATA ===
            "memory_id": mission_id,
            "timestamp": datetime.now().isoformat(),
            "mission_prompt": mission,
            "mission_type": mission_type,
            "tags": tags,
            # === OUTCOME ===
            "outcome": "Success" if final_plan else "Failed",
            "total_iterations": orchestrator_state.get("iteration_count", 0),
            "total_messages": len(all_messages),
            "time_taken_seconds": orchestrator_state.get("execution_time", 0),
            # === FINAL ARTIFACTS ===
            "final_plan": final_plan,
            "final_score": self._extract_final_score(all_messages),
            # === ITERATION DETAILS ===
            "iterations": iterations_data,
            # === KEY INSIGHTS ===
            "critique_evolution": self._track_critique_evolution(iterations_data),
            "aggregator_reasoning": self._extract_aggregator_reasoning(all_messages),
            "proposer_contributions": self._analyze_proposer_contributions(
                all_messages
            ),
            # === LEARNING DATA ===
            "llm_generated_summary": self._generate_mission_summary(
                all_messages, final_plan
            ),
            "identified_patterns": self._extract_patterns_from_debate(all_messages),
            "success_factors": self._identify_success_factors(
                final_plan, iterations_data
            ),
            "failure_points": self._identify_failure_points(iterations_data),
            # === CRITICAL MOMENTS ===
            "critical_moments": critical_moments,
            "turning_points": self._identify_turning_points(iterations_data),
            # === FULL TRANSCRIPT ===
            "full_transcript": cleaned_messages,
            # === METRICS ===
            "performance_metrics": {
                "token_usage": orchestrator_state.get("total_tokens", 0),
                "api_calls": orchestrator_state.get("api_calls", 0),
                "convergence_rate": self._calculate_convergence_rate(iterations_data),
            },
        }

        # NOWA CZĘŚĆ - Zapisz do osobnego pliku
        # --- Zapis misji: GCS jeśli skonfigurowany, inaczej lokalnie ---
        if getattr(self, "use_gcs", False) and getattr(self, "gcs_bucket", None):
            # Czytelna nazwa: YYYY/MM/DD/{YYYYMMDD_HHMMSS}-{slug}-{hash}.json
            slug = _slugify(mission)  # helper poza klasą
            ts_date, ts_time = timestamp.split("_")  # np. 20250829, 212413
            y, m, d = ts_date[:4], ts_date[4:6], ts_date[6:8]
            mission_blob_rel = (
                f"missions/{y}/{m}/{d}/{ts_date}_{ts_time}-{slug}-{mission_hash}.json"
            )
            mission_blob = self._gcs_path(mission_blob_rel)  # helper w klasie

            try:
                _gcs_upload_json(
                    self.gcs_bucket, mission_blob, mission_record
                )  # helper poza klasą
                process_log(
                    f"[MEMORY] Saved mission to GCS: gs://{self.gcs_bucket}/{mission_blob}"
                )
            except Exception as e:
                process_log(f"[MEMORY ERROR] Failed to save mission to GCS: {e}")

            # Lekki indeks: 1 plik per misja (łatwe listowanie prefixem)
            index_entry = {
                "mission_id": mission_id,
                "gcs_path": f"gs://{self.gcs_bucket}/{mission_blob}",
                "timestamp": mission_record.get("timestamp"),
                "mission_prompt": mission_record.get("mission_prompt", "")[:100],
                "mission_type": mission_record.get("mission_type"),
                "final_score": mission_record.get("final_score"),
                "tags": mission_record.get("tags", []),
                "outcome": mission_record.get("outcome"),
            }
            index_blob = self._gcs_path(f"index/{mission_id}.json")
            try:
                _gcs_upload_json(self.gcs_bucket, index_blob, index_entry)
            except Exception as e:
                process_log(f"[MEMORY ERROR] Failed to save index to GCS: {e}")

        else:
            # dotychczasowy zapis lokalny + lokalny indeks
            mission_dir = "memory/missions"
            os.makedirs(mission_dir, exist_ok=True)
            mission_file = os.path.join(mission_dir, f"{mission_id}.json")

            try:
                with open(mission_file, "w", encoding="utf-8") as f:
                    json.dump(mission_record, f, ensure_ascii=False, indent=2)
                process_log(f"[MEMORY] Saved mission to file: {mission_file}")
            except Exception as e:
                process_log(f"[MEMORY ERROR] Failed to save mission file: {e}")

            self._update_mission_index(mission_id, mission_file, mission_record)

        # Nadal zapisz do pamięci runtime jeśli potrzebne
        self.full_mission_records.append(mission_record)
        self.mission_index[mission_id] = len(self.full_mission_records) - 1

        if final_plan:
            self._learn_from_success(mission_record)

        # Aktualizuj wzorce czasowe co 5 misji
        if len(self.full_mission_records) % 5 == 0:
            patterns = self.analyze_temporal_patterns()
            process_log(
                f"[MEMORY] Temporal patterns update: {len(patterns['by_weekday'])} weekdays analyzed"
            )

        # Persist tylko patterns i strategies
        self._persist_lightweight_memory()

        process_log(f"[MEMORY] Saved complete mission: {mission_id}")
        return mission_id

    def _update_mission_index(self, mission_id: str, file_path: str, record: Dict):
        """Aktualizuje lekki indeks wszystkich misji"""
        index_file = "memory/mission_index.json"

        try:
            if os.path.exists(index_file):
                with open(index_file, "r", encoding="utf-8") as f:
                    index = json.load(f)
            else:
                index = {"missions": [], "metadata": {}}
        except:
            index = {"missions": [], "metadata": {}}

        # Dodaj wpis do indeksu
        index_entry = {
            "mission_id": mission_id,
            "file_path": file_path,
            "timestamp": record.get("timestamp"),
            "mission_prompt": record.get("mission_prompt", "")[:100],
            "mission_type": record.get("mission_type"),
            "final_score": record.get("final_score"),
            "tags": record.get("tags", []),
            "outcome": record.get("outcome"),
        }

        index["missions"].append(index_entry)
        index["metadata"]["last_updated"] = datetime.now().isoformat()
        index["metadata"]["total_missions"] = len(index["missions"])

        with open(index_file, "w", encoding="utf-8") as f:
            json.dump(index, f, ensure_ascii=False, indent=2)

    def _persist_lightweight_memory(self):
        """Zapisuje tylko patterns i strategies (bez pełnych rekordów misji)"""
        data = {
            "patterns": self.learned_patterns,
            "strategies": self.successful_strategies,
            "metadata": {
                "last_updated": datetime.now().isoformat(),
                "mission_count": len(self.full_mission_records),
            },
        }

        if self.use_gcs:
            try:
                blob = self._gcs_path("learned_strategies.json")
                _gcs_upload_json(self.gcs_bucket, blob, data)
                process_log(
                    f"[MEMORY] Saved lightweight memory to GCS: gs://{self.gcs_bucket}/{blob}"
                )
                return
            except Exception as e:
                process_log(
                    f"[MEMORY ERROR] Failed to save lightweight memory to GCS: {e}"
                )

        # fallback/local
        os.makedirs("memory", exist_ok=True)
        memory_file = "memory/learned_strategies.json"
        try:
            with open(memory_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"⚠ Nie udało się zapisać pamięci: {e}")

    def load_specific_mission(self, mission_id: str) -> Optional[Dict]:
        """Ładuje konkretną misję z pliku"""
        mission_file = f"memory/missions/{mission_id}.json"

        if os.path.exists(mission_file):
            try:
                with open(mission_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                process_log(f"[MEMORY ERROR] Cannot load mission {mission_id}: {e}")

        return None

    def search_missions(self, query: str, limit: int = 10) -> List[Dict]:
        """Przeszukuje indeks misji bez ładowania wszystkich plików"""
        index_file = "memory/mission_index.json"

        if not os.path.exists(index_file):
            return []

        with open(index_file, "r", encoding="utf-8") as f:
            index = json.load(f)

        results = []
        query_lower = query.lower()

        for entry in index["missions"]:
            # Proste wyszukiwanie tekstowe
            if (
                query_lower in entry.get("mission_prompt", "").lower()
                or query_lower in entry.get("mission_type", "").lower()
                or any(query_lower in tag.lower() for tag in entry.get("tags", []))
            ):

                results.append(entry)
                if len(results) >= limit:
                    break

        return results

    # ------

    #     def save_complete_mission(self,
    #                             mission: str,
    #                             final_plan: Dict,
    #                             all_messages: List[Dict],
    #                             orchestrator_state: Dict) -> str:
    #         """
    #         Zapisuje KOMPLETNY rekord misji z wszystkimi danymi
    #         """
    #         from datetime import datetime
    #         import hashlib

    #         # Generuj unikalne ID
    #         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    #         mission_hash = hashlib.md5(mission.encode()).hexdigest()[:8]
    #         mission_id = f"mission_{timestamp}_{mission_hash}"

    #         # Ekstraktuj kluczowe informacje z transcript
    #         iterations_data = self._extract_iterations_from_transcript(all_messages)

    #         # Klasyfikuj misję i tagi
    #         mission_type = self._classify_mission(mission)
    #         tags = self._extract_tags(mission, final_plan)

    #         # Znajdź krytyczne momenty w debacie
    #         critical_moments = self._identify_critical_moments(all_messages)

    #         # Przygotuj pełny rekord
    #         mission_record = {
    #             # === METADATA ===
    #             "memory_id": mission_id,
    #             "timestamp": datetime.now().isoformat(),
    #             "mission_prompt": mission,
    #             "mission_type": mission_type,
    #             "tags": tags,

    #             # === OUTCOME ===
    #             "outcome": "Success" if final_plan else "Failed",
    #             "total_iterations": orchestrator_state.get("iteration_count", 0),
    #             "total_messages": len(all_messages),
    #             "time_taken_seconds": orchestrator_state.get("execution_time", 0),

    #             # === FINAL ARTIFACTS ===
    #             "final_plan": final_plan,
    #             "final_score": self._extract_final_score(all_messages),

    #             # === ITERATION DETAILS ===
    #             "iterations": iterations_data,

    #             # === KEY INSIGHTS ===
    #             "critique_evolution": self._track_critique_evolution(iterations_data),
    #             "aggregator_reasoning": self._extract_aggregator_reasoning(all_messages),
    #             "proposer_contributions": self._analyze_proposer_contributions(all_messages),

    #             # === LEARNING DATA ===
    #             "llm_generated_summary": self._generate_mission_summary(all_messages, final_plan),
    #             "identified_patterns": self._extract_patterns_from_debate(all_messages),
    #             "success_factors": self._identify_success_factors(final_plan, iterations_data),
    #             "failure_points": self._identify_failure_points(iterations_data),

    #             # === CRITICAL MOMENTS ===
    #             "critical_moments": critical_moments,
    #             "turning_points": self._identify_turning_points(iterations_data),

    #             # === FULL TRANSCRIPT ===
    #             "full_transcript": all_messages,  # Kompletny zapis

    #             # === METRICS ===
    #             "performance_metrics": {
    #                 "token_usage": orchestrator_state.get("total_tokens", 0),
    #                 "api_calls": orchestrator_state.get("api_calls", 0),
    #                 "convergence_rate": self._calculate_convergence_rate(iterations_data)
    #             }
    #         }

    #         # Zapisz do pamięci
    #         self.full_mission_records.append(mission_record)
    #         self.mission_index[mission_id] = len(self.full_mission_records) - 1

    #         if final_plan:  # Jeśli misja się udała
    #             self._learn_from_success(mission_record)

    #         if len(self.full_mission_records) % 5 == 0:
    #             patterns = self.analyze_temporal_patterns()
    #             process_log(f"[MEMORY] Temporal patterns update: {len(patterns['by_weekday'])} weekdays analyzed")

    #         # Persist immediately
    #         self._persist_full_memory()

    #         process_log(f"[MEMORY] Saved complete mission: {mission_id}")
    #         return mission_id

    def _extract_iterations_from_transcript(self, messages: List[Dict]) -> List[Dict]:
        """Ekstraktuje dane każdej iteracji z transkryptu"""
        iterations = []
        current_iteration = {"proposers": [], "aggregator": None, "critic": None}

        for msg in messages:
            role = msg.get("name", "").lower()

            if "proposer" in role or "analyst" in role or "planner" in role:
                current_iteration["proposers"].append(
                    {
                        "agent": msg.get("name"),
                        "content": msg.get("content"),
                        "key_ideas": self._extract_key_ideas(msg.get("content", "")),
                    }
                )

            elif "aggregator" in role:
                current_iteration["aggregator"] = {
                    "content": msg.get("content"),
                    "synthesis": self._extract_synthesis(msg.get("content", "")),
                }

            elif "critic" in role:
                current_iteration["critic"] = {
                    "content": msg.get("content"),
                    "verdict": self._extract_verdict(msg.get("content", "")),
                    "score": self._extract_score(msg.get("content", "")),
                    "weaknesses": self._extract_weaknesses(msg.get("content", "")),
                }

                # Koniec iteracji - zapisz i zacznij nową
                if current_iteration["proposers"]:
                    iterations.append(current_iteration)
                    current_iteration = {
                        "proposers": [],
                        "aggregator": None,
                        "critic": None,
                    }

        return iterations

    def _generate_mission_summary(self, messages: List[Dict], final_plan: Dict) -> str:
        """Generuje BOGATE podsumowanie misji"""
        summary_parts = []

        # 1. Liczba iteracji i czas
        iteration_count = sum(
            1 for m in messages if "critic" in m.get("name", "").lower()
        )
        summary_parts.append(f"Misja zakończona w {iteration_count} iteracji.")

        # 2. Kluczowe innowacje (szukaj w transkrypcie)
        innovations = set()
        for msg in messages:

            content_str = str(msg.get("content", ""))
            content = content_str.lower()

            if "adaptive" in content or "adaptacyjny" in content:
                innovations.add("adaptive routing")
            if "rollback" in content:
                innovations.add("rollback mechanism")
            if "optimiz" in content or "optymali" in content:
                innovations.add("optimization")

        if innovations:
            summary_parts.append(f"Zastosowano: {', '.join(innovations)}.")

        # 3. Analiza struktury planu
        if final_plan:
            nodes = final_plan.get("nodes", [])
            edges = final_plan.get("edges", [])

            # Policz typy ścieżek
            success_paths = len(
                [e for e in edges if e.get("condition") == "on_success"]
            )
            failure_paths = len(
                [e for e in edges if e.get("condition") == "on_failure"]
            )

            summary_parts.append(
                f"Struktura: {len(nodes)} węzłów, "
                f"{success_paths} ścieżek sukcesu, "
                f"{failure_paths} ścieżek obsługi błędów."
            )

            # Znajdź kluczowe węzły
            key_nodes = []
            for node in nodes:
                impl = node.get("implementation", "")
                if impl in [
                    "error_handler",
                    "rollback",
                    "validate_data",
                    "optimize_performance",
                ]:
                    key_nodes.append(impl)

            if key_nodes:
                summary_parts.append(
                    f"Kluczowe komponenty: {', '.join(set(key_nodes))}."
                )

        # 4. Końcowy verdykt
        for msg in reversed(messages):
            if "critic" in msg.get("name", "").lower() and "ZATWIERDZONY" in msg.get(
                "content", ""
            ):
                summary_parts.append("Plan zatwierdzony przez krytyka bez zastrzeżeń.")
                break

        return " ".join(summary_parts)

    def _extract_tags(self, mission: str, final_plan: Dict) -> List[str]:
        """Automatycznie taguje misję"""
        tags = []
        mission_lower = mission.lower()

        # Mission-based tags
        tag_keywords = {
            "error_handling": ["error", "błęd", "obsługa", "handler"],
            "optimization": ["optym", "performance", "wydajność"],
            "causality": ["causal", "przyczyn"],
            "validation": ["valid", "walidac"],
            "retry": ["retry", "ponow"],
            "rollback": ["rollback", "cofn"],
            "ml": ["model", "train", "uczenie"],
            "data": ["data", "dane", "csv", "pipeline"],
        }

        for tag, keywords in tag_keywords.items():
            if any(kw in mission_lower for kw in keywords):
                tags.append(tag)

        # Plan-based tags
        if final_plan:
            nodes_str = str(final_plan.get("nodes", []))
            if "error_handler" in nodes_str:
                tags.append("robust")
            if "optimize" in nodes_str:
                tags.append("optimized")

        return list(set(tags))  # Unique tags

    # ------------
    def _load_persistent_memory(self):
        """Ładuje pamięć - patterns z głównego pliku, misje z indeksu"""
        json_file = "memory/learned_strategies.json"

        # Załaduj patterns i strategies
        if os.path.exists(json_file):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                self.learned_patterns = data.get("patterns", {})
                self.successful_strategies = data.get("strategies", [])
            except Exception as e:
                print(f"⚠ Nie udało się załadować pamięci: {e}")

        # Załaduj listę misji z indeksu
        index_file = "memory/mission_index.json"
        if os.path.exists(index_file):
            try:
                with open(index_file, "r", encoding="utf-8") as f:
                    index = json.load(f)

                # Załaduj ostatnie 10 misji do pamięci runtime
                recent_missions = index["missions"][-10:] if "missions" in index else []
                for entry in recent_missions:
                    if "file_path" in entry and os.path.exists(entry["file_path"]):
                        with open(entry["file_path"], "r", encoding="utf-8") as f:
                            record = json.load(f)
                            self.full_mission_records.append(record)
                            self.mission_index[entry["mission_id"]] = (
                                len(self.full_mission_records) - 1
                            )

                print(f"✔ Załadowano {len(self.full_mission_records)} ostatnich misji")
            except Exception as e:
                print(f"⚠ Problem z indeksem misji: {e}")
        else:
            print("🔍 Tworzę nową pamięć (brak istniejącego indeksu)")

        os.makedirs("memory/missions", exist_ok=True)

    # ------------

    #     def _load_persistent_memory(self):
    #         """
    #         Ładuje pamięć z pliku JSON
    #         """
    #         json_file = "memory/learned_strategies.json"

    #         if os.path.exists(json_file):
    #             try:
    #                 with open(json_file, "r", encoding="utf-8") as f:
    #                     data = json.load(f)
    #                 self.learned_patterns = data.get("patterns", {})
    #                 self.successful_strategies = data.get("strategies", [])

    #                 # Załaduj też nowe full_mission_records jeśli istnieją
    #                 if "full_mission_records" in data:
    #                     self.full_mission_records = data["full_mission_records"]
    #                     # Odbuduj index
    #                     for i, record in enumerate(self.full_mission_records):
    #                         self.mission_index[record["memory_id"]] = i

    #                 print(f"✔ Załadowano pamięć: {len(self.successful_strategies)} strategies, {len(self.full_mission_records)} full records")
    #             except Exception as e:
    #                 print(f"⚠ Nie udało się załadować pamięci: {e}")
    #         else:
    #             print("📝 Tworzę nową pamięć (brak istniejącego pliku)")
    #             os.makedirs("memory", exist_ok=True)

    #     def _persist_memory(self):
    #         """
    #         Zapisuje pamięć do pliku JSON
    #         """
    #         os.makedirs("memory", exist_ok=True)
    #         memory_file = "memory/learned_strategies.json"

    #         data = {
    #             "patterns": self.learned_patterns,
    #             "strategies": self.successful_strategies,
    #             "full_mission_records": self.full_mission_records  # NOWE!
    #         }

    #         try:
    #             with open(memory_file, "w", encoding="utf-8") as f:
    #                 json.dump(data, f, ensure_ascii=False, indent=2)
    #         except Exception as e:
    #             print(f"⚠ Nie udało się zapisać pamięci: {e}")

    def _persist_full_memory(self):
        """Alias dla _persist_memory"""
        # self._persist_memory()
        self._persist_lightweight_memory()

    # def _extract_key_ideas(self, content: str) -> List[str]:
    #     """Ekstraktuje kluczowe pomysły z contentu"""
    #     # Prosta heurystyka - możesz ulepszyć
    #     ideas = []
    #     if "error_handler" in content.lower():
    #         ideas.append("error_handling")
    #     if "rollback" in content.lower():
    #         ideas.append("rollback_mechanism")
    #     if "optimiz" in content.lower():
    #         ideas.append("optimization")
    #     return ideas

    
    
    def _extract_key_ideas(self, content):
        """
        Zwraca listę 'idei' (słów-kluczy) wykrytych w treści.
        Treść może być stringiem albo dict-em (np. plan/odpowiedź krytyka).
        """
        ideas = set()

        # Gdy dostajemy JSON-owy plan/krytykę (dict) — wyciągnijmy sygnały bez tekstu
        if isinstance(content, dict):
            # 1) plan: nodes/edges → dorzuć implementacje/nazwy
            nodes = content.get("nodes") \
                or content.get("final_synthesized_plan", {}).get("nodes") \
                or content.get("final_plan", {}).get("nodes") \
                or []
            if isinstance(nodes, list):
                for n in nodes:
                    if isinstance(n, dict):
                        impl = n.get("implementation") or n.get("name")
                        if impl:
                            ideas.add(str(impl).lower())

            # 2) krytyka: weaknesses → dorzuć nazwy słabości
            cs = content.get("critique_summary") or {}
            for w in (cs.get("identified_weaknesses") or []):
                if isinstance(w, dict):
                    wk = w.get("weakness")
                    if wk:
                        ideas.add(str(wk).lower())

            # 3) przejdź do heurystyk tekstowych na zserializowanym stringu
            text = _to_text_safe(content)
        else:
            text = _to_text_safe(content)

        t = text.lower()

        # Heurystyki — jak wcześniej, ale działają na stringu
        if "error_handler" in t:
            ideas.add("error_handler")
        if "rollback" in t:
            ideas.add("rollback")
        if "retry" in t or "ponów" in t or "ponowienie" in t or "backoff" in t:
            ideas.add("retry")
        if "optimiz" in t or "optymaliz" in t:
            ideas.add("optimize")
        if "validate" in t or "walidac" in t:
            ideas.add("validate_data")
        if "clean" in t or "czyszczen" in t:
            ideas.add("clean_data")
        if "load" in t or "wczyt" in t:
            ideas.add("load_data")

        return sorted(i for i in ideas if i)

    
    def _extract_synthesis(self, content: str) -> str:
        """Ekstraktuje syntezę z odpowiedzi aggregatora"""
        # Szukaj "synthesis_reasoning" w JSON
        try:
            data = json.loads(content) if isinstance(content, str) else content
            return data.get("synthesis_reasoning", "")
        except:
            return ""

    def _extract_verdict(self, content: str) -> str:
        """Ekstraktuje werdykt z odpowiedzi krytyka"""
        content_str = str(content)

        if "ZATWIERDZONY" in content_str:
            return "ZATWIERDZONY"
        return "ODRZUCONY"

    def _extract_score(self, content: str) -> float:
        """Ekstraktuje score z odpowiedzi krytyka"""
        content_str = str(content)
        try:
            import re

            score_match = re.search(r'"Overall_Quality_Q":\s*([\d.]+)', content_str)
            if score_match:
                return float(score_match.group(1))
        except:
            pass
        return 0.0

    def _extract_weaknesses(self, content: str) -> List[str]:
        """Ekstraktuje weaknesses z odpowiedzi krytyka"""
        weaknesses = []
        try:
            if isinstance(content, dict):
                data = content
            else:  # Jeśli nie, spróbuj sparsować go jako JSON
                data = json.loads(str(content))

            weak_list = data.get("critique_summary", {}).get(
                "identified_weaknesses", []
            )
            for w in weak_list:
                if isinstance(w, dict):
                    weaknesses.append(w.get("weakness", ""))
                else:
                    weaknesses.append(str(w))
        except:
            pass
        return weaknesses

    def add_successful_plan(self, plan: Dict[str, Any], mission: str, metadata: Dict):
        """Zapisuje udany plan do pamięci proceduralnej"""
        strategy = {
            "mission_type": self._classify_mission(mission),
            "plan_structure": self._extract_plan_structure(plan),
            "success_factors": metadata.get("success_factors", []),
            "performance_metrics": metadata.get("metrics", {}),
            "timestamp": datetime.now().isoformat(),
        }

        self.successful_strategies.append(strategy)
        self._persist_lightweight_memory()  # Zapisz od razu

        # Loguj dodanie udanego planu
        process_log(
            f"[MEMORY] Added successful plan for mission_type={strategy['mission_type']}, "
            f"nodes={strategy['plan_structure']['num_nodes']}"
        )

    def _classify_mission(self, mission: str) -> str:
        """Klasyfikuje typ misji"""
        mission_lower = mission.lower()

        if "przyczynow" in mission_lower or "causal" in mission_lower:
            return "causal_analysis"
        elif (
            "dane" in mission_lower or "data" in mission_lower or "csv" in mission_lower
        ):
            return "data_processing"
        elif "model" in mission_lower:
            return "model_validation"
        elif "optymali" in mission_lower:
            return "optimization"
        else:
            return "general"

    def _extract_plan_structure(self, plan: Dict) -> Dict:
        """Ekstraktuje strukturalne cechy planu"""
        return {
            "num_nodes": len(plan.get("nodes", [])),
            "num_edges": len(plan.get("edges", [])),
            "has_error_handling": any(
                "error" in str(node).lower() for node in plan.get("nodes", [])
            ),
            "has_validation": any(
                "valid" in str(node).lower() for node in plan.get("nodes", [])
            ),
            "graph_complexity": self._calculate_complexity(plan),
        }

    def _calculate_complexity(self, plan: Dict) -> float:
        """Oblicza złożoność grafu"""
        nodes = len(plan.get("nodes", []))
        edges = len(plan.get("edges", []))

        if nodes == 0:
            return 0.0

        # Złożoność cyklomatyczna aproksymowana
        return (edges - nodes + 2) / nodes

    def _identify_critical_moments(self, messages: List[Dict]) -> List[Dict]:
        """Identyfikuje krytyczne momenty w debacie"""
        critical = []
        for i, msg in enumerate(messages):
            content_str = str(msg.get("content", ""))  # Konwertujemy na string
            content = content_str.lower()

            # Moment krytyczny = duża zmiana w score lub verdict
            if "zatwierdzony" in content or "odrzucony" in content:
                critical.append(
                    {
                        "index": i,
                        "type": "verdict",
                        "agent": msg.get("name"),
                        "summary": "Decyzja krytyka",
                    }
                )
        return critical

    def _extract_final_score(self, messages: List[Dict]) -> float:
        """Znajduje finalny score z ostatniej odpowiedzi krytyka"""
        for msg in reversed(messages):
            if "critic" in msg.get("name", "").lower():
                score = self._extract_score(msg.get("content", ""))
                if score > 0:
                    return score
        return 0.0

    def _track_critique_evolution(self, iterations: List[Dict]) -> List[Dict]:
        """Śledzi jak zmieniała się krytyka między iteracjami"""
        evolution = []
        for i, iteration in enumerate(iterations):
            if iteration.get("critic"):
                evolution.append(
                    {
                        "iteration": i,
                        "score": iteration["critic"].get("score", 0),
                        "verdict": iteration["critic"].get("verdict", ""),
                        "main_issues": iteration["critic"].get("weaknesses", [])[:2],
                    }
                )
        return evolution

    def _extract_aggregator_reasoning(self, messages: List[Dict]) -> str:
        """Wyciąga reasoning agregatora"""
        for msg in reversed(messages):
            if "aggregator" in msg.get("name", "").lower():
                return self._extract_synthesis(msg.get("content", ""))
        return ""

    def _analyze_proposer_contributions(
        self, messages: List[Dict]
    ) -> Dict[str, List[str]]:
        """Analizuje wkład każdego proposera"""
        contributions = {}
        for msg in messages:
            name = msg.get("name", "")
            if any(role in name.lower() for role in ["analyst", "planner", "proposer"]):
                if name not in contributions:
                    contributions[name] = []
                ideas = self._extract_key_ideas(msg.get("content", ""))
                contributions[name].extend(ideas)
        return contributions

    def _extract_patterns_from_debate(self, messages: List[Dict]) -> List[str]:
        """Ekstraktuje wzorce z całej debaty"""
        patterns = []

        all_text = " ".join(str(m.get("content", "")) for m in messages).lower()

        # Szukaj powtarzających się konceptów
        all_text = " ".join(m.get("content", "") for m in messages).lower()

        if all_text.count("error_handler") > 3:
            patterns.append("Częste odniesienia do obsługi błędów")
        if all_text.count("rollback") > 2:
            patterns.append("Rollback jako kluczowy element")
        if all_text.count("optimiz") > 2:
            patterns.append("Focus na optymalizację")

        return patterns

    def _identify_success_factors(
        self, final_plan: Dict, iterations: List[Dict]
    ) -> List[str]:
        """Identyfikuje co przyczyniło się do sukcesu"""
        factors = []

        if final_plan:
            # Analiza struktury planu
            if any("error" in str(n).lower() for n in final_plan.get("nodes", [])):
                factors.append("Comprehensive error handling")
            if any("valid" in str(n).lower() for n in final_plan.get("nodes", [])):
                factors.append("Data validation steps")

            # Analiza iteracji
            if len(iterations) > 1:
                factors.append(f"Iterative improvement ({len(iterations)} rounds)")

        return factors

    def _identify_failure_points(self, iterations: List[Dict]) -> List[Dict]:
        """Identyfikuje gdzie były problemy"""
        failures = []
        for i, iteration in enumerate(iterations):
            if iteration.get("critic", {}).get("verdict") == "ODRZUCONY":
                failures.append(
                    {
                        "iteration": i,
                        "issues": iteration["critic"].get("weaknesses", []),
                        "score": iteration["critic"].get("score", 0),
                    }
                )
        return failures

    def _identify_turning_points(self, iterations: List[Dict]) -> List[Dict]:
        """Znajduje punkty zwrotne w debacie"""
        turning_points = []
        prev_score = 0

        for i, iteration in enumerate(iterations):
            curr_score = iteration.get("critic", {}).get("score", 0)
            if curr_score - prev_score > 20:  # Duży skok w score
                turning_points.append(
                    {
                        "iteration": i,
                        "score_jump": curr_score - prev_score,
                        "reason": "Significant improvement",
                    }
                )
            prev_score = curr_score

        return turning_points

    def _calculate_convergence_rate(self, iterations: List[Dict]) -> float:
        """Oblicza jak szybko system doszedł do rozwiązania"""
        if not iterations:
            return 0.0

        scores = [it.get("critic", {}).get("score", 0) for it in iterations]
        if len(scores) < 2:
            return 1.0

        # Średni przyrost score na iterację
        improvements = [scores[i + 1] - scores[i] for i in range(len(scores) - 1)]
        avg_improvement = sum(improvements) / len(improvements) if improvements else 0

        # Normalizuj do 0-1 (im wyższy przyrost, tym lepsza convergence)
        return min(
            avg_improvement / 20, 1.0
        )  # 20 punktów na iterację = max convergence

    def analyze_temporal_patterns(self) -> Dict[str, Any]:
        """Analizuje wzorce czasowe w performance systemu"""
        from datetime import datetime

        patterns = {"by_weekday": {}, "by_hour": {}, "by_day_hour": {}}

        if not self.full_mission_records:
            return patterns

        # Analiza per dzień tygodnia
        for record in self.full_mission_records:
            timestamp = datetime.fromisoformat(record["timestamp"])
            weekday = timestamp.strftime("%A")
            hour = timestamp.hour
            day_hour = f"{weekday}_{hour:02d}h"

            # Per weekday
            if weekday not in patterns["by_weekday"]:
                patterns["by_weekday"][weekday] = {
                    "missions": [],
                    "avg_score": 0,
                    "avg_iterations": 0,
                    "common_issues": [],
                }

            patterns["by_weekday"][weekday]["missions"].append(record["memory_id"])

            # Per hour
            if hour not in patterns["by_hour"]:
                patterns["by_hour"][hour] = {
                    "missions": [],
                    "avg_score": 0,
                    "avg_iterations": 0,
                }

            patterns["by_hour"][hour]["missions"].append(record["memory_id"])

            # Per day+hour combo
            if day_hour not in patterns["by_day_hour"]:
                patterns["by_day_hour"][day_hour] = {"missions": [], "scores": []}

            patterns["by_day_hour"][day_hour]["missions"].append(record["memory_id"])
            patterns["by_day_hour"][day_hour]["scores"].append(
                record.get("final_score", 0)
            )

        # Oblicz średnie
        for weekday, data in patterns["by_weekday"].items():
            if data["missions"]:
                scores = [
                    r["final_score"]
                    for r in self.full_mission_records
                    if r["memory_id"] in data["missions"]
                ]
                data["avg_score"] = sum(scores) / len(scores) if scores else 0

        return patterns

    def get_current_context_hints(self) -> str:
        """Zwraca wskazówki kontekstowe na podstawie aktualnego czasu"""
        from datetime import datetime

        now = datetime.now()
        patterns = self.analyze_temporal_patterns()

        hints = []

        # Sprawdź wzorce dla aktualnego dnia
        weekday = now.strftime("%A")
        if weekday in patterns["by_weekday"]:
            weekday_data = patterns["by_weekday"][weekday]
            if weekday_data["avg_score"] < 90:
                hints.append(
                    f"Uwaga: {weekday} historycznie mają niższe score ({weekday_data['avg_score']:.1f})"
                )

        # Sprawdź wzorce dla aktualnej godziny
        hour = now.hour
        if hour in patterns["by_hour"]:
            hour_data = patterns["by_hour"][hour]
            if len(hour_data["missions"]) > 2:  # Jeśli mamy wystarczająco danych
                hints.append(
                    f"O godzinie {hour}:00 zazwyczaj wykonywane są misje tego typu"
                )

        return " | ".join(hints) if hints else ""



--- FILE: moa_prompts.py ---

"""
Zaawansowane prompty dla systemu MOA z technikami Chain-of-Thought i Self-Consistency
"""

from typing import Dict, Any, List
from config.models_config import AgentRole


class MOAPrompts:
    """Centralna biblioteka promptów dla systemu MOA"""

    # Uniwersalne zasady dla wszystkich agentów
    UNIVERSAL_PRINCIPLES = """
## UNIVERSAL REASONING & OUTPUT POLICY

1) Deterministic, Structured Reasoning
- Decompose the mission into atomic steps; make dependencies explicit.
- Prefer DAG-like flows with clear success/failure transitions.

2) Output Contract (STRICT)
- Final output MUST be a single valid JSON object (no prose, no code fences, no comments).
- Keys and schema names are in English; user-facing strings are in Polish.
- If you risk exceeding token limits, compress explanations but keep structure intact.

3) Memory & Retrieval Discipline
- When WRITING memory: always store concise English bullet points or JSON objects
  (normalized nouns, present tense, ≤200 tokens per write).
- When READING memory: query only what is needed for the current decision.
- Never copy large memory chunks into the output; summarize instead.

4) Robustness by Design
- For each critical step, state the expected preconditions and postconditions.
- Include failure transitions (on_failure) and remediation (retry, rollback, notify).

5) Metrics & Confidence
- Quantify uncertainty (0–1). Justify with observable signals (e.g., data_quality).
- Prefer measurable thresholds over vague conditions.

6) Tooling Constraints
- Use ONLY nodes present in the node library (exact implementation names).
- Allowed edge.condition values: on_success, on_failure, retry, validated, partial_success,
  needs_optimization, else (as a last-resort catch-all).
"""

    @staticmethod
    def get_proposer_prompt(role: AgentRole, mission: str, node_library: Dict) -> str:
        """English prompt for Proposers; user-facing strings must be Polish."""
        style_mod = {
            "analytical": "Be precise and data-driven; justify every decision with observable signals.",
            "creative": "Explore non-obvious combinations and alternative paths; propose at least one novel twist.",
            "critical": "Stress-test assumptions and highlight edge cases and single points of failure.",
            "systematic": "Aim for holistic, end-to-end coherence with explicit interfaces between steps.",
        }

        expertise = f"""
    # ROLE: {role.role_name}

    ## YOUR EXPERTISE
    You specialize in: {', '.join(role.expertise_areas)}

    ## THINKING STYLE
    {style_mod.get(role.thinking_style, "Default to clarity and rigor.")}

    {MOAPrompts.UNIVERSAL_PRINCIPLES}

    ## ROLE-SPECIFIC TECHNIQUES
    """
        rl = role.role_name.lower()
        if "causal" in rl:
            expertise += """
    - Causal Reasoning:
      * Identify variables and likely causal relations (confounders, mediators).
      * Prefer testable interventions; annotate assumptions explicitly.
    """
        elif "strategic" in rl:
            expertise += """
    - Strategic Planning:
      * SWOT per component; map critical dependencies and critical path.
      * Prepare 1–2 realistic what-if branches with measurable triggers.
    """
        elif "creative" in rl:
            expertise += """
    - Creative Expansion:
      * Apply SCAMPER to at least two nodes.
      * Propose 3 alternative micro-approaches and pick one with rationale.
    """
        elif "risk" in rl or "quality" in rl:
            expertise += """
    - Risk/Quality:
      * FMEA table in your head; identify top 3 failure modes and mitigations.
      * Add explicit rollback/notify paths for irrecoverable states.
    """

        return f"""
    {expertise}

    ## MISSION
    {mission}

    ## AVAILABLE NODE LIBRARY
    {MOAPrompts._format_node_library(node_library)}

    ## OUTPUT CONTRACT (ONLY JSON, NO PROSE)
    - Keys in English; user-facing strings in Polish.
    - Use ONLY implementations from the node library.
    - Ensure failure paths exist for critical steps.
    - Keep "thought_process" and justifications concise in Polish.

    Expected JSON structure:
    {{
      "thought_process": ["Krok 1: ...", "Krok 2: ...", "Krok 3: ..."],
      "plan": {{
        "entry_point": "Start_Node_Name",
        "nodes": [
          {{"name": "Load_Data", "implementation": "load_data"}},
          {{"name": "Clean_Data", "implementation": "clean_data"}},
          {{"name": "Validate_Data", "implementation": "validate_data"}}
        ],
        "edges": [
          {{"from": "Load_Data", "to": "Clean_Data", "condition": "on_success"}},
          {{"from": "Load_Data", "to": "Error_Handler", "condition": "on_failure"}}
        ]
      }},
      "confidence": 0.80,
      "key_innovations": ["Innowacja 1", "Innowacja 2"],
      "risk_mitigation": {{"Ryzyko A": "Mitigacja A", "Ryzyko B": "Mitigacja B"}}
    }}
    - Do NOT include code fences or comments.
    - When you write ANY memory (outside this output), save it in concise EN.
"""

    @staticmethod
    def get_aggregator_prompt() -> str:
        """English prompt for the Master Aggregator; output JSON only; user-facing text Polish."""
        return """
    # ROLE: MASTER AGGREGATOR — SYNTHESIS & GOVERNANCE

    You merge multiple proposals into a single, coherent, executable plan with strong
    robustness and measurable gates. You remove duplication, resolve conflicts, and
    preserve the best ideas.

    {UNIVERSAL_POLICY}

    ## SYNTHESIS PROTOCOL
    1) Score each proposal on: logical soundness, feasibility, innovation, robustness.
    2) Extract the best subcomponents and compose them (component interfaces must align).
    3) Resolve conflicts by explicit trade-offs; document rationale concisely (Polish).
    4) Guarantee failure paths (on_failure/rollback/notify) for critical nodes.
    5) Prefer measurable conditions (e.g., data_quality > 0.9) where applicable.

    ## META-LEARNING HOOKS
    - If prior successful patterns are known, prefer them; otherwise, annotate assumptions.

    ## OUTPUT CONTRACT (ONLY JSON, NO PROSE)
    - Keys in English; user-facing strings in Polish.
    - Provide a final executable DAG under `final_plan`.
    - Include a brief Polish synthesis rationale and confidence score in [0,1].

    Expected JSON structure:
    {
      "thought_process": ["Łączę elementy X i Y...", "Ujednolicam warunki..."],
      "final_plan": {
        "entry_point": "Load_Data",
        "nodes": [
          {"name": "Load_Data", "implementation": "load_data"},
          {"name": "Clean_Data", "implementation": "clean_data"},
          {"name": "Validate_Data", "implementation": "validate_data"},
          {"name": "Error_Handler", "implementation": "error_handler"},
          {"name": "Rollback_Changes", "implementation": "rollback"},
          {"name": "Generate_Report", "implementation": "generate_report"}
        ],
        "edges": [
          {"from": "Load_Data", "to": "Clean_Data", "condition": "on_success"},
          {"from": "Load_Data", "to": "Error_Handler", "condition": "on_failure"},
          {"from": "Clean_Data", "to": "Validate_Data", "condition": "on_success"}
        ]
      },
      "synthesis_reasoning": "Krótko po polsku: dlaczego taki układ jest najlepszy.",
      "component_sources": {"Causal Analyst": ["Validate_Data"], "Creative Planner": ["Generate_Report"]},
      "confidence_score": 0.90
    }
    - Do NOT include code fences or comments.
    - Any memory writes you perform must be saved in concise English.
    """.replace(
            "{UNIVERSAL_POLICY}", MOAPrompts.UNIVERSAL_PRINCIPLES
        )

    @staticmethod
    def get_critic_prompt() -> str:
        return """
# ROLE: QUALITY CRITIC — ADVERSARIAL VALIDATOR

You are the final gate. Stress-test structure, semantics, robustness and compliance
with the mission. If and only if the plan passes, approve it.

{UNIVERSAL_POLICY}

## VALIDATION CHECKLIST
- Structural: valid JSON; required fields present; node names & implementations align with library.
- Semantic: mission alignment; logical flow; dependencies satisfied; measurable conditions preferred.
- Robustness: explicit error paths; rollback and notify; identify SPOFs and mitigations.
- Metrics: compute concise quality metrics; justify scores briefly in Polish.

## DECISION RULE
- APPROVE only if Overall Quality >= threshold you deem reasonable and no critical gaps remain.
- When you APPROVE, set `critique_summary.verdict` to "ZATWIERDZONY" (Polish, uppercase).
- Also include a short Polish justification.

## OUTPUT CONTRACT (ONLY JSON, NO PROSE)
- Keys in English; user-facing strings in Polish.
- If approved, include a complete `final_synthesized_plan` (same schema as proposer/aggregator).
- Optionally include `decision_marker`: "PLAN_ZATWIERDZONY" to facilitate orchestration.

Expected JSON structure:
{
  "critique_summary": {
    "verdict": "ZATWIERDZONY",
    "statement": "Krótki powód po polsku.",
    "key_strengths": ["Mocna strona 1", "Mocna strona 2"],
    "identified_weaknesses": [
      {"weakness": "Słabość X", "severity": "Medium", "description": "Dlaczego to problem"}
    ]
  },
  "quality_metrics": {
    "Complexity_Score_C": 3.1,
    "Robustness_Score_R": 50,
    "Innovation_Score_I": 100,
    "Completeness_Score": 100,
    "Overall_Quality_Q": 84.07
  },
  "final_synthesized_plan": {
    "entry_point": "Load_Data",
    "nodes": [
      {"name": "Load_Data", "implementation": "load_data"},
      {"name": "Clean_Data", "implementation": "clean_data"}
    ],
    "edges": [
      {"from": "Load_Data", "to": "Clean_Data", "condition": "on_success"}
    ]
  },
  "decision_marker": "PLAN_ZATWIERDZONY"
}
- Do NOT include code fences or comments.
-In the final response, end with a line containing only PLAN_ZATWIERDZONY.
- Any memory writes you perform must be saved in concise English.
""".replace(
            "{UNIVERSAL_POLICY}", MOAPrompts.UNIVERSAL_PRINCIPLES
        )

    
    
    @staticmethod
    def get_memory_analyst_prompt() -> str:
        """English prompt for the Memory Analyst; output JSON only; internal text can be Polish for notes."""
        return """
# ROLE: MEMORY ANALYST — RETRIEVAL & CONDENSATION

You produce a single, concise, strictly-structured summary of relevant prior missions.
You DO NOT design or modify plans. You DO NOT debate. Tool-first mindset.

{UNIVERSAL_POLICY}

## OPERATING MODE (TOOL-FIRST)
1) Query mission memory:
   - Local episodic memory (if available)
   - External Vertex/Discovery index (if configured)
2) Optionally fetch up to 3 representative example plans (IDs + URIs).
3) Deduplicate ideas; compress to short, actionable bullets.
4) Do not copy large chunks; never paste full plans. Summarize only.

## QUALITY BAR
- Be factual; avoid speculation.
- Prefer patterns with proven success/approval signals (if present in memory).
- Include pitfalls only if evidenced across multiple prior cases or clearly applicable.
- Keep it short; this is a pre-brief for other agents, not a plan.

## OUTPUT CONTRACT (ONLY JSON, NO PROSE)
- Keys in English; short, user-facing strings can be Polish.
- Strict schema (no extra keys, no code fences):

{
  "recommended_strategies": [  // up to 6 short bullets, Polish allowed
    "Włącz retry z backoff i DLQ",
    "Dodaj rollback dla operacji nieodwracalnych"
  ],
  "common_pitfalls": [         // up to 6 short bullets
    "Brak walidacji schematu (schema drift)",
    "Brak warunków pomiarowych przy decydowaniu o retrainingu"
  ],
  "examples": [                // up to 3 items
    { "mission_id": "mission_2024_09_01_ab12cd", "plan_uri": "gs://bucket/missions/.../plan.json" }
  ],
  "notes": "≤ 50 słów: kiedy powyższe stosować / granice ważności"
}

### RULES
- Return ONLY the JSON object above.
- If no relevant memory is found: return empty arrays and notes="".
- Never propose nodes/edges; never approve/reject plans here.
- Do not include code fences or comments.
""".replace(
            "{UNIVERSAL_POLICY}", MOAPrompts.UNIVERSAL_PRINCIPLES
        )
    
    
    
    @staticmethod
    def _format_node_library(node_library: Dict) -> str:
        """Formatuje bibliotekę węzłów dla promptu"""
        formatted = []
        for name, details in node_library.items():
            formatted.append(f"- {name}: {details.get('description', 'Brak opisu')}")
        return "\n".join(formatted)



--- FILE: models_config.py ---

"""
Definicje struktur danych używanych do opisu ról agentów.
"""

from dataclasses import dataclass
from typing import List


@dataclass
class AgentRole:
    """
    Klasa opisująca rolę agenta w systemie multi‑agentowym.

    :param role_name: Nazwa roli (np. "Causal Analyst", "Creative Planner").
    :param expertise_areas: Lista dziedzin, w których agent się specjalizuje.
    :param thinking_style: Styl myślenia ("analytical", "creative", "critical", "systematic" itp.).
    """

    role_name: str
    expertise_areas: List[str]
    thinking_style: str



--- FILE: process_logger.py ---

"""
Prosty logger procesu generowania planu i rozmów między agentami.
Wszystkie komunikaty są dopisywane do pliku tekstowego z sygnaturą czasu.
"""

import sys
from datetime import datetime  # zamiast: import datetime

LOG_FILE = "process_log.txt"
STREAM_STDOUT = True



def log_exception(msg: str, exc: BaseException):
    import traceback
    tb = traceback.TracebackException.from_exception(exc)
    last = tb.stack[-1] if tb and tb.stack else None
    where = f"{last.filename}:{last.lineno} in {last.name}" if last else "unknown location"
    log(f"{msg}: {type(exc).__name__}: {exc} @ {where}")
    # pełny traceback w kolejnej linii:
    log("".join(tb.format()))



def log(msg: str):
    ts = datetime.now().strftime(
        "%Y-%m-%d %H:%M:%S"
    )  # zamiast: datetime.datetime.now()
    line = f"[{ts}] {msg}"
    try:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(line + "\n")
    except Exception:
        pass
    if STREAM_STDOUT:
        print(line, file=sys.stdout, flush=True)



--- FILE: response_parser.py ---

"""
Inteligentny parser odpowiedzi agentów z auto-korekcją
"""

import json
import re
from typing import Dict, Any, Optional
import ast

# Lokalny logger procesu
from process_logger import log as process_log


class ResponseParser:
    """
    Zaawansowany parser który radzi sobie z różnymi formatami odpowiedzi
    """

    def parse_agent_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        Parsuje odpowiedź agenta próbując różnych strategii
        """
        if not response:
            return None
        # Zaloguj otrzymaną odpowiedź (obcinamy do 200 znaków, aby log nie rósł nadmiernie)
        process_log(f"Received response: {response[:200]}")

        # Strategia 1: Czysty JSON
        parsed = self._try_pure_json(response)
        if parsed:
            process_log(f"Parsed using pure JSON: {parsed}")
            return parsed

        # Strategia 2: JSON z dodatkami (markdown, komentarze)
        parsed = self._try_extract_json(response)
        if parsed:
            process_log(f"Parsed using extract JSON: {parsed}")
            return parsed

        # Strategia 3: Python dict jako string (bez wykonywania kodu)
        parsed = self._try_python_dict(response)
        if parsed:
            process_log(f"Parsed using python-like dict: {parsed}")
            return parsed

        # Strategia 4: Strukturalna ekstrakcja
        parsed = self._try_structural_extraction(response)
        if parsed:
            process_log(f"Parsed using structural extraction: {parsed}")
            return parsed

        # Strategia 5: AI-based repair (używa regex i heurystyk)
        parsed = self._try_ai_repair(response)
        if parsed:
            process_log(f"Parsed using AI repair: {parsed}")
            return parsed

        process_log(f"Parse failed: {response[:200]}")
        print(f"⚠ Nie udało się sparsować odpowiedzi: {response[:100]}...")
        return None

    def _try_pure_json(self, response: str) -> Optional[Dict]:
        """Próbuje parsować jako czysty JSON"""
        try:
            return json.loads(response.strip())
        except:
            return None

    def _try_extract_json(self, response: str) -> Optional[Dict]:
        """Ekstraktuje JSON z tekstu"""
        # Szukamy JSON w blokach kodu
        json_pattern = r"```(?:json)?\s*(\{.*?\})\s*```"
        match = re.search(json_pattern, response, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass

        # Szukamy pierwszego { i ostatniego }
        start = response.find("{")
        end = response.rfind("}")

        if start != -1 and end != -1 and end > start:
            try:
                return json.loads(response[start : end + 1])
            except:
                pass

        return None

    def _try_python_dict(self, response: str) -> Optional[Dict]:
        """
        Próbuje sparsować słownik zapisany w notacji Pythona bez użycia eval. Wyszukuje
        pierwszą strukturę w nawiasach klamrowych, następnie zamienia pojedyncze cudzysłowy
        na podwójne i dodaje cudzysłowy do kluczy, aby użyć json.loads. Jeśli napotka błąd,
        zwraca None.
        """
        try:
            # Wyszukaj fragment przypominający słownik
            dict_pattern = r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}"
            match = re.search(dict_pattern, response)
            if not match:
                return None
            obj_str = match.group(0)
            # Zamień pojedyncze cudzysłowy na podwójne
            json_like = obj_str.replace("'", '"')
            # Dodaj cudzysłowy do kluczy, jeśli ich brakuje
            json_like = re.sub(
                r"(?<!\")\b([A-Za-z_][A-Za-z0-9_]*)\b\s*:", r'"\1":', json_like
            )
            return json.loads(json_like)
        except Exception:
            return None

    def _try_structural_extraction(self, response: str) -> Optional[Dict]:
        """Ekstraktuje strukturę na podstawie kluczowych słów"""
        result = {}

        # Szukamy kluczowych sekcji
        patterns = {
            "thought_process": r"(?:thought_process|thinking|reasoning)[:\s]+([^\n]+(?:\n(?!\w+:)[^\n]+)*)",
            "entry_point": r'(?:entry_point|start)[:\s]+["\']?(\w+)["\']?',
            "confidence": r"(?:confidence|certainty)[:\s]+(\d*\.?\d+)",
            "nodes": r"nodes[:\s]+\[(.*?)\]",
            "edges": r"edges[:\s]+\[(.*?)\]",
        }

        for key, pattern in patterns.items():
            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)
            if match:
                value = match.group(1).strip()

                if key == "confidence":
                    try:
                        result[key] = float(value)
                    except:
                        result[key] = 0.5
                elif key in ["nodes", "edges"]:
                    # Próbuj sparsować jako listę
                    try:
                        result[key] = ast.literal_eval(f"[{value}]")
                    except:
                        result[key] = []
                elif key == "thought_process":
                    # Podziel na kroki
                    steps = [s.strip() for s in value.split("\n") if s.strip()]
                    result[key] = steps
                else:
                    result[key] = value

        return result if result else None

    def _try_ai_repair(self, response: str) -> Optional[Dict]:
        """Próbuje naprawić JSON używając heurystyk"""
        # Usuń komentarze
        response = re.sub(r"//.*?\n", "", response)
        response = re.sub(r"/\*.*?\*/", "", response, flags=re.DOTALL)

        # Napraw typowe błędy
        repairs = [
            (r",\s*}", "}"),  # Usuń trailing commas
            (r",\s*]", "]"),
            (r'"\s*:\s*"([^"]*)"(?=[,}])', r'": "\1"'),  # Napraw cudzysłowy
            (r"(\w+)(?=\s*:)", r'"\1"'),  # Dodaj cudzysłowy do kluczy
            (r':\s*([^",\[\{}\]]+)(?=[,}])', r': "\1"'),  # Dodaj cudzysłowy do wartości
        ]

        for pattern, replacement in repairs:
            response = re.sub(pattern, replacement, response)

        # Spróbuj ponownie
        return self._try_pure_json(response)



--- FILE: run_debate.ipynb ---

from autogen_orchestrator import AutoGenMOAOrchestrator
import config_api


import vertexai
vertexai.init(project="dark-data-discovery", location="us-central1")

# Biblioteka węzłów używana do generowania planów
NODE_LIBRARY = {
    'load_data': {'description': 'Wczytuje dane z różnych źródeł'},
    'clean_data': {'description': 'Czyści dane'},
    'validate_data': {'description': 'Waliduje dane'},
    'discover_causality': {'description': 'Odkrywa relacje przyczynowe (może zawieść)'},
    'error_handler': {'description': 'Obsługuje błędy'},
    'rollback': {'description': 'Cofa zmiany'},
    'generate_report': {'description': 'Generuje raport'},
    'validate_model': {'description': 'Waliduje model'},
    'optimize_performance': {'description': 'Optymalizuje wydajność'},
    'train_model': {'description': 'Uczy model'},
    'notify_user': {'description': 'Powiadamia użytkownika'}
}

# Możesz podać misję na stałe albo poprosić użytkownika o wpisanie
mission = input("Podaj opis misji: ").strip()
if not mission:
    mission = "Stwórz prosty pipeline do analizy danych CSV"

# Inicjalizacja orchestratora z definicją misji i ścieżką do konfiguracji agentów
orchestrator = AutoGenMOAOrchestrator(
    mission=mission,
    node_library=NODE_LIBRARY,
    config_file="agents_config.json"
)

# Uruchom pełną debatę; wynik to słownik z finalnym planem lub None
final_plan = orchestrator.run_full_debate_cycle()
orchestrator.reset()
# Wyświetl wynik w czytelnej formie
if final_plan:
    import json
    print("\n✅ Zatwierdzony plan:")
    print(json.dumps(final_plan, indent=2, ensure_ascii=False))
else:
    print("\n❌ Nie udało się uzyskać zatwierdzonego planu.")
# --- Koniec komórki ---



--- FILE: structured_response_parser.py ---

"""
Structured parser oparty na Pydantic.  Zamiast heurystycznych prób parsowania
ręcznego, wykorzystuje schematy Pydantic do walidacji odpowiedzi LLM.  Ten
moduł zastępuje dotychczasowy `response_parser` w nowej konfiguracji.

Model `ProposerResponse` definiuje minimalną strukturę planu wygenerowanego
przez agentów‑proposerów.  Model `AggregatorResponse` rozszerza go o pole
`final_plan` oraz metadane używane przez agregatora.  Model `CriticResponse`
zawiera ocenę, listę mocnych i słabych stron oraz ewentualne sugestie
poprawek, zgodnie z założonym formatem JSON.

Jeśli odpowiedź nie jest poprawnym JSON‑em (np. zawiera `````markdown````
fences) lub nie spełnia schematu, parser zwraca `None`.
"""

from __future__ import annotations

import json
import re
from typing import List, Optional, Dict, Any
from process_logger import log as process_log
from pydantic import BaseModel, ValidationError, Field


class ProposerPlan(BaseModel):
    """Reprezentuje plan proponowany przez agenta‐proposera."""

    entry_point: str = Field(..., description="Nazwa pierwszego węzła w planie")
    nodes: List[Dict[str, Any]] = Field(..., description="Lista węzłów planu")
    edges: List[Dict[str, Any]] = Field(..., description="Lista krawędzi planu")


class ProposerResponse(BaseModel):
    """Struktura odpowiedzi agenta proponującego."""

    thought_process: List[str] = Field(..., description="Opis kroków rozumowania")
    plan: ProposerPlan = Field(..., description="Plan w formacie grafu")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Pewność (0–1)")
    key_innovations: Optional[List[str]] = Field(default_factory=list)
    risk_mitigation: Optional[Dict[str, Any]] = Field(default_factory=dict)


class AggregatorResponse(BaseModel):
    """Struktura odpowiedzi agregatora.  Rozszerza odpowiedź proponera o finalny plan."""

    thought_process: List[str]
    final_plan: ProposerPlan
    synthesis_reasoning: Optional[str]
    component_sources: Optional[Dict[str, Any]]
    confidence_score: Optional[float]
    improvements: Optional[List[str]] = Field(default_factory=list)


class CriticResponse(BaseModel):
    """Struktura odpowiedzi krytyka."""

    approved: bool
    score: float = Field(..., ge=0.0, le=100.0)
    strengths: List[str] = Field(default_factory=list)
    weaknesses: List[str] = Field(default_factory=list)
    feedback: Optional[str]
    improvements: Optional[List[str]] = Field(default_factory=list)


class StructuredResponseParser:
    """
    Parser, który wykorzystuje modele Pydantic do walidacji i konwersji odpowiedzi
    na słowniki.  Oczekuje, że agent zwraca poprawny JSON zgodny z jednym z
    powyższych schematów.  Można łatwo rozszerzyć o kolejne typy odpowiedzi.
    """

    def __init__(self) -> None:
        pass

    def _strip_code_fences(self, response: str) -> str:
        """Usuwa bloki kodu (```json ... ```) z odpowiedzi."""
        # Usuń bloki ```json ... ``` lub ``` ... ```
        pattern = r"```(?:json)?\s*(\{.*?\})\s*```"
        match = re.search(pattern, response, re.DOTALL)
        if match:
            return match.group(1)
        return response

    def parse_agent_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        Przetwarza odpowiedź agenta i próbuje ją zmapować na jeden z
        zdefiniowanych modeli.  Zwraca zserializowaną postać słownikową,
        lub None, jeśli nie można sparsować.
        """
        if not response:
            return None

        # Usuń otaczające bloki kodu
        cleaned = self._strip_code_fences(response.strip())

        # Spróbuj sparsować jako JSON
        try:
            data = json.loads(cleaned)
        except Exception:
            return None

        # Kolejno próbuj dopasować do modeli
        for model_cls in (ProposerResponse, AggregatorResponse, CriticResponse):
            try:
                obj = model_cls.parse_obj(data)
                return obj.dict()
            except ValidationError:
                continue

        # Jeśli nic nie pasuje, zwróć oryginalne dane
        return data

    def parse_critic_response(self, text: str):
        """
        Parsuje odpowiedź krytyka - zwraca CAŁY JSON
        """
        import json
        import re

        if not text:
            return None

        
        #poprawka
        if isinstance(text, dict):
            process_log("[PARSER] Otrzymano dict - zwracam bez zmian")
            return text
        
        #koniec poprawki
        
        try:
            # Usuń markdown code blocks
            clean_text = text.strip()

            # Usuń ```json i ```
            clean_text = re.sub(r"```json\s*", "", clean_text)
            clean_text = re.sub(r"```\s*", "", clean_text)

            # Usuń PLAN_ZATWIERDZONY z końca
            if "PLAN_ZATWIERDZONY" in clean_text:
                # Znajdź ostatnie wystąpienie i usuń wszystko po nim
                parts = clean_text.rsplit("PLAN_ZATWIERDZONY", 1)
                clean_text = parts[0].strip()

            # Teraz po prostu sparsuj JSON
            result = json.loads(clean_text)

            # Debug - wypisz co znalazłeś
            process_log(f"[PARSER] Znaleziono klucze: {list(result.keys())}")

            return result

        except json.JSONDecodeError as e:
            process_log(f"[PARSER] JSON decode error: {e}")

            # Plan B - znajdź JSON manualnie
            try:
                # Znajdź od pierwszego { do ostatniego }
                start = text.find("{")
                end = text.rfind("}")

                if start >= 0 and end > start:
                    json_str = text[start : end + 1]
                    return json.loads(json_str)
            except:
                pass

        return None



--- FILE: config/models_config.py ---

"""
Definicje struktur danych używanych do opisu ról agentów.
"""

from dataclasses import dataclass
from typing import List


@dataclass
class AgentRole:
    """
    Klasa opisująca rolę agenta w systemie multi‑agentowym.

    :param role_name: Nazwa roli (np. "Causal Analyst", "Creative Planner").
    :param expertise_areas: Lista dziedzin, w których agent się specjalizuje.
    :param thinking_style: Styl myślenia ("analytical", "creative", "critical", "systematic" itp.).
    """

    role_name: str
    expertise_areas: List[str]
    thinking_style: str



