--- FILE: Memory_bank.ipynb ---

import json
import hashlib
from datetime import datetime
from typing import Any, Dict, Optional, Callable
from vertexai import agent_engines as _agent_engines
# --- Koniec kom√≥rki ---
def get_or_create_agent_engine(display_name: str) :
    """
    Pobiera istniejƒÖcy Agent Engine po nazwie wy≈õwietlanej lub tworzy nowy, je≈õli nie istnieje.
    """
    # 1. Pobierz listƒô wszystkich istniejƒÖcych silnik√≥w w projekcie
    all_engines = _agent_engines.list()
    
    # 2. Sprawd≈∫, czy kt√≥ry≈õ z nich ma pasujƒÖcƒÖ nazwƒô
    for engine in all_engines:
        if engine.display_name == display_name:
            print(f"INFO: Znaleziono i po≈ÇƒÖczono z istniejƒÖcym Agent Engine: '{display_name}'")
            return engine
            
    # 3. Je≈õli pƒôtla siƒô zako≈Ñczy≈Ça i nic nie znaleziono, stw√≥rz nowy silnik
    print(f"INFO: Nie znaleziono Agent Engine o nazwie '{display_name}'. Tworzenie nowego...")
    try:
        new_engine = agent_engines.create(
            display_name=display_name
        )
        print(f"INFO: Pomy≈õlnie utworzono nowy Agent Engine.")
        return new_engine
    except Exception as e:
        print(f"KRYTYCZNY B≈ÅƒÑD: Nie mo≈ºna utworzyƒá Agent Engine. Sprawd≈∫ konfiguracjƒô i uprawnienia. B≈ÇƒÖd: {e}")
        exit()
# --- Koniec kom√≥rki ---
MEMORY_ENGINE_DISPLAY_NAME="test_for_moa_debate"
# --- Koniec kom√≥rki ---
agent_engine =get_or_create_agent_engine(MEMORY_ENGINE_DISPLAY_NAME)
AGENT_ENGINE_NAME = agent_engine.resource_name
print(AGENT_ENGINE_NAME)
# --- Koniec kom√≥rki ---
from vertexai import Client  # przygotuj klienta wcze≈õniej
client = Client(project="dark-data-discovery", location="us-central1")
from persist_missions_memory import persist_missions_to_vertex_memory
# --- Koniec kom√≥rki ---

persist_missions_to_vertex_memory(
    json_path="memory/learned_strategies.json",
    engine_name=AGENT_ENGINE_NAME,
    client=client,
    include_transcript=True,
    max_transcript_chunk_chars=15000,
)
# --- Koniec kom√≥rki ---
from retrieve_mission_memory import retrieve_mission_memory
# --- Koniec kom√≥rki ---
mission_data = retrieve_mission_memory(
    engine_name=AGENT_ENGINE_NAME,
    mission_id="mission_20250823_215948_92ed8ebc",
    client=client
)

plan = mission_data["final_plan"]["content"]         # oryginalny plan
transcript = mission_data["debate_transcript"]["content"]  # pe≈Çen transkrypt
# --- Koniec kom√≥rki ---



--- FILE: Untitled.ipynb ---

import importlib, process_logger as pl
importlib.reload(pl)
from process_logger import log as process_log
process_log("Logger OK")

# --- Koniec kom√≥rki ---
from autogen_orchestrator import AutoGenMOAOrchestrator
from autogen import ConversableAgent
from config_api import *
import autogen
from autogen import ConversableAgent




from autogen import ConversableAgent, UserProxyAgent
llm_cfg = {
  "role_name": "Causal Analyst",
  "model": {
    "provider": "google",
    "model_name": "gemini-2.5-pro",
    "temperature": 0.1,
    "client_args": {
      "vertex_project": "dark-data-discovery",
      "vertex_location": "us-central1"
    }
  }
}
g = ConversableAgent("gemini_test", llm_config=llm_cfg)
u = UserProxyAgent("user", human_input_mode="NEVER", max_consecutive_auto_reply=1)
u.initiate_chat(g, message="Powiedz jedno zdanie o mechanice kwantowej.")
# --- Koniec kom√≥rki ---
import os
import zipfile
from datetime import datetime

def spakuj_biezacy_folder(nazwa_pliku_zip=None):
    """
    Pakuje ca≈Çy bie≈ºƒÖcy folder roboczy (wraz z podfolderami) do pliku ZIP.
    Archiwum ZIP jest tworzone w folderze nadrzƒôdnym, aby uniknƒÖƒá
    dodania samego siebie do archiwum.

    Args:
        nazwa_pliku_zip (str, optional): Opcjonalna nazwa dla pliku ZIP (bez rozszerzenia .zip).
                                        Je≈õli nie zostanie podana, nazwa zostanie wygenerowana
                                        automatycznie na podstawie nazwy folderu i aktualnej daty.

    Returns:
        str: Pe≈Çna ≈õcie≈ºka do utworzonego pliku ZIP lub None w przypadku b≈Çƒôdu.
    """
    try:
        # Pobierz pe≈ÇnƒÖ ≈õcie≈ºkƒô do bie≈ºƒÖcego folderu
        biezacy_folder = os.getcwd()
        
        # Uzyskaj nazwƒô bie≈ºƒÖcego folderu
        nazwa_folderu = os.path.basename(biezacy_folder)

        # Ustal nazwƒô pliku wyj≈õciowego
        if nazwa_pliku_zip is None:
            # Wygeneruj domy≈õlnƒÖ nazwƒô, je≈õli nie podano w≈Çasnej
            znacznik_czasu = datetime.now().strftime("%Y%m%d_%H%M%S")
            nazwa_pliku_zip = f"{nazwa_folderu}_{znacznik_czasu}"
        
        # Dodaj rozszerzenie .zip, je≈õli go brakuje
        if not nazwa_pliku_zip.endswith('.zip'):
            nazwa_pliku_zip += '.zip'
            
        # Utw√≥rz plik ZIP w folderze nadrzƒôdnym, aby uniknƒÖƒá rekursji
        folder_nadrzedny = os.path.dirname(biezacy_folder)
        sciezka_do_zipa = os.path.join(folder_nadrzedny, nazwa_pliku_zip)

        print(f"Tworzenie archiwum: {sciezka_do_zipa}")

        # Otw√≥rz plik ZIP do zapisu
        with zipfile.ZipFile(sciezka_do_zipa, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # Przejd≈∫ przez wszystkie pliki i foldery w bie≈ºƒÖcym katalogu
            for root, dirs, files in os.walk(biezacy_folder):
                for file in files:
                    # Stw√≥rz pe≈ÇnƒÖ ≈õcie≈ºkƒô do pliku
                    sciezka_pliku = os.path.join(root, file)
                    # Oblicz ≈õcie≈ºkƒô wzglƒôdnƒÖ, aby zachowaƒá strukturƒô folder√≥w w ZIPie
                    sciezka_w_archiwum = os.path.relpath(sciezka_pliku, biezacy_folder)
                    # Zapisz plik do archiwum
                    zipf.write(sciezka_pliku, sciezka_w_archiwum)

        print("Archiwum zosta≈Ço pomy≈õlnie utworzone!")
        return sciezka_do_zipa

    except Exception as e:
        print(f"WystƒÖpi≈Ç b≈ÇƒÖd podczas tworzenia archiwum: {e}")
        return None

# --- Koniec kom√≥rki ---
spakuj_biezacy_folder()
# --- Koniec kom√≥rki ---



--- FILE: autogen_main.py ---

"""
G≈Ç√≥wny plik demonstracyjny dla systemu MOA u≈ºywajƒÖcego AutoGen
"""
import os
import json
from autogen_orchestrator import AutoGenMOAOrchestrator
from process_logger import log as process_log

# Przyk≈Çadowa biblioteka wƒôz≈Ç√≥w
NODE_LIBRARY = {
    'load_data': {'description': 'Wczytuje dane z r√≥≈ºnych ≈∫r√≥de≈Ç'},
    'clean_data': {'description': 'Czy≈õci dane'},
    'validate_data': {'description': 'Waliduje dane'},
    'discover_causality': {'description': 'Odkrywa relacje przyczynowe (mo≈ºe zawie≈õƒá)'},
    'error_handler': {'description': 'Obs≈Çuguje b≈Çƒôdy'},
    'rollback': {'description': 'Cofa zmiany'},
    'generate_report': {'description': 'Generuje raport'},
    'validate_model': {'description': 'Waliduje model'},
    'optimize_performance': {'description': 'Optymalizuje wydajno≈õƒá'}
}

def ensure_dummy_wrapper():
    """Upewnia siƒô, ≈ºe LLMWrapper dzia≈Ça w trybie dummy"""
    # Import extended wrapper ≈ºeby dodaƒá lepsze dummy responses
    try:
        import extended_llm_wrapper
        print("‚úì Extended LLM wrapper loaded (better dummy responses)")
    except:
        print("‚Ñπ Using basic LLM wrapper")

def run_autogen_demo():
    """Uruchamia demo z AutoGen"""
    
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           ü§ñ AUTOGEN MOA DEBATE SYSTEM                              ‚ïë
‚ïë                                                                      ‚ïë
‚ïë  ‚Ä¢ Multi-agent debate using AutoGen GroupChat                       ‚ïë
‚ïë  ‚Ä¢ Dynamic context injection from memory                            ‚ïë
‚ïë  ‚Ä¢ Iterative improvements with critic feedback                      ‚ïë
‚ïë  ‚Ä¢ Automatic termination on "PLAN_ZATWIERDZONY"                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # Przyk≈Çadowe misje
    missions = {
        "1": "Stw√≥rz prosty pipeline do analizy danych CSV",
        "2": "Zaprojektuj ODPORNY NA B≈ÅƒòDY przep≈Çyw odkrywania przyczynowo≈õci z mechanizmem retry",
        "3": "Zbuduj adaptacyjny system ML z continuous learning"
    }
    
    print("\nChoose a mission:")
    for key, mission in missions.items():
        print(f"  {key}. {mission[:60]}...")
    print("  4. Custom mission")
    print("  0. Exit")
    
    choice = input("\nYour choice: ").strip()
    
    if choice == "0":
        return
    elif choice in missions:
        mission = missions[choice]
    elif choice == "4":
        mission = input("\nEnter your custom mission:\n> ").strip()
        if not mission:
            print("Mission cannot be empty!")
            return
    else:
        print("Invalid choice!")
        return
    
    print(f"\nüìã MISSION: {mission}")
    print("-" * 70)
    
    # Inicjalizuj orchestrator
    orchestrator = AutoGenMOAOrchestrator(
        mission=mission,
        node_library=NODE_LIBRARY,
        config_file="agents_config.json"
    )
    
    # Uruchom pe≈Çny cykl debaty
    final_plan = orchestrator.run_full_debate_cycle()
    
    if final_plan:
        print("\n" + "="*70)
        print("üìä FINAL APPROVED PLAN:")
        print(json.dumps(final_plan, indent=2))
    else:
        print("\n‚ùå No plan was approved")
    
    print("\n" + "="*70)
    print("üìÅ Check outputs/ for saved plans")
    print("üìù Check logs/conversation_log.txt for detailed logs")
    print("üß† Check memory/ for learned patterns")

def main():
    """G≈Ç√≥wna funkcja"""
    # Upewnij siƒô ≈ºe katalogi istniejƒÖ
    os.makedirs("outputs", exist_ok=True)
    os.makedirs("logs", exist_ok=True) 
    os.makedirs("memory", exist_ok=True)
    
    # Za≈Çaduj extended wrapper dla lepszych dummy responses
    ensure_dummy_wrapper()
    
    # Log start
    process_log("=== AutoGen MOA System Started ===")
    
    try:
        run_autogen_demo()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        process_log("=== AutoGen MOA System Ended ===")

if __name__ == "__main__":
    main()


--- FILE: autogen_orchestrator.py ---

"""
Pe≈Çny orchestrator MOA u≈ºywajƒÖcy AutoGen do zarzƒÖdzania debatƒÖ agent√≥w
"""
import json
from datetime import datetime
import config_api
from autogen import UserProxyAgent, ConversableAgent, GroupChat, GroupChatManager
from typing import Dict, List, Any, Optional
from datetime import datetime
import autogen
from google.cloud import secretmanager
from models_config import AgentRole
from moa_prompts import MOAPrompts
from memory_system import ContextMemory
# U≈ºywamy structured parsera zamiast heurystycznego response_parser
from structured_response_parser import StructuredResponseParser
from process_logger import log as process_log
import os, json, time, traceback
from typing import Any, Dict, Optional
from process_logger import log as process_log
import vertexai

from config_api import basic_config_agent

class AutoGenMOAOrchestrator:
    """
    Orchestrator systemu MOA u≈ºywajƒÖcy AutoGen do wieloturowej debaty
    """
    
    def __init__(self, mission: str, node_library: Dict[str, Any], config_file: str = "agents_config.json"):
        self.mission = mission
        self.node_library = node_library
        self.memory = ContextMemory(max_episodes=50)
        # Parser oparty na Pydantic ‚Äì oczekuje czystego JSON zgodnego ze schematem
        self.parser = StructuredResponseParser()
        
        # Wczytaj konfiguracjƒô
        self._load_config(config_file)
        
        # Stan debaty
        self.iteration_count = 0
        self.max_iterations = 5
        self.current_context = {}
        self.final_plan = None
        self._forced_speaker: Optional[str] = None
        # Inicjalizuj agent√≥w AutoGen
        self.enable_sanity_ping = False
        process_log(f"[CFG] enable_sanity_ping={self.enable_sanity_ping}")
        self._initialize_autogen_agents()
        self._secret_cache = {}
        
        process_log(f"=== AutoGen MOA Orchestrator initialized for mission: {mission[:100]}... ===")
    
    
    def reset(self):
        # ... resetuje liczniki ...
        # U≈ºywa wbudowanej metody .reset() do wyczyszczenia historii ka≈ºdego agenta
        all_agents = [self.user_proxy, *self.proposer_agents, self.aggregator_agent, self.critic_agent]
        for agent in all_agents:
            if agent:
                agent.reset()
    
    def _get_api_key_from_gcp_secret_manager(self, model_cfg: Dict) -> str | None:
        """
        Czyta klucz z GCP Secret Manager.
        Oczekuje: model_cfg["secret_manager"] = {"project_id": "...", "secret_id": "...", "version": "latest"|"1"|...}
        Zwraca: string lub None (gdy brak/nieudane).
        """
        sm = model_cfg.get("secret_manager") or {}
        project_id = (sm.get("project_id") or "").strip()
        secret_id  = (sm.get("secret_id") or "").strip()
        version    = (sm.get("version") or "latest").strip()

        if not project_id or not secret_id:
            return None

        cache_key = (project_id, secret_id, version)
        if cache_key in self._secret_cache:
            return self._secret_cache[cache_key]

        try:
            client = secretmanager.SecretManagerServiceClient()
            name = f"projects/{project_id}/secrets/{secret_id}/versions/{version}"
            resp = client.access_secret_version(name=name)
            value = resp.payload.data.decode("utf-8")
            # cache in-memory (nie logujemy!)
            self._secret_cache[cache_key] = value
            return value
        except Exception as e:
            # Nie loguj warto≈õci sekretu. Mo≈ºesz zalogowaƒá TYLKO metadane.
            from process_logger import log as process_log
            process_log(f"[SECRETS] Failed to read {secret_id}@{project_id}/{version}: {type(e).__name__}: {e}")
            return None
    
    #Raport:
    def _ensure_dir(self, path: str):
        os.makedirs(path, exist_ok=True)

    def _now_stamp(self) -> str:
        return time.strftime("%Y%m%d_%H%M%S", time.localtime())

    def _extract_llm_hint(self, text: str) -> Optional[str]:
        """Prosta heurystyka do rozpoznawania typowych problem√≥w LLM-a."""
        if not text:
            return None
        t = text.lower()
        hints = {
            "quota/rate_limit": ["rate limit", "too many requests", "quota", "insufficient_quota"],
            "context_length": ["maximum context length", "token limit", "context window", "too many tokens"],
            "safety": ["safety", "blocked", "content filter"],
            "auth/api": ["invalid api key", "unauthorized", "forbidden", "permission"],
            "timeout": ["timeout", "timed out", "deadline exceeded"]
        }
        for label, kws in hints.items():
            if any(k in t for k in kws):
                return label
        return None

    def _write_failure_report(
        self,
        reason: str,
        stage: str,
        aggregator_raw: Optional[str],
        critic_raw: Optional[str],
        exception: Optional[BaseException] = None,
        parsed_aggregator: Optional[Dict[str, Any]] = None
    ) -> str:
        """Zapisuje raport awaryjny JSON + MD i zwraca ≈õcie≈ºkƒô do pliku JSON."""
        self._ensure_dir("reports")
        ts = self._now_stamp()
        jpath = f"reports/failure_report_{ts}.json"
        mpath = f"reports/failure_report_{ts}.md"

        agg_hint = self._extract_llm_hint(aggregator_raw or "")
        crit_hint = self._extract_llm_hint(critic_raw or "")

        report = {
            "timestamp": ts,
            "mission": self.mission,
            "stage": stage,  # np. "aggregator", "groupchat", "critic"
            "reason": reason,  # np. "AGGREGATOR_NO_VALID_JSON", "EXCEPTION_DURING_DEBATE"
            "aggregator_model": getattr(self, "aggregator_config", {}).get("model", {}),
            "critic_model": getattr(self, "critic_config", {}).get("model", {}),
            "aggregator_output_excerpt": (aggregator_raw or "")[:4000],
            "critic_output_excerpt": (critic_raw or "")[:4000],
            "aggregator_llm_hint": agg_hint,
            "critic_llm_hint": crit_hint,
            "parsed_aggregator": parsed_aggregator,
            "exception": None if not exception else {
                "type": type(exception).__name__,
                "message": str(exception),
                "traceback": traceback.format_exc()
            }
        }

        with open(jpath, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # kr√≥tkie MD dla ludzi
        with open(mpath, "w", encoding="utf-8") as f:
            f.write(f"# Failure Report ({ts})\n\n")
            f.write(f"**Mission:** {self.mission}\n\n")
            f.write(f"**Stage:** {stage}\n\n")
            f.write(f"**Reason:** {reason}\n\n")
            if agg_hint:
                f.write(f"**Aggregator LLM hint:** `{agg_hint}`\n\n")
            if crit_hint:
                f.write(f"**Critic LLM hint:** `{crit_hint}`\n\n")
            if exception:
                f.write(f"**Exception:** `{type(exception).__name__}: {exception}`\n\n")
            f.write("## Last Aggregator Output (excerpt)\n\n")
            f.write("```\n" + (aggregator_raw or "")[:4000] + "\n```\n\n")
            f.write("## Last Critic Output (excerpt)\n\n")
            f.write("```\n" + (critic_raw or "")[:4000] + "\n```\n")

        
        process_log(f"[FAILSAFE] Saved failure report: {jpath}")

        return jpath

    def _get_last_message_from(self, groupchat, agent_name: str) -> Optional[str]:
        """Zwraca tekst ostatniej wiadomo≈õci danego agenta z obiektu GroupChat."""
        try:
            msgs = getattr(groupchat, "messages", [])
            for m in reversed(msgs):
                if (m.get("name") or m.get("role")) == agent_name:
                    return m.get("content") or ""
        except Exception:
            pass
        return None
    
    
    # ========== UNIVERSAL JSON REPAIR ==========

    MAX_REPAIR_ATTEMPTS = 2
    REPAIR_JSON_SUFFIX = "\n\nZWR√ìƒÜ TYLKO I WY≈ÅƒÑCZNIE JSON, bez komentarzy, bez dodatkowego tekstu."

    def _schema_example_for(self, role: str) -> str:
        if role == "proposer":
            return (
                '{\n'
                '  "thought_process": ["Krok 1...", "Krok 2..."],\n'
                '  "plan": {\n'
                '    "entry_point": "Start_Node",\n'
                '    "nodes": [ {"name":"Start_Node","implementation":"load_data"} ],\n'
                '    "edges": [ {"from":"Start_Node","to":"Next_Node","condition":"on_success"} ]\n'
                '  },\n'
                '  "confidence": 0.80\n'
                '}'
            )
        if role == "aggregator":
            return (
                '{\n'
                '  "thought_process": ["Agregujƒô elementy X i Y..."],\n'
                '  "final_plan": {\n'
                '    "entry_point": "Start_Node",\n'
                '    "nodes": [ {"name":"Start_Node","implementation":"load_data"} ],\n'
                '    "edges": [ {"from":"Start_Node","to":"Next_Node","condition":"on_success"} ]\n'
                '  },\n'
                '  "confidence_score": 0.90\n'
                '}'
            )
        if role == "critic":
            return (
                '{\n'
                '  "critique_summary": {\n'
                '    "verdict": "ZATWIERDZONY",\n'
                '    "statement": "Uzasadnienie...",\n'
                '    "key_strengths": ["..."],\n'
                '    "identified_weaknesses": [{"weakness":"...", "severity":"Low", "description":"..."}]\n'
                '  },\n'
                '  "quality_metrics": {\n'
                '    "Complexity_Score_C": 3.1,\n'
                '    "Robustness_Score_R": 50,\n'
                '    "Innovation_Score_I": 100,\n'
                '    "Completeness_Score": 100,\n'
                '    "Overall_Quality_Q": 84.07\n'
                '  },\n'
                '  "final_synthesized_plan": {\n'
                '    "entry_point": "Start_Node",\n'
                '    "nodes": [ {"name":"Start_Node","implementation":"load_data"} ],\n'
                '    "edges": [ {"from":"Start_Node","to":"Next_Node","condition":"on_success"} ]\n'
                '  }\n'
                '}'
            )
        return "{}"

    def _try_parse_by_role(self, role: str, text: str):
        try:
            if role == "proposer":
                parsed = self.parser.parse_agent_response(text)
            elif role == "aggregator":
                parsed = self.parser.parse_aggregator_response(text)
            elif role == "critic":
                parsed = self.parser.parse_critic_response(text)
            else:
                return None, f"Unknown role: {role}"
            if parsed:
                return parsed, None
            return None, "Parser returned None"
        except Exception as e:
            return None, f"{type(e).__name__}: {e}"

    def _repair_prompt_for(self, role: str, err_msg: str) -> str:
        return (
            f"Twoja poprzednia odpowied≈∫ NIE SPE≈ÅNIA wymaganego schematu dla roli '{role}'.\n"
            f"B≈ÇƒÖd/diagnoza parsera: {err_msg}\n\n"
            f"Wymagana struktura JSON (minimalny przyk≈Çad):\n{self._schema_example_for(role)}\n"
            f"{REPAIR_JSON_SUFFIX}"
        )

    def _force_one_turn(self, agent, manager) -> str:
        self._forced_speaker = agent.name
        try:
            manager.step()  # je≈õli Twoja wersja AG2 nie wspiera .step(), u≈ºyj run(max_round=1)
        except Exception:
            pass
        return self._get_last_message_from(manager.groupchat, agent.name) or ""

    def _auto_repair_and_parse(self, role: str, agent, manager, last_text: str):
        parsed, err = self._try_parse_by_role(role, last_text or "")
        if parsed:
            return parsed
        
        for attempt in range(1, MAX_REPAIR_ATTEMPTS + 1):
            repair_msg = self._repair_prompt_for(role, err or "Invalid JSON")
            manager.groupchat.messages.append({
                "role": "user",
                "name": "Orchestrator",
                "content": repair_msg
            })
            process_log(f"[REPAIR][{role}] attempt {attempt}: requesting strictly JSON output.")
            repaired_text = self._force_one_turn(agent, manager)
            parsed, err2 = self._try_parse_by_role(role, repaired_text or "")
            if parsed:
                return parsed
            err = err2
        return None
    
    
    
    def _load_config(self, config_file: str):
        """Wczytuje konfiguracjƒô agent√≥w"""
        with open(config_file, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
    
    
    def _is_final_plan_message(self, m: dict) -> bool:
        """Ko≈Ñczymy TYLKO na odpowiedzi CRITICA, gdy ko≈Ñczy siƒô markerem."""
        content = (m.get("content") or "").strip()
        name = (m.get("name") or "").lower()
        role = (m.get("role") or "").lower()
        return role == "assistant" and content.endswith("PLAN_ZATWIERDZONY") and "critic" in name
    
    
#     def custom_speaker_selection_logic(self, last_speaker: ConversableAgent, groupchat: GroupChat):
#         """
#         ZarzƒÖdza cyklem debaty: Proposerzy -> Aggregator -> Krytyk.
#         """
#         messages = groupchat.messages

#         # **POPRAWKA:** Je≈õli rozmowa dopiero siƒô zaczyna (tylko 1 wiadomo≈õƒá od Orchestratora),
#         # zawsze zaczynaj od pierwszego proposera.
#         if len(messages) <= 1:
#             return self.proposer_agents[0]

#         if last_speaker.name == "Master_Aggregator":
#             return self.critic_agent

#         if last_speaker.name == "Quality_Critic":
#             last_message_content = messages[-1].get("content", "").upper()
#             if "PLAN_ZATWIERDZONY" in last_message_content:
#                 return None  # Zako≈Ñcz debatƒô

#             self.iteration_count += 1
#             if self.iteration_count >= self.max_iterations:
#                 process_log(f"Max iterations ({self.max_iterations}) reached. Ending debate.")
#                 return None
            
#             process_log(f"--- Starting iteration {self.iteration_count + 1} ---")
#             self._update_context_from_last_critique(messages[-1].get("content", ""))
#             return self.proposer_agents[0]

#         if last_speaker in self.proposer_agents:
#             try:
#                 idx = self.proposer_agents.index(last_speaker)
#                 if idx < len(self.proposer_agents) - 1:
#                     return self.proposer_agents[idx + 1]
#                 else:
#                     return self.aggregator_agent
#             except ValueError:
#                 return self.aggregator_agent
        
#         # Domy≈õlny fallback na wszelki wypadek
#         return self.proposer_agents[0]

    def custom_speaker_selection_logic(self, last_speaker, groupchat):
        """
        Proposers ‚Üí Aggregator ‚Üí Critic. Je≈õli Critic nie zatwierdzi, nowa iteracja od pierwszego Proposera.
        Por√≥wnujemy po NAZWACH z historii wiadomo≈õci (AutoGen mo≈ºe podawaƒá inne instancje agent√≥w).
        """
        msgs = groupchat.messages
        
        for msg in msgs:
            if "PLAN_ZATWIERDZONY" in msg.get("content", ""):
                raise StopIteration("Plan zatwierdzony - ko≈Ñczymy debatƒô")
        
        last_name = (msgs[-1].get("name") or "").lower() if msgs else ""
        last_content = (msgs[-1].get("content") or "")

        # ‚ù∂ Po bootstrapie (ostatni by≈Ç Orchestrator ‚Üí wybieramy pierwszego proposera)
        if last_name == (self.user_proxy.name or "").lower() and last_content.strip():
            return self.proposer_agents[0]

        # ‚ù∑ Po Aggregatorze ‚Üí czas na Critica
        if last_name == (self.aggregator_agent.name or "").lower():
            return self.critic_agent

        # ‚ù∏ Po Criticu ‚Üí zatwierdzenie albo nowa iteracja
        if last_name == (self.critic_agent.name or "").lower():
            self._save_iteration_to_memory(last_content, self.iteration_count)
            if "PLAN_ZATWIERDZONY" in last_content:
                return None
            # nowa iteracja
            self.iteration_count += 1
            if self.iteration_count >= self.max_iterations:
                process_log(f"[FAILSAFE] OsiƒÖgniƒôto maksymalnƒÖ liczbƒô iteracji ({self.max_iterations}). Koniec debaty.")
                return None
            process_log(f"===== ROZPOCZYNAM ITERACJƒò DEBATY NR {self.iteration_count + 1} =====")
            self._update_context_from_last_critique(last_content)
            return self.proposer_agents[0]

        # ‚ùπ WewnƒÖtrz puli proposer√≥w ‚Äì leƒá kolejno po nazwach
        proposer_names = [p.name.lower() for p in self.proposer_agents]
        if last_name in proposer_names:
            idx = proposer_names.index(last_name)
            if idx < len(self.proposer_agents) - 1:
                return self.proposer_agents[idx + 1]
            return self.aggregator_agent  # po ostatnim proposerze m√≥wi Aggregator

        # ‚ù∫ Domy≈õlnie ‚Äì zacznij od pierwszego proposera
        return self.proposer_agents[0]
    
    
    def _save_iteration_to_memory(self, critic_response: str, iteration: int):
        """Zapisuje dane z iteracji do pamiƒôci"""
        try:
            # Parse odpowiedzi krytyka
            parsed = self.parser.parse_critic_response(critic_response)
            if not parsed:
                process_log(f"[MEMORY] Nie mogƒô sparsowaƒá odpowiedzi krytyka w iteracji {iteration}")
                return

            # WyciƒÖgnij kluczowe dane
            score = parsed.get("quality_metrics", {}).get("Overall_Quality_Q", 0)
            weaknesses = parsed.get("critique_summary", {}).get("identified_weaknesses", [])
            verdict = parsed.get("critique_summary", {}).get("verdict", "")

            # Stw√≥rz feedback string
            feedback_data = {
                "score": score,
                "verdict": verdict,
                "weaknesses": [w.get("weakness", "") for w in weaknesses if isinstance(w, dict)],
                "iteration": iteration
            }

            # ZAPISZ DO PAMIƒòCI
            self.memory.add_iteration_feedback(
                iteration=iteration,
                feedback=json.dumps(feedback_data),
                timestamp=datetime.now()
            )

            process_log(f"[MEMORY] Zapisano iteracjƒô {iteration}: score={score}, verdict={verdict}")

        except Exception as e:
            process_log(f"[MEMORY ERROR] B≈ÇƒÖd zapisu iteracji {iteration}: {e}")

    
    def _initialize_autogen_agents(self):
        """Inicjalizuje agent√≥w AutoGen dla debaty ‚Äî minimalistycznie i niezawodnie."""
        # Atrybuty ZAWSZE istniejƒÖ
        self.proposer_agents = []
        self.aggregator = None
        self.critic = None
        self.aggregator_agent = None
        self.critic_agent = None

        # User Proxy ‚Äì nigdy nie ko≈Ñczy rozmowy
        self.user_proxy = autogen.ConversableAgent( # <-- POPRAWKA
        name="Orchestrator",
        human_input_mode="NEVER",
        llm_config=False,  # Ten agent nie potrzebuje LLM, tylko rozpoczyna rozmowƒô
        system_message="You are the orchestrator who starts the debate and then observes."
        )
        self.user_proxy.silent = False
        process_log("[INIT] UserProxy initialized")

        # Proposerzy
        for agent_config in self.config['agents']:
            rn = agent_config['role_name'].lower()
            if 'aggregator' in rn or 'critic' in rn:
                continue
            role = AgentRole(
                role_name=agent_config['role_name'],
                expertise_areas=agent_config['expertise_areas'],
                thinking_style=agent_config['thinking_style']
            )
            prompt = self._build_proposer_prompt(role)
            ag = autogen.ConversableAgent(
                name=agent_config['role_name'].replace(" ", "_"),
                llm_config=self._build_llm_config(agent_config['model']),
                system_message=prompt,
                human_input_mode="NEVER"
            )
            ag.silent = False
            self.proposer_agents.append(ag)
            process_log(f"[INIT] Proposer initialized: {ag.name}")

        # Aggregator
        aggregator_config = next((a for a in self.config['agents'] if 'aggregator' in a['role_name'].lower()), None)
        if aggregator_config:
            self.aggregator = autogen.ConversableAgent(
                name="Master_Aggregator",
                llm_config=self._build_llm_config(aggregator_config['model']),
                system_message=MOAPrompts.get_aggregator_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=lambda m: False,
            )
        else:
            self.aggregator = autogen.ConversableAgent(
                name="Master_Aggregator",
                llm_config={"config_list": [{"model": "dummy", "api_type": "dummy"}]},
                system_message=MOAPrompts.get_aggregator_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=lambda m: False,
            )
        self.aggregator.silent = False
        self.aggregator_agent = self.aggregator
        process_log("[INIT] Aggregator initialized")

        # Critic
        critic_config = next((a for a in self.config['agents'] if 'critic' in a['role_name'].lower()), None)
        if critic_config:
            self.critic = autogen.ConversableAgent(
                name="Quality_Critic",
                llm_config=self._build_llm_config(critic_config['model']),
                system_message=self._build_critic_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=self._is_final_plan_message,
            )
        else:
            self.critic = autogen.ConversableAgent(
                name="Quality_Critic",
                llm_config={"config_list": [{"model": "dummy", "api_type": "dummy"}]},
                system_message=self._build_critic_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=self._is_final_plan_message,
            )
        self.critic.silent = False
        self.critic_agent = self.critic
        process_log("[INIT] Critic initialized")

        process_log(f"Initialized {len(self.proposer_agents)} proposers, 1 aggregator, 1 critic using AutoGen")
    
    
    def _runtime_env_snapshot(self) -> dict:
        # tylko presence, bez warto≈õci
        def present(k): return bool(os.getenv(k))
        return {
            "VERTEXAI_PROJECT": present("VERTEXAI_PROJECT") or present("GOOGLE_CLOUD_PROJECT") or present("GCP_PROJECT"),
            "VERTEXAI_LOCATION": present("VERTEXAI_LOCATION") or present("GOOGLE_CLOUD_REGION"),
            "ANTHROPIC_API_KEY": present("ANTHROPIC_API_KEY"),
            "OPENAI_API_KEY": present("OPENAI_API_KEY"),
        }

    def _agent_signature(self, agent) -> dict:
        llm = getattr(agent, "llm_config", {})
        # wyciƒÖgamy pierwszy wpis z config_list dla kr√≥tkiego podpisu
        vendor = None; model = None
        try:
            entry = (llm.get("config_list") or [{}])[0]
            if "google" in entry:
                vendor = "google"; model = entry["google"].get("model")
            elif "anthropic" in entry:
                vendor = "anthropic"; model = entry["anthropic"].get("model")
            elif "openai" in entry:
                vendor = "openai"; model = entry["openai"].get("model")
            else:
                vendor = (entry.get("api_type") or "unknown")
                model = entry.get("model")
        except Exception:
            pass
        return {"name": getattr(agent, "name", "?"), "vendor": vendor, "model": model}

    def _sanity_ping_agent(self, agent) -> None:
        tmp_user = UserProxyAgent(
            "sanity_user", human_input_mode="NEVER", max_consecutive_auto_reply=1,
            is_termination_msg=lambda m: True, code_execution_config=False
        )
        try:
            tmp_user.initiate_chat(agent, message="Odpowiedz dok≈Çadnie s≈Çowem: PONG")
        except Exception as e:
            sig = self._agent_signature(agent)
            snap = self._runtime_env_snapshot()
            raise RuntimeError(
                f"[SANITY PING FAILED] agent={sig} | env={snap} | err={type(e).__name__}: {e}"
            ) from e
    
    
    
    
    
    
    def _build_llm_config(self, model_config: dict) -> dict:
        """
        Buduje llm_config dla AutoGen na bazie agents_config.json,
        u≈ºywajƒÖc config_api.basic_config_agent (ten sam format co w solo).
        Google => Vertex/ADC (bez api_key), Anthropic/OpenAI => klucze z ENV/SM.
        """
        from config_api import basic_config_agent, PROJECT_ID as DEFAULT_PROJECT_ID, LOCATION as DEFAULT_LOCATION

        # --- helpery ---
        DEFAULT_MODEL_BY_PROVIDER = {
            "google": "gemini-2.5-pro",
            "anthropic": "claude-3-7-sonnet",
            "openai": "gpt-4o-mini",
        }

        def _map_provider_to_api_type(provider: str) -> str:
            p = (provider or "google").strip().lower()
            return {
                "google": "google", "gemini": "google", "vertex": "google",
                "anthropic": "anthropic",
                "openai": "openai", "azure_openai": "openai",
            }.get(p, p)

        def _validate_provider_model_pair(api_type: str, model: str) -> None:
            m = (model or "").lower()
            if api_type == "google" and not m.startswith("gemini"):
                raise ValueError(f"Model '{model}' nie pasuje do providera 'google' (Vertex/Gemini).")
            if api_type == "anthropic" and not m.startswith("claude"):
                raise ValueError(f"Model '{model}' nie pasuje do 'anthropic'.")
            if api_type == "openai" and not ("gpt" in m or m.startswith("o")):
                raise ValueError(f"Model '{model}' nie wyglƒÖda na model OpenAI.")

        # 1) provider -> api_type
        api_type = _map_provider_to_api_type(model_config.get("provider"))

        # 2) model + sanity
        agent_name = model_config.get("model_name") or DEFAULT_MODEL_BY_PROVIDER.get(api_type, "gemini-2.5-pro")
        _validate_provider_model_pair(api_type, agent_name)

        # 3) projekt/region tylko dla Google/Vertex
        project_id = model_config.get("project_id") or DEFAULT_PROJECT_ID
        location   = model_config.get("location")   or DEFAULT_LOCATION
        if api_type == "google" and not project_id:
            raise RuntimeError("Vertex/Gemini: brak project_id. Ustaw VERTEXAI_PROJECT/GOOGLE_CLOUD_PROJECT albo podaj 'project_id' w agents_config.json.")

        # 4) api_key tylko dla nie-Google
        api_key_arg = None if api_type == "google" else model_config.get("api_key")

        # 5) wo≈Çamy Tw√≥j builder
        flat_list = basic_config_agent(
            agent_name = agent_name,
            api_type   = api_type,
            location   = (location if api_type == "google" else None),   # <-- KLUCZOWA ZMIANA
            project_id = (project_id if api_type == "google" else None), # <-- KLUCZOWA ZMIANA
            api_key    = api_key_arg,
        )
        if not isinstance(flat_list, list) or not flat_list:
            raise ValueError("basic_config_agent powinien zwr√≥ciƒá niepustƒÖ listƒô.")

        entry = dict(flat_list[0])  # kopia, ≈ºeby m√≥c czy≈õciƒá

        # 6) sanity: dla nie-Google WYTNJIJ project/location (gdyby kiedy≈õ zn√≥w wpad≈Çy)
        if api_type != "google":
            entry.pop("project_id", None)
            entry.pop("location", None)

        # 7) finalny llm_config
        return {
            "config_list": [entry],
            "temperature": float(model_config.get("temperature", 0.0)),
            "seed": 42,
            "cache_seed": 42,
        }
       
    
    def _build_proposer_prompt(self, role: AgentRole) -> str:
        """Buduje prompt dla proposera z kontekstem"""
        base_prompt = MOAPrompts.get_proposer_prompt(role, self.mission, self.node_library)
        
        # Dodaj dynamiczny kontekst
        if self.current_context:
            context_injection = self._build_context_injection()
            return base_prompt + "\n\n" + context_injection
        
        return base_prompt
    
    def _build_critic_prompt(self) -> str:
        """Buduje prompt dla krytyka"""
        base_prompt = MOAPrompts.get_critic_prompt()

        # Dodaj specjalnƒÖ instrukcjƒô o frazie ko≈ÑczƒÖcej i nowej strukturze JSON
        additional_instruction = """

        ## CRITICAL OUTPUT STRUCTURE
        - If you REJECT the plan, provide your standard critique with weaknesses and suggestions.
        - If you APPROVE the plan, your JSON response MUST contain a top-level key named `plan_approved`. Inside this key, you MUST place the complete, final, synthesized plan object. The other keys (like critique_summary) should still be present.

        Example of an APPROVED response structure:
        ```json
        {
          "critique_summary": {
            "verdict": "ZATWIERDZONY",
            "statement": "Plan jest doskona≈Çy, spe≈Çnia wszystkie wymagania.",
            ...
          },
          "plan_approved": {
            "entry_point": "Start_Node",
            "nodes": [ ... ],
            "edges": [ ... ]
          },
          ...
        }
        ```

        ## GOLDEN TERMINATION RULE
        If you approve the plan, you MUST end your ENTIRE response with the exact phrase on a new line, after the JSON block:
        PLAN_ZATWIERDZONY
        """

        return base_prompt + additional_instruction
    
    def _build_context_injection(self) -> str:
        """Buduje wstrzykniƒôcie kontekstu"""
        parts = []
        
        if self.current_context.get('recommended_strategies'):
            parts.append("## üí° RECOMMENDED STRATEGIES (from memory):")
            for strategy in self.current_context['recommended_strategies']:
                parts.append(f"‚Ä¢ {strategy}")
        
        if self.current_context.get('common_pitfalls'):
            parts.append("\n## ‚ö†Ô∏è COMMON PITFALLS TO AVOID:")
            for pitfall in self.current_context['common_pitfalls']:
                parts.append(f"‚Ä¢ {pitfall}")
        
        if self.current_context.get('last_feedback'):
            parts.append(f"\n## üìù LAST FEEDBACK:\n{self.current_context['last_feedback']}")
        
        return "\n".join(parts)
    

    def run_full_debate_cycle(self):
        from autogen import GroupChat, GroupChatManager
        import json, os, traceback
        from datetime import datetime
        self.reset()
        # Lazy-guard: je≈õli kto≈õ zawo≈Ça przed init
        for must in ("user_proxy", "proposer_agents", "aggregator_agent", "critic_agent"):
            if not hasattr(self, must) or getattr(self, must) is None:
                self._initialize_autogen_agents()
                break

        # Szybkie asserty z czytelnym komunikatem
        if not self.proposer_agents:
            raise RuntimeError("Brak proposer√≥w. Sprawd≈∫ agents_config.json (role bez 'aggregator'/'critic').")
        if not self.aggregator_agent:
            raise RuntimeError("Brak agregatora. Sprawd≈∫ agents_config.json (rola 'Aggregator').")
        if not self.critic_agent:
            raise RuntimeError("Brak krytyka. Sprawd≈∫ agents_config.json (rola 'Critic').")

        max_rounds = len(self.proposer_agents) + 2

        # Bootstrap misji ‚Äì bez 'PLAN_ZATWIERDZONY' w tre≈õci, ≈ºeby manager nie ko≈Ñczy≈Ç po 1 msg
        bootstrap = (
            f"## MISJA\n{self.mission}\n\n"
    "Zaproponuj kompletny PLAN w formacie JSON {entry_point, nodes[], edges[]}.\n"
    "Rola: Proposerzy proponujƒÖ swoje wersje planu. Nastƒôpnie Aggregator scala je w jednƒÖ, sp√≥jnƒÖ propozycjƒô. "
    "Na ko≈Ñcu, Quality_Critic oceni finalny, zagregowany plan."
        )

        # Uczestnicy ‚Äì tylko agenci
        agents = [*self.proposer_agents, self.aggregator_agent, self.critic_agent]

        turns_per_iteration = len(self.proposer_agents) + 2 
        max_rounds = self.max_iterations * turns_per_iteration + 5 # Dodajemy bufor bezpiecze≈Ñstwa

        gc = GroupChat(
            agents=agents,
            messages=[],
            max_round=max_rounds, # U≈ºywamy nowej, dynamicznie obliczonej warto≈õci
            speaker_selection_method=self.custom_speaker_selection_logic)
        
        manager = GroupChatManager(
            groupchat=gc,
            llm_config=self.aggregator_agent.llm_config,
            human_input_mode="NEVER",
            system_message=MOAPrompts.get_aggregator_prompt(),
            is_termination_msg=self._is_final_plan_message
        )

        try:
            # Start rozmowy ‚Äì to uruchamia ca≈ÇƒÖ maszynkƒô
            self.user_proxy.initiate_chat(manager, message=bootstrap, max_turns=max_rounds)
        except StopIteration:
            # To jest OK - plan zosta≈Ç zatwierdzony
            process_log("[SUCCESS] Debata zako≈Ñczona przez StopIteration - plan zatwierdzony")
        try:
            # Szukamy finalnej odpowiedzi
            final_plan_message_content = None
            messages = manager.groupchat.messages
            for msg in reversed(messages):
                # U≈ºywamy Twojej nowej, precyzyjnej funkcji sprawdzajƒÖcej
                if "PLAN_ZATWIERDZONY" in msg.get("content", ""):
                    final_plan_message_content = msg.get("content")
                    break
                    

            # Je≈õli znaleziono zatwierdzonƒÖ wiadomo≈õƒá, sparsuj jƒÖ
            if final_plan_message_content:
                process_log("[SUCCESS] Krytyk zatwierdzi≈Ç plan. Rozpoczynam parsowanie...")
                try:
                    parsed_critic_response = self.parser.parse_critic_response(final_plan_message_content)

                    # TUTAJ WKLEJ NOWY KOD (zamiast linii 67-75):
                    if parsed_critic_response:
                        # Szukaj planu w r√≥≈ºnych mo≈ºliwych miejscach
                        final_plan = None

                        # Lista mo≈ºliwych kluczy
                        possible_keys = [
                            "plan_approved",
                            "final_synthesized_plan", 
                            "final_plan",
                            "synthesized_plan",
                            "approved_plan",
                            "plan"
                        ]

                        for key in possible_keys:
                            if key in parsed_critic_response:
                                candidate = parsed_critic_response[key]
                                # Sprawd≈∫ czy to wyglƒÖda jak plan (ma entry_point i nodes)
                                if isinstance(candidate, dict) and "entry_point" in candidate and "nodes" in candidate:
                                    final_plan = candidate
                                    process_log(f"[SUCCESS] Znaleziono plan pod kluczem: '{key}'")
                                    break

                        if final_plan:
                            self.final_plan = final_plan
                            self._save_successful_plan()
                            
                            
                            
                            # Zbierz stan orchestratora
                            orchestrator_state = {
                                "iteration_count": self.iteration_count,
                                "execution_time": (datetime.now() - start_time).total_seconds() if 'start_time' in locals() else 0,
                                "total_tokens": getattr(self, 'token_counter', 0),
                                "api_calls": getattr(self, 'api_call_counter', 0)
                            }

                            # Zapisz KOMPLETNƒÑ misjƒô
                            mission_id = self.memory.save_complete_mission(
                            mission=self.mission,
                            final_plan=self.final_plan,
                            all_messages=manager.groupchat.messages,
                            orchestrator_state=orchestrator_state
                            )

                            process_log(f"[ORCHESTRATOR] Mission completed and saved as: {mission_id}")
    
                            
                            
                            return self.final_plan
                        else:
                            # Je≈õli nie znaleziono planu w ≈ºadnym kluczu
                            raise RuntimeError(f"Nie znaleziono planu w odpowiedzi. Dostƒôpne klucze: {list(parsed_critic_response.keys())}")
                    else:
                        raise RuntimeError("Parser zwr√≥ci≈Ç None - nie uda≈Ço siƒô sparsowaƒá JSON")
            
                except Exception as parse_error:
                    # Sytuacja awaryjna: nie uda≈Ço siƒô sparsowaƒá odpowiedzi krytyka
                    process_log(f"[ERROR] Nie uda≈Ço siƒô sparsowaƒá odpowiedzi krytyka: {parse_error}")
                    # Zapisz raport z surowƒÖ odpowiedziƒÖ do analizy
                    self._write_failure_report(
                        reason="CRITIC_RESPONSE_PARSE_FAILURE",
                        stage="post-debate_parsing",
                        aggregator_raw=None, # Nieistotne na tym etapie
                        critic_raw=final_plan_message_content,
                        exception=parse_error
                    )
                    return None # Zwracamy None w przypadku b≈Çƒôdu parsowania
            else:
                # Je≈õli pƒôtla siƒô zako≈Ñczy≈Ça i nie znaleziono zatwierdzonej wiadomo≈õci
                raise RuntimeError("Debata zako≈Ñczona, ale krytyk nigdy nie zwr√≥ci≈Ç wiadomo≈õci z 'PLAN_ZATWIERDZONY'.")

        except Exception as e:
            # Raport diagnostyczny
            tb = traceback.format_exc()
            os.makedirs("reports", exist_ok=True)
            path = f"reports/failure_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(path, "w", encoding="utf-8") as f:
                json.dump({"error_type": type(e).__name__,
                           "error_message": str(e),
                           "stacktrace": tb}, f, ensure_ascii=False, indent=2)
            process_log(f"[FAILSAFE] Saved failure report: {path}")
            process_log(tb)
            return None
    
   

    
        
    
    def _update_context_from_last_critique(self, critique_message: str):
        """Aktualizuje kontekst na podstawie krytyki"""
        # Parsuj krytykƒô
        parsed = self.parser.parse_agent_response(critique_message)
        
        if parsed:
            feedback = f"Score: {parsed.get('score', 'N/A')}. "
            feedback += f"Weaknesses: {', '.join(parsed.get('weaknesses', []))}. "
            feedback += f"Improvements: {', '.join(parsed.get('improvements', []))}"
            
            self.current_context['last_feedback'] = feedback
            
            # Zapisz do pamiƒôci
            self.memory.add_iteration_feedback(
                iteration=self.iteration_count,
                feedback=feedback,
                timestamp=datetime.now()
            )
        
        # Od≈õwie≈º kontekst z pamiƒôci
        self.current_context = self.memory.get_relevant_context(self.mission)
        
        process_log(f"Context updated for iteration {self.iteration_count}")
    
    def _extract_final_plan(self, messages: List[Dict]):
        """Wyodrƒôbnia zatwierdzony plan z historii wiadomo≈õci"""
        # Szukaj od ko≈Ñca
        for msg in reversed(messages):
            content = msg.get("content", "")
            name = msg.get("name", "")
            
            # Je≈õli krytyk zatwierdzi≈Ç
            if name == "Quality_Critic" and "PLAN_ZATWIERDZONY" in content:
                # Znajd≈∫ ostatni plan od agregatora
                for prev_msg in reversed(messages):
                    if prev_msg.get("name") == "Master_Aggregator":
                        parsed = self.parser.parse_agent_response(prev_msg.get("content", ""))
                        if parsed:
                            self.final_plan = parsed.get("final_plan", parsed.get("plan"))
                            break
                break
        
        process_log(f"Final plan extracted: {self.final_plan is not None}")
    
    def _save_successful_plan(self):
        """Zapisuje udany plan do pamiƒôci i pliku"""
        if not self.final_plan:
            return
        
        # Zapisz do pamiƒôci
        self.memory.add_successful_plan(
            plan=self.final_plan,
            mission=self.mission,
            metadata={
                'iterations': self.iteration_count,
                'agents_count': len(self.proposer_agents)
            }
        )
        
        # Zapisz do pliku
        output = {
            "mission": self.mission,
            "final_plan": self.final_plan,
            "metadata": {
                "iterations": self.iteration_count,
                "timestamp": datetime.now().isoformat(),
                "autogen_debate": True
            }
        }
        
        os.makedirs("outputs", exist_ok=True)
        output_file = f"outputs/autogen_plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Plan saved to: {output_file}")
        process_log(f"Successful plan saved to {output_file}")
        
        
    def _debug_dump_transcript(self, groupchat, tail: int = 30):
        """Wypisz ostatnie ~N wiadomo≈õci debaty, ≈ºeby by≈Ço je widaƒá w notebooku."""
        from process_logger import log as process_log
        try:
            msgs = getattr(groupchat, "messages", [])[-tail:]
            process_log("----- TRANSCRIPT (tail) -----")
            for m in msgs:
                role = m.get("role") or m.get("name") or "?"
                name = m.get("name") or ""
                content = m.get("content") or ""
                head = (content[:400] + "...") if len(content) > 400 else content
                process_log(f"{role} {name}: {head}")
            process_log("----- END TRANSCRIPT -----")
        except Exception as e:
            process_log(f"[TRANSCRIPT_DUMP_FAIL] {type(e).__name__}: {e}")


--- FILE: config_api.py ---

import os
import logging
from enum import Enum
from google.cloud import secretmanager
import langchain
from langchain.cache import SQLiteCache





def get_secret(project_id: str, secret_id: str, version_id: str = "latest") -> str:
    """Pobiera warto≈õƒá sekretu z Google Secret Manager."""
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"
    response = client.access_secret_version(request={"name": name})
   
    return response.payload.data.decode("UTF-8")


class ApiType(Enum):
    GOOGLE = "google"
    ANTHROPIC = "anthropic"
    OPENAI = "openai"
    def __str__(self):
        return self.value


LOCATION="us-central1"
PROJECT_ID="dark-data-discovery"

#---------AGENTS--------:
MAIN_AGENT="gemini-2.5-pro"
API_TYPE_GEMINI=str(ApiType.GOOGLE)

CRITIC_MODEL="claude-3-7-sonnet-20250219"
ARCHITECT_MODEL ="claude-opus-4-1-20250805"
CODE_MODEL="claude-sonnet-4-20250514"
QUICK_SMART_MODEL="gemini-2.5-flash"

GPT_MODEL = "gpt-4o" # U≈ºywamy gpt-4o jako odpowiednika "gpt-5"
API_TYPE_OPENAI = str(ApiType.OPENAI)

API_TYPE_SONNET = str(ApiType.ANTHROPIC)

LANGCHAIN_API_KEY = get_secret(PROJECT_ID,"LANGCHAIN_API_KEY")
ANTHROPIC_API_KEY=get_secret(PROJECT_ID,"ANTHROPIC_API_KEY")
TAVILY_API_KEY = get_secret(PROJECT_ID,"TAVILY_API_KEY")
OPENAI_API_KEY = get_secret(PROJECT_ID, "OPENAI_API_KEY")

MEMORY_ENGINE_DISPLAY_NAME="memory-gamma-way"

INPUT_FILE_PATH = "gs://super_model/data/structural_data/synthetic_fraud_dataset.csv"

MAX_CORRECTION_ATTEMPTS=5



os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_PROJECT"] = "Projekt Multi-Agent-System Dynamic-graphs"
os.environ["ANTHROPIC_API_KEY"] =ANTHROPIC_API_KEY
os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY
# os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

os.environ.setdefault("MOA_SANITY_PING", "0")
#---cache-------
langchain.llm_cache = SQLiteCache(database_path=".langchain.db")




#FUNKCJA KONFIGURACYJNA AGENTOW AUTOGEN
def basic_config_agent(agent_name:str, api_type:str, location:str=None, project_id:str=None, api_key:str=None):
    try:
        configuration = {"model": agent_name}
        configuration.update({"api_type": api_type})
        if api_key: configuration["api_key"] = api_key
        if project_id: configuration["project_id"] = project_id
        if location: configuration["location"] = location

        logging.info(f"Model configuration: {configuration}")
        return [configuration]

    except Exception as e:
        logging.error(f"Failed to initialize Vertex AI or configure LLM: {e}")
        print(f"Error: Failed to initialize Vertex AI or configure LLM. Please check your project ID, region, and permissions. Details: {e}")
        exit()



--- FILE: extended_llm_wrapper.py ---

"""
Rozszerzony wrapper LLM z bardziej realistycznymi dummy responses dla r√≥≈ºnych r√≥l
"""
import json
import random
from typing import Dict, Any

class ExtendedLLMWrapper:
    """
    Rozszerzona wersja wrappera z r√≥≈ºnorodnymi odpowiedziami dla demo
    """
    
    @staticmethod
    def generate_dummy_response(model_name: str, prompt: str) -> str:
        """Generuje r√≥≈ºne odpowiedzi w zale≈ºno≈õci od typu agenta"""
        
        # Sprawd≈∫ typ agenta na podstawie nazwy modelu lub promptu
        if "causal" in model_name.lower() or "Causal" in prompt:
            return ExtendedLLMWrapper._causal_analyst_response()
        elif "creative" in model_name.lower() or "Creative" in prompt:
            return ExtendedLLMWrapper._creative_planner_response()
        elif "risk" in model_name.lower() or "Risk" in prompt:
            return ExtendedLLMWrapper._risk_analyst_response()
        elif "aggregator" in model_name.lower() or "Aggregator" in prompt:
            return ExtendedLLMWrapper._aggregator_response(prompt)
        elif "critic" in model_name.lower() or "Critic" in prompt:
            return ExtendedLLMWrapper._critic_response(prompt)
        else:
            return ExtendedLLMWrapper._default_response()
    
    @staticmethod
    def _causal_analyst_response() -> str:
        """Odpowied≈∫ analityka przyczynowego"""
        response = {
            "thought_process": [
                "Analizujƒô potencjalne relacje przyczynowe w przep≈Çywie danych",
                "Identyfikujƒô zmienne confounding i mediatory",
                "Projektujƒô DAG (Directed Acyclic Graph) dla workflow"
            ],
            "plan": {
                "entry_point": "validate_data",
                "nodes": [
                    {"name": "validate_data", "implementation": "validate_data"},
                    {"name": "check_quality", "implementation": "check_quality"},
                    {"name": "discover_causality", "implementation": "discover_causality"},
                    {"name": "error_handler", "implementation": "error_handler"},
                    {"name": "validate_model", "implementation": "validate_model"},
                    {"name": "generate_report", "implementation": "generate_report"}
                ],
                "edges": [
                    {"from": "validate_data", "to": "check_quality"},
                    {"from": "check_quality", "to": "discover_causality"},
                    {"from": "discover_causality", "to": "validate_model", "condition": "check_success"},
                    {"from": "discover_causality", "to": "error_handler", "condition": "check_error"},
                    {"from": "error_handler", "to": "discover_causality"},
                    {"from": "validate_model", "to": "generate_report"}
                ]
            },
            "confidence": 0.85,
            "key_innovations": [
                "Dodanie pƒôtli retry dla discover_causality",
                "Walidacja jako≈õci przed analizƒÖ przyczynowƒÖ"
            ],
            "risk_mitigation": {
                "data_quality": "Podw√≥jna walidacja przed analizƒÖ",
                "algorithm_failure": "Error handler z retry mechanism"
            }
        }
        return json.dumps(response)
    
    @staticmethod
    def _creative_planner_response() -> str:
        """Odpowied≈∫ kreatywnego planera"""
        response = {
            "thought_process": [
                "My≈õlƒô nieszablonowo - co gdyby pipeline sam siƒô optymalizowa≈Ç?",
                "Inspiracja z natury: mr√≥wki znajdujƒÖ optymalnƒÖ ≈õcie≈ºkƒô",
                "Dodajƒô element adaptacyjno≈õci i uczenia siƒô"
            ],
            "plan": {
                "entry_point": "load_data",
                "nodes": [
                    {"name": "load_data", "implementation": "load_data"},
                    {"name": "clean_data", "implementation": "clean_data"},
                    {"name": "optimize_performance", "implementation": "optimize_performance"},
                    {"name": "discover_causality", "implementation": "discover_causality"},
                    {"name": "train_model", "implementation": "train_model"},
                    {"name": "notify_user", "implementation": "notify_user"}
                ],
                "edges": [
                    {"from": "load_data", "to": "clean_data"},
                    {"from": "clean_data", "to": "optimize_performance"},
                    {"from": "optimize_performance", "to": "discover_causality"},
                    {"from": "discover_causality", "to": "train_model"},
                    {"from": "train_model", "to": "notify_user"}
                ]
            },
            "confidence": 0.75,
            "key_innovations": [
                "Samooptymalizacja pipeline'u",
                "Proaktywne powiadomienia u≈ºytkownika",
                "Adaptacyjne dostosowanie do typu danych"
            ],
            "risk_mitigation": {
                "performance": "Continuous optimization",
                "user_experience": "Real-time notifications"
            }
        }
        return json.dumps(response)
    
    @staticmethod
    def _risk_analyst_response() -> str:
        """Odpowied≈∫ analityka ryzyka"""
        response = {
            "thought_process": [
                "Identyfikujƒô wszystkie mo≈ºliwe punkty awarii",
                "Analizujƒô cascading failures",
                "Projektujƒô redundancjƒô i fallback paths"
            ],
            "plan": {
                "entry_point": "validate_data",
                "nodes": [
                    {"name": "validate_data", "implementation": "validate_data"},
                    {"name": "clean_data", "implementation": "clean_data"},
                    {"name": "check_quality", "implementation": "check_quality"},
                    {"name": "discover_causality", "implementation": "discover_causality"},
                    {"name": "error_handler", "implementation": "error_handler"},
                    {"name": "rollback", "implementation": "rollback"},
                    {"name": "validate_model", "implementation": "validate_model"},
                    {"name": "generate_report", "implementation": "generate_report"}
                ],
                "edges": [
                    {"from": "validate_data", "to": "clean_data"},
                    {"from": "clean_data", "to": "check_quality"},
                    {"from": "check_quality", "to": "discover_causality", "condition": "quality_ok"},
                    {"from": "check_quality", "to": "rollback", "condition": "quality_fail"},
                    {"from": "discover_causality", "to": "validate_model", "condition": "success"},
                    {"from": "discover_causality", "to": "error_handler", "condition": "error"},
                    {"from": "error_handler", "to": "rollback", "condition": "cannot_recover"},
                    {"from": "error_handler", "to": "discover_causality", "condition": "can_retry"},
                    {"from": "validate_model", "to": "generate_report"},
                    {"from": "rollback", "to": "generate_report"}
                ]
            },
            "confidence": 0.90,
            "key_innovations": [
                "Comprehensive error handling",
                "Multiple fallback paths",
                "Quality gates at critical points"
            ],
            "risk_mitigation": {
                "data_corruption": "Rollback mechanism",
                "algorithm_failure": "Multiple retry with degradation",
                "quality_issues": "Early detection and abort"
            }
        }
        return json.dumps(response)
    
    @staticmethod
    def _aggregator_response(prompt: str) -> str:
        """Odpowied≈∫ agregatora - synteza propozycji"""
        # Sprawd≈∫ iteracjƒô je≈õli jest w prompcie
        iteration = 1
        if "ITERATION:" in prompt:
            try:
                iteration = int(prompt.split("ITERATION:")[1].split("/")[0].strip())
            except:
                pass
        
        response = {
            "thought_process": [
                "Analizujƒô si≈Çy ka≈ºdej propozycji",
                "Identyfikujƒô synergie miƒôdzy podej≈õciami",
                "≈ÅƒÖczƒô najlepsze elementy w sp√≥jnƒÖ ca≈Ço≈õƒá"
            ],
            "final_plan": {
                "entry_point": "validate_data",
                "nodes": [
                    {"name": "validate_data", "implementation": "validate_data"},
                    {"name": "clean_data", "implementation": "clean_data"},
                    {"name": "check_quality", "implementation": "check_quality"},
                    {"name": "optimize_performance", "implementation": "optimize_performance"},
                    {"name": "discover_causality", "implementation": "discover_causality"},
                    {"name": "error_handler", "implementation": "error_handler"},
                    {"name": "rollback", "implementation": "rollback"},
                    {"name": "train_model", "implementation": "train_model"},
                    {"name": "validate_model", "implementation": "validate_model"},
                    {"name": "generate_report", "implementation": "generate_report"},
                    {"name": "notify_user", "implementation": "notify_user"}
                ],
                "edges": [
                    {"from": "validate_data", "to": "clean_data"},
                    {"from": "clean_data", "to": "check_quality"},
                    {"from": "check_quality", "to": "optimize_performance", "condition": "quality_ok"},
                    {"from": "check_quality", "to": "rollback", "condition": "quality_fail"},
                    {"from": "optimize_performance", "to": "discover_causality"},
                    {"from": "discover_causality", "to": "train_model", "condition": "success"},
                    {"from": "discover_causality", "to": "error_handler", "condition": "error"},
                    {"from": "error_handler", "to": "rollback", "condition": "max_retries"},
                    {"from": "error_handler", "to": "discover_causality", "condition": "can_retry"},
                    {"from": "train_model", "to": "validate_model"},
                    {"from": "validate_model", "to": "generate_report"},
                    {"from": "generate_report", "to": "notify_user"}
                ]
            },
            "synthesis_reasoning": "Po≈ÇƒÖczy≈Çem solidnƒÖ obs≈Çugƒô b≈Çƒôd√≥w od Risk Analyst, innowacyjnƒÖ optymalizacjƒô od Creative Planner, i rygorystycznƒÖ walidacjƒô od Causal Analyst",
            "component_sources": {
                "Causal Analyst": ["validate_data", "check_quality", "validate_model"],
                "Creative Planner": ["optimize_performance", "notify_user"],
                "Risk Analyst": ["error_handler", "rollback", "conditional_edges"]
            },
            "confidence_score": 0.80 + iteration * 0.05,  # Ro≈õnie z iteracjami
            "improvements": [
                "Dodanie cache dla powtarzalnych operacji",
                "Implementacja progressive enhancement",
                "Monitoring w czasie rzeczywistym"
            ]
        }
        return json.dumps(response)
    
    @staticmethod
    def _critic_response(prompt: str) -> str:
        """Odpowied≈∫ krytyka - ocena planu"""
        # Sprawd≈∫ iteracjƒô
        iteration = 1
        if "ITERATION:" in prompt:
            try:
                iteration = int(prompt.split("ITERATION:")[1].split("/")[0].strip())
            except:
                pass
        
        # Dostosuj ocenƒô do iteracji
        base_score = 60 + iteration * 8
        approved = base_score >= 75 or iteration >= 4
        
        response = {
            "approved": approved,
            "score": min(base_score + random.randint(-5, 10), 95),
            "strengths": [
                "Comprehensive error handling",
                "Good balance between robustness and efficiency",
                "Clear separation of concerns",
                "Innovative optimization approach"
            ][:2 + iteration],  # Wiƒôcej mocnych stron w p√≥≈∫niejszych iteracjach
            "weaknesses": [
                "Missing parallelization opportunities",
                "No caching mechanism",
                "Limited monitoring capabilities",
                "Could benefit from more granular error types"
            ][iteration-1:],  # Mniej s≈Çabo≈õci w p√≥≈∫niejszych iteracjach
            "feedback": f"Plan shows {'significant' if iteration > 2 else 'good'} improvement. {'Ready for deployment.' if approved else 'Further refinement needed.'}",
            "improvements": [
                "Add parallel processing for independent steps",
                "Implement result caching",
                "Add detailed logging and monitoring",
                "Consider adding A/B testing capability"
            ][iteration-1:] if not approved else []
        }
        
        # KLUCZOWA ZMIANA: Dodaj frazƒô "PLAN_ZATWIERDZONY" je≈õli zatwierdzamy
        response_json = json.dumps(response)
        
        if approved:
            # Dodaj magicznƒÖ frazƒô PO JSONie
            response_json += "\n\nPLAN_ZATWIERDZONY"
        
        return response_json
    
    @staticmethod
    def _default_response() -> str:
        """Domy≈õlna odpowied≈∫"""
        response = {
            "thought_process": ["Analyzing task", "Creating plan"],
            "plan": {
                "entry_point": "load_data",
                "nodes": [
                    {"name": "load_data", "implementation": "load_data"},
                    {"name": "process", "implementation": "clean_data"},
                    {"name": "output", "implementation": "generate_report"}
                ],
                "edges": [
                    {"from": "load_data", "to": "process"},
                    {"from": "process", "to": "output"}
                ]
            },
            "confidence": 0.7
        }
        return json.dumps(response)

# ZastƒÖp oryginalnƒÖ klasƒô LLMWrapper
import llm_wrapper
original_call = llm_wrapper.LLMWrapper.__call__

def enhanced_call(self, prompt: str) -> str:
    """Rozszerzone wywo≈Çanie z lepszymi dummy responses"""
    if self.provider == "dummy":
        return ExtendedLLMWrapper.generate_dummy_response(self.model_name, prompt)
    else:
        return original_call(self, prompt)

# Monkey-patch oryginalnej klasy
llm_wrapper.LLMWrapper.__call__ = enhanced_call


--- FILE: llm_wrapper.py ---

"""
Wrapper modeli LLM. Umo≈ºliwia ≈ÇatwƒÖ zamianƒô ≈∫r√≥d≈Ça modelu (np. OpenAI, lokalny model itp.).
W tym przyk≈Çadzie implementujemy klasƒô `LLMWrapper`, kt√≥ra w trybie demonstracyjnym
generuje sztucznƒÖ odpowied≈∫. Aby u≈ºyƒá prawdziwego modelu (np. GPT‚Äë5), nale≈ºy
uzupe≈Çniƒá implementacjƒô wywo≈Çania API w metodzie `__call__`.
"""

import os
import json


class LLMWrapper:
    def __init__(self, provider: str, model_name: str, api_key_env: str = None, temperature: float = 0.5):
        """
        :param provider: dostawca modelu, np. "openai" lub "dummy" dla demonstracji
        :param model_name: nazwa modelu u dostawcy
        :param api_key_env: nazwa zmiennej ≈õrodowiskowej z kluczem API
        :param temperature: parametr kreatywno≈õci dla modeli typu GPT
        """
        self.provider = provider
        self.model_name = model_name
        self.temperature = temperature
        self.api_key = os.environ.get(api_key_env) if api_key_env else None

    def __call__(self, prompt: str) -> str:
        """
        Zwraca odpowied≈∫ modelu na dany prompt. W wersji demonstracyjnej,
        je≈õli provider to "dummy", generuje prosty plan w formacie JSON.
        W przeciwnym razie wymaga zaimplementowania wywo≈Çania API.
        """
        if self.provider == "dummy":
            # Zwr√≥ƒá przyk≈Çadowy JSON jako ciƒÖg znak√≥w
            response = {
                "thought_process": ["Analiza zadania", "Propozycja rozwiƒÖzania"],
                "plan": {
                    "entry_point": "start",
                    "nodes": [
                        {"name": "start", "implementation": "init_task"},
                        {"name": "finish", "implementation": "end_task"}
                    ],
                    "edges": [
                        {"from": "start", "to": "finish"}
                    ]
                },
                "confidence": 0.85
            }
            return json.dumps(response)
        elif self.provider == "openai":
            # Przyk≈Çad wywo≈Çania OpenAI ChatCompletion ‚Äì wymaga biblioteki openai i klucza API
            try:
                import openai  # zaimportuj wewnƒÖtrz, aby uniknƒÖƒá zale≈ºno≈õci dla dummy
            except ImportError:
                raise RuntimeError("Biblioteka openai nie jest zainstalowana. Zainstaluj jƒÖ lub u≈ºyj provider='dummy'.")
            if not self.api_key:
                raise RuntimeError("Brak klucza API. Ustaw zmiennƒÖ ≈õrodowiskowƒÖ lub przeka≈º api_key_env.")
            openai.api_key = self.api_key
            # Buduj listƒô wiadomo≈õci zgodnie z API ChatCompletion
            messages = [
                {"role": "system", "content": "You are an advanced planning agent."},
                {"role": "user", "content": prompt}
            ]
            response = openai.ChatCompletion.create(
                model=self.model_name,
                messages=messages,
                temperature=self.temperature
            )
            return response.choices[0].message["content"]
        else:
            raise NotImplementedError(f"Provider '{self.provider}' nie jest obs≈Çugiwany.")


--- FILE: memory_system.py ---

"""
System pamiƒôci kontekstowej z uczeniem siƒô z poprzednich iteracji
"""
from typing import Dict, List, Any, Optional
from datetime import datetime
import json
import numpy as np
from collections import deque
import os

# Zewnƒôtrzne biblioteki do obliczania podobie≈Ñstwa tekstu
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Lokalny logger procesu
from process_logger import log as process_log

class ContextMemory:
    def __init__(self, max_episodes: int = 100):
        # Existing
        self.episodes = deque(maxlen=max_episodes)
        self.learned_patterns = {}
        self.successful_strategies = []
        
        # NOWE - Pe≈Çne dane misji
        self.full_mission_records = []  # Bez limitu - wszystko zapisujemy
        self.mission_index = {}  # Szybkie wyszukiwanie po ID
        
        self._load_persistent_memory()
    
    
    
    
    def _learn_from_success(self, mission_record: Dict):
        """Ekstraktuje i zapisuje PRAWDZIWE wzorce z udanej misji"""

        # 1. Zapisz wzorzec sukcesu dla tego typu misji
        pattern_key = f"success_pattern_{mission_record['mission_type']}"

        if pattern_key not in self.learned_patterns:
            self.learned_patterns[pattern_key] = {
                "occurrences": 0,
                "examples": [],
                "common_elements": {},
                "avg_score": 0,
                "best_practices": []
            }

        # 2. Aktualizuj statystyki
        pattern = self.learned_patterns[pattern_key]
        pattern["occurrences"] += 1
        current_score = mission_record.get("final_score", 0)
        pattern["avg_score"] = (
            (pattern["avg_score"] * (pattern["occurrences"] - 1) + current_score) 
            / pattern["occurrences"]
        )

        # 3. Znajd≈∫ kluczowe elementy sukcesu
        success_elements = []

        # Sprawd≈∫ co by≈Ço w tym planie
        plan = mission_record.get("final_plan", {})
        nodes = plan.get("nodes", [])

        # Zapisz kt√≥re wƒôz≈Çy by≈Çy u≈ºyte
        node_types = [n.get("implementation") for n in nodes]

        if "error_handler" in node_types:
            success_elements.append("comprehensive_error_handling")
        if "rollback" in node_types:
            success_elements.append("rollback_mechanism")
        if "validate_data" in node_types:
            success_elements.append("data_validation")
        if "optimize_performance" in node_types:
            success_elements.append("performance_optimization")

        # 4. Znajd≈∫ unikalne innowacje z tej misji
        if "Adaptive_Router" in str(nodes):
            success_elements.append("adaptive_routing")

        # 5. Zapisz jako best practice je≈õli score > 90
        if current_score > 90:
            best_practice = {
                "mission_id": mission_record["memory_id"],
                "score": current_score,
                "key_success_factors": success_elements,
                "node_count": len(nodes),
                "complexity": mission_record["performance_metrics"].get("convergence_rate", 0)
            }
            pattern["best_practices"].append(best_practice)

        # 6. Zaktualizuj common_elements (co wystƒôpuje najczƒô≈õciej)
        for element in success_elements:
            if element not in pattern["common_elements"]:
                pattern["common_elements"][element] = 0
            pattern["common_elements"][element] += 1

        # 7. Dodaj przyk≈Çad
        pattern["examples"].append({
            "mission_prompt": mission_record["mission_prompt"],
            "success_factors": success_elements,
            "score": current_score
        })

        process_log(f"[MEMORY] Learned from success: {pattern_key}, "
                    f"occurrences={pattern['occurrences']}, "
                    f"avg_score={pattern['avg_score']:.2f}")
    
    
    
    def export_temporal_report(self, filepath: str = "memory/temporal_patterns.json"):
        """Eksportuje raport wzorc√≥w czasowych"""
        patterns = self.analyze_temporal_patterns()

        report = {
            "generated_at": datetime.now().isoformat(),
            "total_missions": len(self.full_mission_records),
            "patterns": patterns,
            "insights": []
        }

        # Znajd≈∫ najlepszy/najgorszy czas
        best_day = max(patterns['by_weekday'].items(), 
                       key=lambda x: x[1].get('avg_score', 0))
        worst_day = min(patterns['by_weekday'].items(), 
                        key=lambda x: x[1].get('avg_score', 100))

        report['insights'].append(f"Best day: {best_day[0]} (avg: {best_day[1]['avg_score']:.1f})")
        report['insights'].append(f"Worst day: {worst_day[0]} (avg: {worst_day[1]['avg_score']:.1f})")

        with open(filepath, 'w') as f:
            json.dump(report, f, indent=2)

        return report
    
    
    
    def save_complete_mission(self, 
                            mission: str,
                            final_plan: Dict,
                            all_messages: List[Dict],
                            orchestrator_state: Dict) -> str:
        """
        Zapisuje KOMPLETNY rekord misji z wszystkimi danymi
        """
        from datetime import datetime
        import hashlib
        
        # Generuj unikalne ID
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        mission_hash = hashlib.md5(mission.encode()).hexdigest()[:8]
        mission_id = f"mission_{timestamp}_{mission_hash}"
        
        # Ekstraktuj kluczowe informacje z transcript
        iterations_data = self._extract_iterations_from_transcript(all_messages)
        
        # Klasyfikuj misjƒô i tagi
        mission_type = self._classify_mission(mission)
        tags = self._extract_tags(mission, final_plan)
        
        # Znajd≈∫ krytyczne momenty w debacie
        critical_moments = self._identify_critical_moments(all_messages)
        
        # Przygotuj pe≈Çny rekord
        mission_record = {
            # === METADATA ===
            "memory_id": mission_id,
            "timestamp": datetime.now().isoformat(),
            "mission_prompt": mission,
            "mission_type": mission_type,
            "tags": tags,
            
            # === OUTCOME ===
            "outcome": "Success" if final_plan else "Failed",
            "total_iterations": orchestrator_state.get("iteration_count", 0),
            "total_messages": len(all_messages),
            "time_taken_seconds": orchestrator_state.get("execution_time", 0),
            
            # === FINAL ARTIFACTS ===
            "final_plan": final_plan,
            "final_score": self._extract_final_score(all_messages),
            
            # === ITERATION DETAILS ===
            "iterations": iterations_data,
            
            # === KEY INSIGHTS ===
            "critique_evolution": self._track_critique_evolution(iterations_data),
            "aggregator_reasoning": self._extract_aggregator_reasoning(all_messages),
            "proposer_contributions": self._analyze_proposer_contributions(all_messages),
            
            # === LEARNING DATA ===
            "llm_generated_summary": self._generate_mission_summary(all_messages, final_plan),
            "identified_patterns": self._extract_patterns_from_debate(all_messages),
            "success_factors": self._identify_success_factors(final_plan, iterations_data),
            "failure_points": self._identify_failure_points(iterations_data),
            
            # === CRITICAL MOMENTS ===
            "critical_moments": critical_moments,
            "turning_points": self._identify_turning_points(iterations_data),
            
            # === FULL TRANSCRIPT ===
            "full_transcript": all_messages,  # Kompletny zapis
            
            # === METRICS ===
            "performance_metrics": {
                "token_usage": orchestrator_state.get("total_tokens", 0),
                "api_calls": orchestrator_state.get("api_calls", 0),
                "convergence_rate": self._calculate_convergence_rate(iterations_data)
            }
        }
        
        # Zapisz do pamiƒôci
        self.full_mission_records.append(mission_record)
        self.mission_index[mission_id] = len(self.full_mission_records) - 1
        
        if final_plan:  # Je≈õli misja siƒô uda≈Ça
            self._learn_from_success(mission_record)
        
        
        
        if len(self.full_mission_records) % 5 == 0:
            patterns = self.analyze_temporal_patterns()
            process_log(f"[MEMORY] Temporal patterns update: {len(patterns['by_weekday'])} weekdays analyzed")
        
        # Persist immediately
        self._persist_full_memory()
        
        process_log(f"[MEMORY] Saved complete mission: {mission_id}")
        return mission_id
    
    def _extract_iterations_from_transcript(self, messages: List[Dict]) -> List[Dict]:
        """Ekstraktuje dane ka≈ºdej iteracji z transkryptu"""
        iterations = []
        current_iteration = {"proposers": [], "aggregator": None, "critic": None}
        
        for msg in messages:
            role = msg.get("name", "").lower()
            
            if "proposer" in role or "analyst" in role or "planner" in role:
                current_iteration["proposers"].append({
                    "agent": msg.get("name"),
                    "content": msg.get("content"),
                    "key_ideas": self._extract_key_ideas(msg.get("content", ""))
                })
            
            elif "aggregator" in role:
                current_iteration["aggregator"] = {
                    "content": msg.get("content"),
                    "synthesis": self._extract_synthesis(msg.get("content", ""))
                }
            
            elif "critic" in role:
                current_iteration["critic"] = {
                    "content": msg.get("content"),
                    "verdict": self._extract_verdict(msg.get("content", "")),
                    "score": self._extract_score(msg.get("content", "")),
                    "weaknesses": self._extract_weaknesses(msg.get("content", ""))
                }
                
                # Koniec iteracji - zapisz i zacznij nowƒÖ
                if current_iteration["proposers"]:
                    iterations.append(current_iteration)
                    current_iteration = {"proposers": [], "aggregator": None, "critic": None}
        
        return iterations
    
#     def _generate_mission_summary(self, messages: List[Dict], final_plan: Dict) -> str:
#         """Generuje podsumowanie misji (mo≈ºesz tu u≈ºyƒá LLM)"""
#         # Prosta heurystyka - w przysz≈Ço≈õci mo≈ºesz wywo≈Çaƒá LLM
#         summary_parts = []
        
#         # Analiza iteracji
#         iteration_count = sum(1 for m in messages if "critic" in m.get("name", "").lower())
#         summary_parts.append(f"Misja wymaga≈Ça {iteration_count} iteracji.")
        
#         # Kluczowe poprawki
#         weaknesses_mentioned = set()
#         for msg in messages:
#             if "weakness" in msg.get("content", "").lower():
#                 # Ekstraktuj weakness (uproszczenie)
#                 weaknesses_mentioned.add("obs≈Çuga b≈Çƒôd√≥w")
        
#         if weaknesses_mentioned:
#             summary_parts.append(f"G≈Ç√≥wne wyzwania: {', '.join(weaknesses_mentioned)}.")
        
#         # Finalny sukces
#         if final_plan:
#             node_count = len(final_plan.get("nodes", []))
#             summary_parts.append(f"Finalny plan zawiera {node_count} wƒôz≈Ç√≥w.")
        
#         return " ".join(summary_parts)
    
    
    def _generate_mission_summary(self, messages: List[Dict], final_plan: Dict) -> str:
        """Generuje BOGATE podsumowanie misji"""
        summary_parts = []

        # 1. Liczba iteracji i czas
        iteration_count = sum(1 for m in messages if "critic" in m.get("name", "").lower())
        summary_parts.append(f"Misja zako≈Ñczona w {iteration_count} iteracji.")

        # 2. Kluczowe innowacje (szukaj w transkrypcie)
        innovations = set()
        for msg in messages:
            content = msg.get("content", "").lower()
            if "adaptive" in content or "adaptacyjny" in content:
                innovations.add("adaptive routing")
            if "rollback" in content:
                innovations.add("rollback mechanism")
            if "optimiz" in content or "optymali" in content:
                innovations.add("optimization")

        if innovations:
            summary_parts.append(f"Zastosowano: {', '.join(innovations)}.")

        # 3. Analiza struktury planu
        if final_plan:
            nodes = final_plan.get("nodes", [])
            edges = final_plan.get("edges", [])

            # Policz typy ≈õcie≈ºek
            success_paths = len([e for e in edges if e.get("condition") == "on_success"])
            failure_paths = len([e for e in edges if e.get("condition") == "on_failure"])

            summary_parts.append(
                f"Struktura: {len(nodes)} wƒôz≈Ç√≥w, "
                f"{success_paths} ≈õcie≈ºek sukcesu, "
                f"{failure_paths} ≈õcie≈ºek obs≈Çugi b≈Çƒôd√≥w."
            )

            # Znajd≈∫ kluczowe wƒôz≈Çy
            key_nodes = []
            for node in nodes:
                impl = node.get("implementation", "")
                if impl in ["error_handler", "rollback", "validate_data", "optimize_performance"]:
                    key_nodes.append(impl)

            if key_nodes:
                summary_parts.append(f"Kluczowe komponenty: {', '.join(set(key_nodes))}.")

        # 4. Ko≈Ñcowy verdykt
        for msg in reversed(messages):
            if "critic" in msg.get("name", "").lower() and "ZATWIERDZONY" in msg.get("content", ""):
                summary_parts.append("Plan zatwierdzony przez krytyka bez zastrze≈ºe≈Ñ.")
                break

        return " ".join(summary_parts)
    
    
    
    
    def _extract_tags(self, mission: str, final_plan: Dict) -> List[str]:
        """Automatycznie taguje misjƒô"""
        tags = []
        mission_lower = mission.lower()
        
        # Mission-based tags
        tag_keywords = {
            "error_handling": ["error", "b≈Çƒôd", "obs≈Çuga", "handler"],
            "optimization": ["optym", "performance", "wydajno≈õƒá"],
            "causality": ["causal", "przyczyn"],
            "validation": ["valid", "walidac"],
            "retry": ["retry", "ponow"],
            "rollback": ["rollback", "cofn"],
            "ml": ["model", "train", "uczenie"],
            "data": ["data", "dane", "csv", "pipeline"]
        }
        
        for tag, keywords in tag_keywords.items():
            if any(kw in mission_lower for kw in keywords):
                tags.append(tag)
        
        # Plan-based tags
        if final_plan:
            nodes_str = str(final_plan.get("nodes", []))
            if "error_handler" in nodes_str:
                tags.append("robust")
            if "optimize" in nodes_str:
                tags.append("optimized")
        
        return list(set(tags))  # Unique tags
    
    
    def _load_persistent_memory(self):
        """
        ≈Åaduje pamiƒôƒá z pliku JSON
        """
        json_file = "memory/learned_strategies.json"

        if os.path.exists(json_file):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                self.learned_patterns = data.get("patterns", {})
                self.successful_strategies = data.get("strategies", [])

                # Za≈Çaduj te≈º nowe full_mission_records je≈õli istniejƒÖ
                if "full_mission_records" in data:
                    self.full_mission_records = data["full_mission_records"]
                    # Odbuduj index
                    for i, record in enumerate(self.full_mission_records):
                        self.mission_index[record["memory_id"]] = i

                print(f"‚úî Za≈Çadowano pamiƒôƒá: {len(self.successful_strategies)} strategies, {len(self.full_mission_records)} full records")
            except Exception as e:
                print(f"‚ö† Nie uda≈Ço siƒô za≈Çadowaƒá pamiƒôci: {e}")
        else:
            print("üìù Tworzƒô nowƒÖ pamiƒôƒá (brak istniejƒÖcego pliku)")
            os.makedirs("memory", exist_ok=True)

    def _persist_memory(self):
        """
        Zapisuje pamiƒôƒá do pliku JSON
        """
        os.makedirs("memory", exist_ok=True)
        memory_file = "memory/learned_strategies.json"

        data = {
            "patterns": self.learned_patterns,
            "strategies": self.successful_strategies,
            "full_mission_records": self.full_mission_records  # NOWE!
        }

        try:
            with open(memory_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö† Nie uda≈Ço siƒô zapisaƒá pamiƒôci: {e}")

            
            
            
            
    def _persist_full_memory(self):
        """Alias dla _persist_memory"""
        self._persist_memory()


        
    def _extract_key_ideas(self, content: str) -> List[str]:
        """Ekstraktuje kluczowe pomys≈Çy z contentu"""
        # Prosta heurystyka - mo≈ºesz ulepszyƒá
        ideas = []
        if "error_handler" in content.lower():
            ideas.append("error_handling")
        if "rollback" in content.lower():
            ideas.append("rollback_mechanism")
        if "optimiz" in content.lower():
            ideas.append("optimization")
        return ideas

    def _extract_synthesis(self, content: str) -> str:
        """Ekstraktuje syntezƒô z odpowiedzi aggregatora"""
        # Szukaj "synthesis_reasoning" w JSON
        try:
            data = json.loads(content) if isinstance(content, str) else content
            return data.get("synthesis_reasoning", "")
        except:
            return ""

    def _extract_verdict(self, content: str) -> str:
        """Ekstraktuje werdykt z odpowiedzi krytyka"""
        if "ZATWIERDZONY" in content:
            return "ZATWIERDZONY"
        return "ODRZUCONY"

    def _extract_score(self, content: str) -> float:
        """Ekstraktuje score z odpowiedzi krytyka"""
        try:
            import re
            score_match = re.search(r'"Overall_Quality_Q":\s*([\d.]+)', content)
            if score_match:
                return float(score_match.group(1))
        except:
            pass
        return 0.0

    def _extract_weaknesses(self, content: str) -> List[str]:
        """Ekstraktuje weaknesses z odpowiedzi krytyka"""
        weaknesses = []
        try:
            data = json.loads(content) if isinstance(content, str) else content
            weak_list = data.get("critique_summary", {}).get("identified_weaknesses", [])
            for w in weak_list:
                if isinstance(w, dict):
                    weaknesses.append(w.get("weakness", ""))
                else:
                    weaknesses.append(str(w))
        except:
            pass
        return weaknesses


    def add_successful_plan(self, plan: Dict[str, Any], mission: str, metadata: Dict):
        """Zapisuje udany plan do pamiƒôci proceduralnej"""
        strategy = {
            "mission_type": self._classify_mission(mission),
            "plan_structure": self._extract_plan_structure(plan),
            "success_factors": metadata.get("success_factors", []),
            "performance_metrics": metadata.get("metrics", {}),
            "timestamp": datetime.now().isoformat()
        }

        self.successful_strategies.append(strategy)
        self._persist_memory()  # Zapisz od razu

        # Loguj dodanie udanego planu
        process_log(
            f"[MEMORY] Added successful plan for mission_type={strategy['mission_type']}, "
            f"nodes={strategy['plan_structure']['num_nodes']}"
        )

    def _classify_mission(self, mission: str) -> str:
        """Klasyfikuje typ misji"""
        mission_lower = mission.lower()

        if "przyczynow" in mission_lower or "causal" in mission_lower:
            return "causal_analysis"
        elif "dane" in mission_lower or "data" in mission_lower or "csv" in mission_lower:
            return "data_processing"
        elif "model" in mission_lower:
            return "model_validation"
        elif "optymali" in mission_lower:
            return "optimization"
        else:
            return "general"

    def _extract_plan_structure(self, plan: Dict) -> Dict:
        """Ekstraktuje strukturalne cechy planu"""
        return {
            "num_nodes": len(plan.get("nodes", [])),
            "num_edges": len(plan.get("edges", [])),
            "has_error_handling": any("error" in str(node).lower() 
                                     for node in plan.get("nodes", [])),
            "has_validation": any("valid" in str(node).lower() 
                                 for node in plan.get("nodes", [])),
            "graph_complexity": self._calculate_complexity(plan)
        }

    def _calculate_complexity(self, plan: Dict) -> float:
        """Oblicza z≈Ço≈ºono≈õƒá grafu"""
        nodes = len(plan.get("nodes", []))
        edges = len(plan.get("edges", []))

        if nodes == 0:
            return 0.0

        # Z≈Ço≈ºono≈õƒá cyklomatyczna aproksymowana
        return (edges - nodes + 2) / nodes
    
    
    def _identify_critical_moments(self, messages: List[Dict]) -> List[Dict]:
        """Identyfikuje krytyczne momenty w debacie"""
        critical = []
        for i, msg in enumerate(messages):
            content = msg.get("content", "").lower()
            # Moment krytyczny = du≈ºa zmiana w score lub verdict
            if "zatwierdzony" in content or "odrzucony" in content:
                critical.append({
                    "index": i,
                    "type": "verdict",
                    "agent": msg.get("name"),
                    "summary": "Decyzja krytyka"
                })
        return critical

    def _extract_final_score(self, messages: List[Dict]) -> float:
        """Znajduje finalny score z ostatniej odpowiedzi krytyka"""
        for msg in reversed(messages):
            if "critic" in msg.get("name", "").lower():
                score = self._extract_score(msg.get("content", ""))
                if score > 0:
                    return score
        return 0.0

    def _track_critique_evolution(self, iterations: List[Dict]) -> List[Dict]:
        """≈öledzi jak zmienia≈Ça siƒô krytyka miƒôdzy iteracjami"""
        evolution = []
        for i, iteration in enumerate(iterations):
            if iteration.get("critic"):
                evolution.append({
                    "iteration": i,
                    "score": iteration["critic"].get("score", 0),
                    "verdict": iteration["critic"].get("verdict", ""),
                    "main_issues": iteration["critic"].get("weaknesses", [])[:2]
                })
        return evolution

    def _extract_aggregator_reasoning(self, messages: List[Dict]) -> str:
        """WyciƒÖga reasoning agregatora"""
        for msg in reversed(messages):
            if "aggregator" in msg.get("name", "").lower():
                return self._extract_synthesis(msg.get("content", ""))
        return ""

    def _analyze_proposer_contributions(self, messages: List[Dict]) -> Dict[str, List[str]]:
        """Analizuje wk≈Çad ka≈ºdego proposera"""
        contributions = {}
        for msg in messages:
            name = msg.get("name", "")
            if any(role in name.lower() for role in ["analyst", "planner", "proposer"]):
                if name not in contributions:
                    contributions[name] = []
                ideas = self._extract_key_ideas(msg.get("content", ""))
                contributions[name].extend(ideas)
        return contributions

    def _extract_patterns_from_debate(self, messages: List[Dict]) -> List[str]:
        """Ekstraktuje wzorce z ca≈Çej debaty"""
        patterns = []
        # Szukaj powtarzajƒÖcych siƒô koncept√≥w
        all_text = " ".join(m.get("content", "") for m in messages).lower()

        if all_text.count("error_handler") > 3:
            patterns.append("Czƒôste odniesienia do obs≈Çugi b≈Çƒôd√≥w")
        if all_text.count("rollback") > 2:
            patterns.append("Rollback jako kluczowy element")
        if all_text.count("optimiz") > 2:
            patterns.append("Focus na optymalizacjƒô")

        return patterns

    def _identify_success_factors(self, final_plan: Dict, iterations: List[Dict]) -> List[str]:
        """Identyfikuje co przyczyni≈Ço siƒô do sukcesu"""
        factors = []

        if final_plan:
            # Analiza struktury planu
            if any("error" in str(n).lower() for n in final_plan.get("nodes", [])):
                factors.append("Comprehensive error handling")
            if any("valid" in str(n).lower() for n in final_plan.get("nodes", [])):
                factors.append("Data validation steps")

            # Analiza iteracji
            if len(iterations) > 1:
                factors.append(f"Iterative improvement ({len(iterations)} rounds)")

        return factors

    def _identify_failure_points(self, iterations: List[Dict]) -> List[Dict]:
        """Identyfikuje gdzie by≈Çy problemy"""
        failures = []
        for i, iteration in enumerate(iterations):
            if iteration.get("critic", {}).get("verdict") == "ODRZUCONY":
                failures.append({
                    "iteration": i,
                    "issues": iteration["critic"].get("weaknesses", []),
                    "score": iteration["critic"].get("score", 0)
                })
        return failures

    def _identify_turning_points(self, iterations: List[Dict]) -> List[Dict]:
        """Znajduje punkty zwrotne w debacie"""
        turning_points = []
        prev_score = 0

        for i, iteration in enumerate(iterations):
            curr_score = iteration.get("critic", {}).get("score", 0)
            if curr_score - prev_score > 20:  # Du≈ºy skok w score
                turning_points.append({
                    "iteration": i,
                    "score_jump": curr_score - prev_score,
                    "reason": "Significant improvement"
                })
            prev_score = curr_score

        return turning_points

    def _calculate_convergence_rate(self, iterations: List[Dict]) -> float:
        """Oblicza jak szybko system doszed≈Ç do rozwiƒÖzania"""
        if not iterations:
            return 0.0

        scores = [it.get("critic", {}).get("score", 0) for it in iterations]
        if len(scores) < 2:
            return 1.0

        # ≈öredni przyrost score na iteracjƒô
        improvements = [scores[i+1] - scores[i] for i in range(len(scores)-1)]
        avg_improvement = sum(improvements) / len(improvements) if improvements else 0

        # Normalizuj do 0-1 (im wy≈ºszy przyrost, tym lepsza convergence)
        return min(avg_improvement / 20, 1.0)  # 20 punkt√≥w na iteracjƒô = max convergence
    
    
    def analyze_temporal_patterns(self) -> Dict[str, Any]:
        """Analizuje wzorce czasowe w performance systemu"""
        from datetime import datetime

        patterns = {
            'by_weekday': {},
            'by_hour': {},
            'by_day_hour': {}
        }

        if not self.full_mission_records:
            return patterns

        # Analiza per dzie≈Ñ tygodnia
        for record in self.full_mission_records:
            timestamp = datetime.fromisoformat(record['timestamp'])
            weekday = timestamp.strftime('%A')
            hour = timestamp.hour
            day_hour = f"{weekday}_{hour:02d}h"

            # Per weekday
            if weekday not in patterns['by_weekday']:
                patterns['by_weekday'][weekday] = {
                    'missions': [],
                    'avg_score': 0,
                    'avg_iterations': 0,
                    'common_issues': []
                }

            patterns['by_weekday'][weekday]['missions'].append(record['memory_id'])

            # Per hour
            if hour not in patterns['by_hour']:
                patterns['by_hour'][hour] = {
                    'missions': [],
                    'avg_score': 0,
                    'avg_iterations': 0
                }

            patterns['by_hour'][hour]['missions'].append(record['memory_id'])

            # Per day+hour combo
            if day_hour not in patterns['by_day_hour']:
                patterns['by_day_hour'][day_hour] = {
                    'missions': [],
                    'scores': []
                }

            patterns['by_day_hour'][day_hour]['missions'].append(record['memory_id'])
            patterns['by_day_hour'][day_hour]['scores'].append(record.get('final_score', 0))

        # Oblicz ≈õrednie
        for weekday, data in patterns['by_weekday'].items():
            if data['missions']:
                scores = [r['final_score'] for r in self.full_mission_records 
                         if r['memory_id'] in data['missions']]
                data['avg_score'] = sum(scores) / len(scores) if scores else 0

        return patterns

    def get_current_context_hints(self) -> str:
        """Zwraca wskaz√≥wki kontekstowe na podstawie aktualnego czasu"""
        from datetime import datetime

        now = datetime.now()
        patterns = self.analyze_temporal_patterns()

        hints = []

        # Sprawd≈∫ wzorce dla aktualnego dnia
        weekday = now.strftime('%A')
        if weekday in patterns['by_weekday']:
            weekday_data = patterns['by_weekday'][weekday]
            if weekday_data['avg_score'] < 90:
                hints.append(f"Uwaga: {weekday} historycznie majƒÖ ni≈ºsze score ({weekday_data['avg_score']:.1f})")

        # Sprawd≈∫ wzorce dla aktualnej godziny
        hour = now.hour
        if hour in patterns['by_hour']:
            hour_data = patterns['by_hour'][hour]
            if len(hour_data['missions']) > 2:  # Je≈õli mamy wystarczajƒÖco danych
                hints.append(f"O godzinie {hour}:00 zazwyczaj wykonywane sƒÖ misje tego typu")

        return " | ".join(hints) if hints else ""


--- FILE: moa_prompts.py ---

"""
Zaawansowane prompty dla systemu MOA z technikami Chain-of-Thought i Self-Consistency
"""
from typing import Dict, Any, List
from config.models_config import AgentRole

class MOAPrompts:
    """Centralna biblioteka prompt√≥w dla systemu MOA"""
    
    # Uniwersalne zasady dla wszystkich agent√≥w
    UNIVERSAL_PRINCIPLES = """
## UNIVERSAL REASONING & OUTPUT POLICY

1) Deterministic, Structured Reasoning
- Decompose the mission into atomic steps; make dependencies explicit.
- Prefer DAG-like flows with clear success/failure transitions.

2) Output Contract (STRICT)
- Final output MUST be a single valid JSON object (no prose, no code fences, no comments).
- Keys and schema names are in English; user-facing strings are in Polish.
- If you risk exceeding token limits, compress explanations but keep structure intact.

3) Memory & Retrieval Discipline
- When WRITING memory: always store concise English bullet points or JSON objects
  (normalized nouns, present tense, ‚â§200 tokens per write).
- When READING memory: query only what is needed for the current decision.
- Never copy large memory chunks into the output; summarize instead.

4) Robustness by Design
- For each critical step, state the expected preconditions and postconditions.
- Include failure transitions (on_failure) and remediation (retry, rollback, notify).

5) Metrics & Confidence
- Quantify uncertainty (0‚Äì1). Justify with observable signals (e.g., data_quality).
- Prefer measurable thresholds over vague conditions.

6) Tooling Constraints
- Use ONLY nodes present in the node library (exact implementation names).
- Allowed edge.condition values: on_success, on_failure, retry, validated, partial_success,
  needs_optimization, else (as a last-resort catch-all).
"""
    
    @staticmethod
    def get_proposer_prompt(role: AgentRole, mission: str, node_library: Dict) -> str:
        """English prompt for Proposers; user-facing strings must be Polish."""
        style_mod = {
            "analytical": "Be precise and data-driven; justify every decision with observable signals.",
            "creative": "Explore non-obvious combinations and alternative paths; propose at least one novel twist.",
            "critical": "Stress-test assumptions and highlight edge cases and single points of failure.",
            "systematic": "Aim for holistic, end-to-end coherence with explicit interfaces between steps."
        }

        expertise = f"""
    # ROLE: {role.role_name}

    ## YOUR EXPERTISE
    You specialize in: {', '.join(role.expertise_areas)}

    ## THINKING STYLE
    {style_mod.get(role.thinking_style, "Default to clarity and rigor.")}

    {MOAPrompts.UNIVERSAL_PRINCIPLES}

    ## ROLE-SPECIFIC TECHNIQUES
    """
        rl = role.role_name.lower()
        if "causal" in rl:
            expertise += """
    - Causal Reasoning:
      * Identify variables and likely causal relations (confounders, mediators).
      * Prefer testable interventions; annotate assumptions explicitly.
    """
        elif "strategic" in rl:
            expertise += """
    - Strategic Planning:
      * SWOT per component; map critical dependencies and critical path.
      * Prepare 1‚Äì2 realistic what-if branches with measurable triggers.
    """
        elif "creative" in rl:
            expertise += """
    - Creative Expansion:
      * Apply SCAMPER to at least two nodes.
      * Propose 3 alternative micro-approaches and pick one with rationale.
    """
        elif "risk" in rl or "quality" in rl:
            expertise += """
    - Risk/Quality:
      * FMEA table in your head; identify top 3 failure modes and mitigations.
      * Add explicit rollback/notify paths for irrecoverable states.
    """

        return f"""
    {expertise}

    ## MISSION
    {mission}

    ## AVAILABLE NODE LIBRARY
    {MOAPrompts._format_node_library(node_library)}

    ## OUTPUT CONTRACT (ONLY JSON, NO PROSE)
    - Keys in English; user-facing strings in Polish.
    - Use ONLY implementations from the node library.
    - Ensure failure paths exist for critical steps.
    - Keep "thought_process" and justifications concise in Polish.

    Expected JSON structure:
    {{
      "thought_process": ["Krok 1: ...", "Krok 2: ...", "Krok 3: ..."],
      "plan": {{
        "entry_point": "Start_Node_Name",
        "nodes": [
          {{"name": "Load_Data", "implementation": "load_data"}},
          {{"name": "Clean_Data", "implementation": "clean_data"}},
          {{"name": "Validate_Data", "implementation": "validate_data"}}
        ],
        "edges": [
          {{"from": "Load_Data", "to": "Clean_Data", "condition": "on_success"}},
          {{"from": "Load_Data", "to": "Error_Handler", "condition": "on_failure"}}
        ]
      }},
      "confidence": 0.80,
      "key_innovations": ["Innowacja 1", "Innowacja 2"],
      "risk_mitigation": {{"Ryzyko A": "Mitigacja A", "Ryzyko B": "Mitigacja B"}}
    }}
    - Do NOT include code fences or comments.
    - When you write ANY memory (outside this output), save it in concise EN.
"""
    
    @staticmethod
    def get_aggregator_prompt() -> str:
        """English prompt for the Master Aggregator; output JSON only; user-facing text Polish."""
        return """
    # ROLE: MASTER AGGREGATOR ‚Äî SYNTHESIS & GOVERNANCE

    You merge multiple proposals into a single, coherent, executable plan with strong
    robustness and measurable gates. You remove duplication, resolve conflicts, and
    preserve the best ideas.

    {UNIVERSAL_POLICY}

    ## SYNTHESIS PROTOCOL
    1) Score each proposal on: logical soundness, feasibility, innovation, robustness.
    2) Extract the best subcomponents and compose them (component interfaces must align).
    3) Resolve conflicts by explicit trade-offs; document rationale concisely (Polish).
    4) Guarantee failure paths (on_failure/rollback/notify) for critical nodes.
    5) Prefer measurable conditions (e.g., data_quality > 0.9) where applicable.

    ## META-LEARNING HOOKS
    - If prior successful patterns are known, prefer them; otherwise, annotate assumptions.

    ## OUTPUT CONTRACT (ONLY JSON, NO PROSE)
    - Keys in English; user-facing strings in Polish.
    - Provide a final executable DAG under `final_plan`.
    - Include a brief Polish synthesis rationale and confidence score in [0,1].

    Expected JSON structure:
    {
      "thought_process": ["≈ÅƒÖczƒô elementy X i Y...", "Ujednolicam warunki..."],
      "final_plan": {
        "entry_point": "Load_Data",
        "nodes": [
          {"name": "Load_Data", "implementation": "load_data"},
          {"name": "Clean_Data", "implementation": "clean_data"},
          {"name": "Validate_Data", "implementation": "validate_data"},
          {"name": "Error_Handler", "implementation": "error_handler"},
          {"name": "Rollback_Changes", "implementation": "rollback"},
          {"name": "Generate_Report", "implementation": "generate_report"}
        ],
        "edges": [
          {"from": "Load_Data", "to": "Clean_Data", "condition": "on_success"},
          {"from": "Load_Data", "to": "Error_Handler", "condition": "on_failure"},
          {"from": "Clean_Data", "to": "Validate_Data", "condition": "on_success"}
        ]
      },
      "synthesis_reasoning": "Kr√≥tko po polsku: dlaczego taki uk≈Çad jest najlepszy.",
      "component_sources": {"Causal Analyst": ["Validate_Data"], "Creative Planner": ["Generate_Report"]},
      "confidence_score": 0.90
    }
    - Do NOT include code fences or comments.
    - Any memory writes you perform must be saved in concise English.
    """.replace("{UNIVERSAL_POLICY}", MOAPrompts.UNIVERSAL_PRINCIPLES)
    
    @staticmethod
    def get_critic_prompt() -> str:
        return """
# ROLE: QUALITY CRITIC ‚Äî ADVERSARIAL VALIDATOR

You are the final gate. Stress-test structure, semantics, robustness and compliance
with the mission. If and only if the plan passes, approve it.

{UNIVERSAL_POLICY}

## VALIDATION CHECKLIST
- Structural: valid JSON; required fields present; node names & implementations align with library.
- Semantic: mission alignment; logical flow; dependencies satisfied; measurable conditions preferred.
- Robustness: explicit error paths; rollback and notify; identify SPOFs and mitigations.
- Metrics: compute concise quality metrics; justify scores briefly in Polish.

## DECISION RULE
- APPROVE only if Overall Quality >= threshold you deem reasonable and no critical gaps remain.
- When you APPROVE, set `critique_summary.verdict` to "ZATWIERDZONY" (Polish, uppercase).
- Also include a short Polish justification.

## OUTPUT CONTRACT (ONLY JSON, NO PROSE)
- Keys in English; user-facing strings in Polish.
- If approved, include a complete `final_synthesized_plan` (same schema as proposer/aggregator).
- Optionally include `decision_marker`: "PLAN_ZATWIERDZONY" to facilitate orchestration.

Expected JSON structure:
{
  "critique_summary": {
    "verdict": "ZATWIERDZONY",
    "statement": "Kr√≥tki pow√≥d po polsku.",
    "key_strengths": ["Mocna strona 1", "Mocna strona 2"],
    "identified_weaknesses": [
      {"weakness": "S≈Çabo≈õƒá X", "severity": "Medium", "description": "Dlaczego to problem"}
    ]
  },
  "quality_metrics": {
    "Complexity_Score_C": 3.1,
    "Robustness_Score_R": 50,
    "Innovation_Score_I": 100,
    "Completeness_Score": 100,
    "Overall_Quality_Q": 84.07
  },
  "final_synthesized_plan": {
    "entry_point": "Load_Data",
    "nodes": [
      {"name": "Load_Data", "implementation": "load_data"},
      {"name": "Clean_Data", "implementation": "clean_data"}
    ],
    "edges": [
      {"from": "Load_Data", "to": "Clean_Data", "condition": "on_success"}
    ]
  },
  "decision_marker": "PLAN_ZATWIERDZONY"
}
- Do NOT include code fences or comments.
-In the final response, end with a line containing only PLAN_ZATWIERDZONY.
- Any memory writes you perform must be saved in concise English.
""".replace("{UNIVERSAL_POLICY}", MOAPrompts.UNIVERSAL_PRINCIPLES)
    
    @staticmethod
    def _format_node_library(node_library: Dict) -> str:
        """Formatuje bibliotekƒô wƒôz≈Ç√≥w dla promptu"""
        formatted = []
        for name, details in node_library.items():
            formatted.append(f"- {name}: {details.get('description', 'Brak opisu')}")
        return "\n".join(formatted)


--- FILE: models_config.py ---

"""
Definicje struktur danych u≈ºywanych do opisu r√≥l agent√≥w.
"""

from dataclasses import dataclass
from typing import List


@dataclass
class AgentRole:
    """
    Klasa opisujƒÖca rolƒô agenta w systemie multi‚Äëagentowym.

    :param role_name: Nazwa roli (np. "Causal Analyst", "Creative Planner").
    :param expertise_areas: Lista dziedzin, w kt√≥rych agent siƒô specjalizuje.
    :param thinking_style: Styl my≈õlenia ("analytical", "creative", "critical", "systematic" itp.).
    """
    role_name: str
    expertise_areas: List[str]
    thinking_style: str


--- FILE: persist_missions_memory.py ---

"""
Utilities for persisting mission records and their corresponding debate artefacts
into a Vertex AI Agent Engine memory bank.

This module contains a single helper function, ``persist_missions_to_vertex_memory``,
which expects a path to a JSON file containing missions (in the same shape
as produced by your multi‚Äëagent workflow) and writes several structured
memories for each mission into a specified Agent Engine.  Unlike earlier
implementations that attempted to write directly through the high‚Äëlevel
``vertexai.agent_engines`` module, this version uses a ``vertexai.Client``
instance to perform the memory writes.  The Vertex SDK requires that
``create_memory`` be invoked via ``client.agent_engines.create_memory(...)``
when interacting with an existing engine by name.

Example usage::

    from vertexai import agent_engines
    from tools.persist_missions_memory import persist_missions_to_vertex_memory

    # create or fetch your Agent Engine up front
    engine = agent_engines.create(display_name="my-engine")

    # initialise a Vertex client (project and location can be omitted if
    # configured via environment variables)
    client = vertexai.Client(project="my-project", location="us-central1")

    # persist the missions into memory
    persist_missions_to_vertex_memory(
        json_path="/path/to/learned_strategies.json",
        engine_name=engine.resource_name,
        client=client
    )

Each mission record will produce several memory entries: an overview,
the final plan (graph), a summary of aggregator contributions, a critic
report and optionally the full debate transcript split into chunks.  The
``scope`` of each memory contains identifiers like ``mission_id`` and
``mission_type`` so that downstream components can query the memory bank
precisely.
"""

from __future__ import annotations

import json
import hashlib
from datetime import datetime
from typing import Any, Dict, Optional, Callable

try:
    import vertexai  # type: ignore
except ImportError as e:  # pragma: no cover
    raise RuntimeError(
        "vertexai package is required for persist_missions_to_vertex_memory" ) from e

__all__ = ["persist_missions_to_vertex_memory"]


def _md5(text: str) -> str:
    """Return an MD5 hex digest for the given text."""
    return hashlib.md5(text.encode("utf-8")).hexdigest()


def _now() -> str:
    """Return current UTC time in ISO8601 format with a 'Z' suffix."""
    return datetime.utcnow().isoformat() + "Z"


def _merge_dicts(base: Dict[str, Any], extra: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Helper to merge two dictionaries.  ``extra`` entries override those in
    ``base``.  None values are skipped.
    """
    result = dict(base)
    if extra:
        for k, v in extra.items():
            if v is not None:
                result[k] = v
    return result


def persist_missions_to_vertex_memory(
    json_path: str,
    *,
    engine_name: str,
    client: "vertexai.Client",
    include_transcript: bool = True,
    max_transcript_chunk_chars: int = 15000,
    make_scope: Optional[Callable[[Dict[str, Any], Dict[str, Any]], Dict[str, Any]]] = None,
) -> None:
    """
    Persist missions and their associated debate artefacts into a Vertex AI Agent
    Engine memory bank.

    Parameters
    ----------
    json_path: str
        Path to the JSON file containing mission records.  Each record should
        resemble the entries under ``full_mission_records`` in your data.

    engine_name: str
        The fully qualified resource name of the Agent Engine (e.g.
        ``projects/123/locations/us-central1/reasoningEngines/456``).  The
        engine must already exist; this function does not create or look up
        the engine.

    client: vertexai.Client
        An initialised Vertex AI client.  Creation of memories must be
        performed via ``client.agent_engines.create_memory(...)``.

    include_transcript: bool, default True
        Whether to write the full debate transcript (``full_transcript``) into
        memory.  When ``True``, the transcript will be stored in one or
        multiple memory records.  To satisfy the Vertex AI limitation that
        ``fact`` strings must be shorter than ~2k characters, transcripts are
        automatically split into chunks of approximately 2040 characters.

    max_transcript_chunk_chars: int, default 15000
        **Deprecated.** Previously controlled the size of transcript chunks,
        but is now ignored.  All memory entries are automatically split to
        satisfy the Vertex AI fact size limit of ~2k characters.

    make_scope: Callable, optional
        Optional callback to build a scope dictionary given the base mission
        scope and a view-specific override.  If not provided, a simple
        merge of dictionaries is used.  This can be used to inject custom
        metadata or enforce specific scoping rules.

    Notes
    -----
    Each mission produces the following memory entries:

    1. ``mission_overview``: contains the prompt, mission type, tags and timestamps.
    2. ``final_plan``: stores the ``final_plan`` object with nodes and edges and
       the mission's final score.
    3. ``aggregator_summary``: summarises the aggregator's reasoning and
       proposer contributions, if present.
    4. ``critic_report``: stores the critic's verdict, score and any
       weaknesses.
    5. ``debate_transcript``: optional, contains the full list of messages
       exchanged during the debate.  For long transcripts a ``content_json_chunk``
       field is used to store raw JSON segments.

    All entries are written using the Vertex SDK's ``create_memory`` method on
    ``client.agent_engines``, which attaches the memory to the specified engine
    and associates a scope.  The scope includes identifiers like ``mission_id``
    and ``mission_type`` so that consumers can later retrieve only the
    relevant memories.
    """
    # Validate inputs early
    if not engine_name:
        raise ValueError("engine_name must be provided and non-empty")
    if client is None:
        raise ValueError("A vertexai.Client instance must be provided via the 'client' parameter")

    # Limit for the fact field.  The Vertex AI Agent Engine requires the fact
    # string to be under 2KiB (roughly 2048 characters).  We leave
    # a comfortable margin so that after adding JSON overhead (field names,
    # mission_id, imported_at etc.) the final string stays within the limit.
    MAX_FACT_LENGTH = 1500

    # Load the JSON file and compute a corpus signature to group related entries
    with open(json_path, "r", encoding="utf-8") as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError as exc:
            raise ValueError(f"Failed to parse JSON at {json_path}: {exc}") from exc

    corpus_sig = _md5(json.dumps(data, ensure_ascii=False, sort_keys=True))

    # Use provided or default scope builder
    build_scope = (
        make_scope
        if make_scope is not None
        else lambda base, extra: _merge_dicts(base, extra)
    )

    def _create_memory_fact(
        *,
        kind: str,
        fact_dict: Dict[str, Any],
        view: str,
        replic_fields: Optional[tuple[str, ...]] = None,
    ) -> None:
        """
        Construct a JSON fact from ``fact_dict`` (ensuring ``mission_id``,
        ``kind`` and ``imported_at`` are present) and persist it to Vertex AI
        memory.  If the resulting JSON string is small, it is stored directly.
        Otherwise the payload is split into chunks small enough that each
        serialized memory entry, including metadata, does not exceed 2048
        characters in UTF‚Äë8 encoding.  Each chunk entry has a
        ``content_json_chunk`` field along with ``chunk_index`` and ``chunk_total``.
        Selected fields listed in ``replic_fields`` are copied onto every
        chunk to make retrieval easier.
        """
        # Compose the complete payload once (includes the entire fact_dict)
        payload = dict(fact_dict)
        payload["kind"] = kind
        payload["mission_id"] = mission_id
        payload["imported_at"] = _now()
        payload_str = json.dumps(payload, ensure_ascii=False)
        # Quick path: small payload
        if len(payload_str.encode("utf-8")) < 2000:
            # Under limit: write directly
            client.agent_engines.create_memory(
                name=engine_name,
                fact=payload_str,
                scope=build_scope(base_scope, {"view": view}),
            )
            return
        # Otherwise, split the payload into parts that will fit into the 2‚ÄØKiB limit
        # Precompute replic field values (small data)
        replic_values: Dict[str, Any] = {}
        if replic_fields:
            for field in replic_fields:
                if field in fact_dict:
                    replic_values[field] = fact_dict[field]
        # Build the list of chunk strings (content_json_chunk) with dynamic sizing
        parts: list[str] = []
        # We will reduce the candidate content size until the entire serialized
        # memory entry fits within the 2‚ÄØKiB limit.  Initial candidate is
        # MAX_FACT_LENGTH, which is conservatively small.
        start_idx = 0
        total_len = len(payload_str)
        while start_idx < total_len:
            # Start with the maximum allowed raw content size
            candidate_len = min(MAX_FACT_LENGTH, total_len - start_idx)
            # Dynamically shrink the candidate until the resulting JSON entry is within limit
            while candidate_len > 0:
                part_content = payload_str[start_idx : start_idx + candidate_len]
                test_fact: Dict[str, Any] = {
                    "kind": f"{kind}_part",
                    "mission_id": mission_id,
                    "chunk_index": 0,  # placeholder; true index set later
                    "chunk_total": 0,  # placeholder; true total set later
                    "imported_at": _now(),
                    "content_json_chunk": part_content,
                }
                # copy replic field values to test_fact to measure overhead
                test_fact.update(replic_values)
                test_json = json.dumps(test_fact, ensure_ascii=False).encode("utf-8")
                if len(test_json) < 2040:
                    # Accept this candidate size
                    parts.append(part_content)
                    start_idx += candidate_len
                    break
                # Too large: shrink the candidate and try again
                candidate_len -= 50
            else:
                # If no candidate size worked, raise an error
                raise ValueError(
                    f"Unable to fit payload part into memory fact for mission {mission_id}; content may be too large."
                )
        # Now persist each part with accurate chunk_index and chunk_total
        total_parts = len(parts)
        for idx, part in enumerate(parts):
            part_fact: Dict[str, Any] = {
                "kind": f"{kind}_part",
                "mission_id": mission_id,
                "chunk_index": idx,
                "chunk_total": total_parts,
                "imported_at": _now(),
                "content_json_chunk": part,
            }
            part_fact.update(replic_values)
            client.agent_engines.create_memory(
                name=engine_name,
                fact=json.dumps(part_fact, ensure_ascii=False),
                scope=build_scope(base_scope, {"view": view}),
            )

    missions = data.get("full_mission_records", [])
    for rec in missions:
        # Derive mission identifiers; fall back to a digest of the record if missing
        mission_id = (
            rec.get("memory_id")
            or rec.get("mission_id")
            or _md5(json.dumps(rec, ensure_ascii=False, sort_keys=True))
        )
        mission_prompt = rec.get("mission_prompt")
        mission_type = rec.get("mission_type")
        final_plan = rec.get("final_plan")
        final_score = rec.get("final_score")
        tags = rec.get("tags", [])
        timestamp = rec.get("timestamp")
        aggregator_reasoning = rec.get("aggregator_reasoning")
        proposer_contributions = rec.get("proposer_contributions")
        critic = rec.get("critic", {}) or {}
        verdict = critic.get("verdict")
        critic_score = critic.get("score")
        weaknesses = critic.get("weaknesses", [])
        transcript = rec.get("full_transcript", [])

        # Base scope for all memories of this mission
        base_scope = {
            "corpus_signature": corpus_sig,
            "mission_id": mission_id,
            "mission_type": mission_type,
            "source": "learned_strategies_json",
        }

        # 1. Mission overview
        overview_fact = {
            "mission_prompt": mission_prompt,
            "mission_type": mission_type,
            "tags": tags,
            "timestamp": timestamp,
        }
        _create_memory_fact(kind="mission_overview", fact_dict=overview_fact, view="overview")

        # 2. Final plan (if present)
        if final_plan:
            plan_fact = {
                "plan": final_plan,
                "final_score": final_score,
            }
            _create_memory_fact(
                kind="final_plan",
                fact_dict=plan_fact,
                view="plan",
                replic_fields=("final_score",),
            )

        # 3. Aggregator summary (if present)
        if aggregator_reasoning or proposer_contributions:
            agg_fact = {
                "aggregator_reasoning": aggregator_reasoning,
                "proposer_contributions": proposer_contributions,
            }
            _create_memory_fact(
                kind="aggregator_summary",
                fact_dict=agg_fact,
                view="aggregator",
            )

        # 4. Critic report (if any critic info exists)
        if verdict or critic_score is not None or weaknesses:
            critic_fact = {
                "verdict": verdict,
                "score": critic_score,
                "weaknesses": weaknesses,
            }
            _create_memory_fact(
                kind="critic_report",
                fact_dict=critic_fact,
                view="critic",
                replic_fields=("verdict", "score", "weaknesses"),
            )

        # 5. Debate transcript (optional)
        if include_transcript and transcript:
            trans_fact = {
                "content": transcript,
            }
            _create_memory_fact(
                kind="debate_transcript",
                fact_dict=trans_fact,
                view="transcript",
            )

    # End of for loop
    # Explicitly return None for clarity
    return None


--- FILE: process_logger.py ---

"""
Prosty logger procesu generowania planu i rozm√≥w miƒôdzy agentami.
Wszystkie komunikaty sƒÖ dopisywane do pliku tekstowego z sygnaturƒÖ czasu.
"""

import sys
from datetime import datetime  # zamiast: import datetime

LOG_FILE = "process_log.txt"
STREAM_STDOUT = True

def log(msg: str):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")  # zamiast: datetime.datetime.now()
    line = f"[{ts}] {msg}"
    try:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(line + "\n")
    except Exception:
        pass
    if STREAM_STDOUT:
        print(line, file=sys.stdout, flush=True)


--- FILE: response_parser.py ---

"""
Inteligentny parser odpowiedzi agent√≥w z auto-korekcjƒÖ
"""
import json
import re
from typing import Dict, Any, Optional
import ast

# Lokalny logger procesu
from process_logger import log as process_log

class ResponseParser:
    """
    Zaawansowany parser kt√≥ry radzi sobie z r√≥≈ºnymi formatami odpowiedzi
    """
    
    def parse_agent_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        Parsuje odpowied≈∫ agenta pr√≥bujƒÖc r√≥≈ºnych strategii
        """
        if not response:
            return None
        # Zaloguj otrzymanƒÖ odpowied≈∫ (obcinamy do 200 znak√≥w, aby log nie r√≥s≈Ç nadmiernie)
        process_log(f"Received response: {response[:200]}")
        
        # Strategia 1: Czysty JSON
        parsed = self._try_pure_json(response)
        if parsed:
            process_log(f"Parsed using pure JSON: {parsed}")
            return parsed
        
        # Strategia 2: JSON z dodatkami (markdown, komentarze)
        parsed = self._try_extract_json(response)
        if parsed:
            process_log(f"Parsed using extract JSON: {parsed}")
            return parsed
        
        # Strategia 3: Python dict jako string (bez wykonywania kodu)
        parsed = self._try_python_dict(response)
        if parsed:
            process_log(f"Parsed using python-like dict: {parsed}")
            return parsed
        
        # Strategia 4: Strukturalna ekstrakcja
        parsed = self._try_structural_extraction(response)
        if parsed:
            process_log(f"Parsed using structural extraction: {parsed}")
            return parsed
        
        # Strategia 5: AI-based repair (u≈ºywa regex i heurystyk)
        parsed = self._try_ai_repair(response)
        if parsed:
            process_log(f"Parsed using AI repair: {parsed}")
            return parsed
        
        process_log(f"Parse failed: {response[:200]}")
        print(f"‚ö† Nie uda≈Ço siƒô sparsowaƒá odpowiedzi: {response[:100]}...")
        return None
    
    def _try_pure_json(self, response: str) -> Optional[Dict]:
        """Pr√≥buje parsowaƒá jako czysty JSON"""
        try:
            return json.loads(response.strip())
        except:
            return None
    
    def _try_extract_json(self, response: str) -> Optional[Dict]:
        """Ekstraktuje JSON z tekstu"""
        # Szukamy JSON w blokach kodu
        json_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
        match = re.search(json_pattern, response, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass
        
        # Szukamy pierwszego { i ostatniego }
        start = response.find('{')
        end = response.rfind('}')
        
        if start != -1 and end != -1 and end > start:
            try:
                return json.loads(response[start:end+1])
            except:
                pass
        
        return None
    
    def _try_python_dict(self, response: str) -> Optional[Dict]:
        """
        Pr√≥buje sparsowaƒá s≈Çownik zapisany w notacji Pythona bez u≈ºycia eval. Wyszukuje
        pierwszƒÖ strukturƒô w nawiasach klamrowych, nastƒôpnie zamienia pojedyncze cudzys≈Çowy
        na podw√≥jne i dodaje cudzys≈Çowy do kluczy, aby u≈ºyƒá json.loads. Je≈õli napotka b≈ÇƒÖd,
        zwraca None.
        """
        try:
            # Wyszukaj fragment przypominajƒÖcy s≈Çownik
            dict_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            match = re.search(dict_pattern, response)
            if not match:
                return None
            obj_str = match.group(0)
            # Zamie≈Ñ pojedyncze cudzys≈Çowy na podw√≥jne
            json_like = obj_str.replace("'", '"')
            # Dodaj cudzys≈Çowy do kluczy, je≈õli ich brakuje
            json_like = re.sub(r'(?<!\")\b([A-Za-z_][A-Za-z0-9_]*)\b\s*:', r'"\1":', json_like)
            return json.loads(json_like)
        except Exception:
            return None
    
    def _try_structural_extraction(self, response: str) -> Optional[Dict]:
        """Ekstraktuje strukturƒô na podstawie kluczowych s≈Ç√≥w"""
        result = {}
        
        # Szukamy kluczowych sekcji
        patterns = {
            "thought_process": r'(?:thought_process|thinking|reasoning)[:\s]+([^\n]+(?:\n(?!\w+:)[^\n]+)*)',
            "entry_point": r'(?:entry_point|start)[:\s]+["\']?(\w+)["\']?',
            "confidence": r'(?:confidence|certainty)[:\s]+(\d*\.?\d+)',
            "nodes": r'nodes[:\s]+\[(.*?)\]',
            "edges": r'edges[:\s]+\[(.*?)\]'
        }
        
        for key, pattern in patterns.items():
            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)
            if match:
                value = match.group(1).strip()
                
                if key == "confidence":
                    try:
                        result[key] = float(value)
                    except:
                        result[key] = 0.5
                elif key in ["nodes", "edges"]:
                    # Pr√≥buj sparsowaƒá jako listƒô
                    try:
                        result[key] = ast.literal_eval(f"[{value}]")
                    except:
                        result[key] = []
                elif key == "thought_process":
                    # Podziel na kroki
                    steps = [s.strip() for s in value.split('\n') if s.strip()]
                    result[key] = steps
                else:
                    result[key] = value
        
        return result if result else None
    
    def _try_ai_repair(self, response: str) -> Optional[Dict]:
        """Pr√≥buje naprawiƒá JSON u≈ºywajƒÖc heurystyk"""
        # Usu≈Ñ komentarze
        response = re.sub(r'//.*?\n', '', response)
        response = re.sub(r'/\*.*?\*/', '', response, flags=re.DOTALL)
        
        # Napraw typowe b≈Çƒôdy
        repairs = [
            (r',\s*}', '}'),  # Usu≈Ñ trailing commas
            (r',\s*]', ']'),
            (r'"\s*:\s*"([^"]*)"(?=[,}])', r'": "\1"'),  # Napraw cudzys≈Çowy
            (r'(\w+)(?=\s*:)', r'"\1"'),  # Dodaj cudzys≈Çowy do kluczy
            (r':\s*([^",\[\{}\]]+)(?=[,}])', r': "\1"'),  # Dodaj cudzys≈Çowy do warto≈õci
        ]
        
        for pattern, replacement in repairs:
            response = re.sub(pattern, replacement, response)
        
        # Spr√≥buj ponownie
        return self._try_pure_json(response)


--- FILE: retrieve_mission_memory.py ---

"""
Helper functions for reconstructing mission memory entries from a Vertex AI Agent
Engine Memory Bank.

When using the `persist_missions_to_vertex_memory` helper to store mission
records, long entries (e.g. final plans, aggregator reasoning or
transcripts) are split into multiple memory records, each with a
``content_json_chunk`` field, a ``chunk_index`` and a ``chunk_total``.  This
module provides a utility to fetch and reassemble these parts into a single
Python object for downstream processing.

Example usage::

    from vertexai import Client
    from tools.retrieve_mission_memory import retrieve_mission_memory

    client = Client(project=PROJECT_ID, location=LOCATION)
    mission_data = retrieve_mission_memory(
        engine_name=AGENT_ENGINE_NAME,
        mission_id="abcdefg-1234",
        client=client
    )
    # mission_data is a dict keyed by kind (e.g. 'final_plan', 'mission_overview')
    full_plan = mission_data["final_plan"]["content"]
    # ...

"""

from __future__ import annotations

import json
from typing import Any, Dict, List, Optional, Iterable

try:
    from vertexai import Client  # type: ignore
except ImportError:
    Client = Any  # pragma: no cover

__all__ = ["retrieve_mission_memory"]


def _assemble_chunks(items: Iterable[Dict[str, Any]]) -> Any:
    """
    Given an iterable of memory entries (dictionaries) that represent parts
    of a single logical fact, concatenate the ``content_json_chunk`` fields in
    order of ``chunk_index``, parse the resulting JSON and return the
    reconstructed Python object.  If the concatenated string cannot be
    deserialised, return the raw string.
    """
    items_sorted = sorted(items, key=lambda d: d.get("chunk_index", 0))
    combined_str = "".join(d.get("content_json_chunk", "") for d in items_sorted)
    try:
        return json.loads(combined_str)
    except json.JSONDecodeError:
        return combined_str


def retrieve_mission_memory(
    *,
    engine_name: str,
    mission_id: str,
    client: Client,
    view: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Retrieve and reassemble all memory records for a given mission.

    Parameters
    ----------
    engine_name: str
        Fully qualified name of the Agent Engine (e.g. ``projects/.../locations/.../reasoningEngines/...``).

    mission_id: str
        The identifier of the mission whose memories should be retrieved.

    client: vertexai.Client
        An initialised Vertex AI client.  Used to call ``retrieve_memories``.

    view: str, optional
        If provided, restricts retrieval to memories with this ``view`` scope
        (e.g. "overview", "plan", "aggregator", "critic", "transcript").

    Returns
    -------
    Dict[str, Any]
        A dictionary keyed by ``kind`` where each value is a reconstructed
        memory object.  For kinds that were split into multiple parts, the
        returned value is a dict with ``content`` holding the full Python
        object and any replicated metadata (e.g. ``final_score``, ``verdict``).
        For kinds that were stored in a single record, the original fact
        dictionary is returned.

    Notes
    -----
    This helper assumes that long facts are stored using the convention
    ``{kind}_part`` with ``chunk_index`` and ``chunk_total`` keys, and that
    the payload is serialised as JSON in the ``content_json_chunk`` field.
    """
    if not engine_name:
        raise ValueError("engine_name must be provided")
    if not mission_id:
        raise ValueError("mission_id must be provided")
    if client is None:
        raise ValueError("client must be provided")

    scope: Dict[str, Any] = {"mission_id": mission_id}
    if view:
        scope["view"] = view

    # Retrieve all memories matching the mission and optional view
    memories_iter = client.agent_engines.retrieve_memories(
        name=engine_name,
        scope=scope,
    )
    memories: List[Any] = list(memories_iter)

    # Group by base kind (strip off '_part' suffix if present)
    grouped: Dict[str, List[Dict[str, Any]]] = {}
    for m in memories:
        try:
            fact_dict = json.loads(m.memory.fact)
        except Exception:
            # fall back to raw string in case of unexpected format
            fact_dict = {"kind": "unknown", "content": m.memory.fact}
        kind = str(fact_dict.get("kind", ""))
        base_kind = kind[:-5] if kind.endswith("_part") else kind
        grouped.setdefault(base_kind, []).append(fact_dict)

    # Reconstruct each kind
    result: Dict[str, Any] = {}
    for kind_key, items in grouped.items():
        # Determine if this kind was split
        if any("content_json_chunk" in it for it in items):
            # Assemble the content
            full_content = _assemble_chunks(items)
            # Collect replicated metadata (first occurrence wins)
            metadata: Dict[str, Any] = {
                key: it[key]
                for it in items
                for key in ("final_score", "verdict", "score", "weaknesses", "tags", "timestamp", "mission_type")
                if key in it and key not in result.get(kind_key, {})
            }
            assembled = {"kind": kind_key, "mission_id": mission_id, "content": full_content}
            assembled.update(metadata)
            result[kind_key] = assembled
        else:
            # Single record: return as-is
            # Use the first entry (in case multiple matches exist)
            result[kind_key] = items[0]
    return result


--- FILE: run_debate.ipynb ---

from autogen_orchestrator import AutoGenMOAOrchestrator
import config_api


import vertexai
vertexai.init(project="dark-data-discovery", location="us-central1")

# Biblioteka wƒôz≈Ç√≥w u≈ºywana do generowania plan√≥w
NODE_LIBRARY = {
    'load_data': {'description': 'Wczytuje dane z r√≥≈ºnych ≈∫r√≥de≈Ç'},
    'clean_data': {'description': 'Czy≈õci dane'},
    'validate_data': {'description': 'Waliduje dane'},
    'discover_causality': {'description': 'Odkrywa relacje przyczynowe (mo≈ºe zawie≈õƒá)'},
    'error_handler': {'description': 'Obs≈Çuguje b≈Çƒôdy'},
    'rollback': {'description': 'Cofa zmiany'},
    'generate_report': {'description': 'Generuje raport'},
    'validate_model': {'description': 'Waliduje model'},
    'optimize_performance': {'description': 'Optymalizuje wydajno≈õƒá'},
    'train_model': {'description': 'Uczy model'},
    'notify_user': {'description': 'Powiadamia u≈ºytkownika'}
}

# Mo≈ºesz podaƒá misjƒô na sta≈Çe albo poprosiƒá u≈ºytkownika o wpisanie
mission = input("Podaj opis misji: ").strip()
if not mission:
    mission = "Stw√≥rz prosty pipeline do analizy danych CSV"

# Inicjalizacja orchestratora z definicjƒÖ misji i ≈õcie≈ºkƒÖ do konfiguracji agent√≥w
orchestrator = AutoGenMOAOrchestrator(
    mission=mission,
    node_library=NODE_LIBRARY,
    config_file="agents_config.json"
)

# Uruchom pe≈ÇnƒÖ debatƒô; wynik to s≈Çownik z finalnym planem lub None
final_plan = orchestrator.run_full_debate_cycle()

# Wy≈õwietl wynik w czytelnej formie
if final_plan:
    import json
    print("\n‚úÖ Zatwierdzony plan:")
    print(json.dumps(final_plan, indent=2, ensure_ascii=False))
else:
    print("\n‚ùå Nie uda≈Ço siƒô uzyskaƒá zatwierdzonego planu.")
# --- Koniec kom√≥rki ---



--- FILE: structured_response_parser.py ---

"""
Structured parser oparty na Pydantic.  Zamiast heurystycznych pr√≥b parsowania
rƒôcznego, wykorzystuje schematy Pydantic do walidacji odpowiedzi LLM.  Ten
modu≈Ç zastƒôpuje dotychczasowy `response_parser` w nowej konfiguracji.

Model `ProposerResponse` definiuje minimalnƒÖ strukturƒô planu wygenerowanego
przez agent√≥w‚Äëproposer√≥w.  Model `AggregatorResponse` rozszerza go o pole
`final_plan` oraz metadane u≈ºywane przez agregatora.  Model `CriticResponse`
zawiera ocenƒô, listƒô mocnych i s≈Çabych stron oraz ewentualne sugestie
poprawek, zgodnie z za≈Ço≈ºonym formatem JSON.

Je≈õli odpowied≈∫ nie jest poprawnym JSON‚Äëem (np. zawiera `````markdown````
fences) lub nie spe≈Çnia schematu, parser zwraca `None`.
"""

from __future__ import annotations

import json
import re
from typing import List, Optional, Dict, Any
from process_logger import log as process_log
from pydantic import BaseModel, ValidationError, Field


class ProposerPlan(BaseModel):
    """Reprezentuje plan proponowany przez agenta‚Äêproposera."""

    entry_point: str = Field(..., description="Nazwa pierwszego wƒôz≈Ça w planie")
    nodes: List[Dict[str, Any]] = Field(..., description="Lista wƒôz≈Ç√≥w planu")
    edges: List[Dict[str, Any]] = Field(..., description="Lista krawƒôdzi planu")


class ProposerResponse(BaseModel):
    """Struktura odpowiedzi agenta proponujƒÖcego."""

    thought_process: List[str] = Field(..., description="Opis krok√≥w rozumowania")
    plan: ProposerPlan = Field(..., description="Plan w formacie grafu")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Pewno≈õƒá (0‚Äì1)")
    key_innovations: Optional[List[str]] = Field(default_factory=list)
    risk_mitigation: Optional[Dict[str, Any]] = Field(default_factory=dict)


class AggregatorResponse(BaseModel):
    """Struktura odpowiedzi agregatora.  Rozszerza odpowied≈∫ proponera o finalny plan."""

    thought_process: List[str]
    final_plan: ProposerPlan
    synthesis_reasoning: Optional[str]
    component_sources: Optional[Dict[str, Any]]
    confidence_score: Optional[float]
    improvements: Optional[List[str]] = Field(default_factory=list)


class CriticResponse(BaseModel):
    """Struktura odpowiedzi krytyka."""

    approved: bool
    score: float = Field(..., ge=0.0, le=100.0)
    strengths: List[str] = Field(default_factory=list)
    weaknesses: List[str] = Field(default_factory=list)
    feedback: Optional[str]
    improvements: Optional[List[str]] = Field(default_factory=list)


class StructuredResponseParser:
    """
    Parser, kt√≥ry wykorzystuje modele Pydantic do walidacji i konwersji odpowiedzi
    na s≈Çowniki.  Oczekuje, ≈ºe agent zwraca poprawny JSON zgodny z jednym z
    powy≈ºszych schemat√≥w.  Mo≈ºna ≈Çatwo rozszerzyƒá o kolejne typy odpowiedzi.
    """

    def __init__(self) -> None:
        pass

    def _strip_code_fences(self, response: str) -> str:
        """Usuwa bloki kodu (```json ... ```) z odpowiedzi."""
        # Usu≈Ñ bloki ```json ... ``` lub ``` ... ```
        pattern = r"```(?:json)?\s*(\{.*?\})\s*```"
        match = re.search(pattern, response, re.DOTALL)
        if match:
            return match.group(1)
        return response

    def parse_agent_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        Przetwarza odpowied≈∫ agenta i pr√≥buje jƒÖ zmapowaƒá na jeden z
        zdefiniowanych modeli.  Zwraca zserializowanƒÖ postaƒá s≈ÇownikowƒÖ,
        lub None, je≈õli nie mo≈ºna sparsowaƒá.
        """
        if not response:
            return None

        # Usu≈Ñ otaczajƒÖce bloki kodu
        cleaned = self._strip_code_fences(response.strip())

        # Spr√≥buj sparsowaƒá jako JSON
        try:
            data = json.loads(cleaned)
        except Exception:
            return None

        # Kolejno pr√≥buj dopasowaƒá do modeli
        for model_cls in (ProposerResponse, AggregatorResponse, CriticResponse):
            try:
                obj = model_cls.parse_obj(data)
                return obj.dict()
            except ValidationError:
                continue

        # Je≈õli nic nie pasuje, zwr√≥ƒá oryginalne dane
        return data
    
    def parse_critic_response(self, text: str):
        """
        Parsuje odpowied≈∫ krytyka - zwraca CA≈ÅY JSON
        """
        import json
        import re

        if not text:
            return None

        try:
            # Usu≈Ñ markdown code blocks
            clean_text = text.strip()

            # Usu≈Ñ ```json i ```
            clean_text = re.sub(r'```json\s*', '', clean_text)
            clean_text = re.sub(r'```\s*', '', clean_text)

            # Usu≈Ñ PLAN_ZATWIERDZONY z ko≈Ñca
            if "PLAN_ZATWIERDZONY" in clean_text:
                # Znajd≈∫ ostatnie wystƒÖpienie i usu≈Ñ wszystko po nim
                parts = clean_text.rsplit("PLAN_ZATWIERDZONY", 1)
                clean_text = parts[0].strip()

            # Teraz po prostu sparsuj JSON
            result = json.loads(clean_text)

            # Debug - wypisz co znalaz≈Çe≈õ
            process_log(f"[PARSER] Znaleziono klucze: {list(result.keys())}")

            return result

        except json.JSONDecodeError as e:
            process_log(f"[PARSER] JSON decode error: {e}")

            # Plan B - znajd≈∫ JSON manualnie
            try:
                # Znajd≈∫ od pierwszego { do ostatniego }
                start = text.find('{')
                end = text.rfind('}')

                if start >= 0 and end > start:
                    json_str = text[start:end+1]
                    return json.loads(json_str)
            except:
                pass

        return None


--- FILE: config/models_config.py ---

"""
Definicje struktur danych u≈ºywanych do opisu r√≥l agent√≥w.
"""

from dataclasses import dataclass
from typing import List


@dataclass
class AgentRole:
    """
    Klasa opisujƒÖca rolƒô agenta w systemie multi‚Äëagentowym.

    :param role_name: Nazwa roli (np. "Causal Analyst", "Creative Planner").
    :param expertise_areas: Lista dziedzin, w kt√≥rych agent siƒô specjalizuje.
    :param thinking_style: Styl my≈õlenia ("analytical", "creative", "critical", "systematic" itp.).
    """
    role_name: str
    expertise_areas: List[str]
    thinking_style: str


