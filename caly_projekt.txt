--- FILE: autogen_orchestrator.py ---

"""
Pe≈Çny orchestrator MOA u≈ºywajƒÖcy AutoGen do zarzƒÖdzania debatƒÖ agent√≥w
"""
import json
from datetime import datetime
import config_api
from autogen import UserProxyAgent, ConversableAgent, GroupChat, GroupChatManager
from typing import Dict, List, Any, Optional
from datetime import datetime
import autogen
from google.cloud import secretmanager
from models_config import AgentRole
from moa_prompts import MOAPrompts
from memory_system import ContextMemory
# U≈ºywamy structured parsera zamiast heurystycznego response_parser
from structured_response_parser import StructuredResponseParser
from process_logger import log as process_log
import os, json, time, traceback
from typing import Any, Dict, Optional
from process_logger import log as process_log
import vertexai
from process_logger import log_exception as log_exc
from config_api import basic_config_agent
from exporter_missions_gcs import export_local_by_filename_date
from explainability_layer import ExplainabilityHooks
from memory_helpers import save_mission_to_gcs
EXPLAINABILITY = ExplainabilityHooks()

class AutoGenMOAOrchestrator:
    """
    Orchestrator systemu MOA u≈ºywajƒÖcy AutoGen do wieloturowej debaty
    """
    
    def __init__(self, mission: str, node_library: Dict[str, Any], config_file: str = "agents_config.json"):
        self.mission = mission
        self.node_library = node_library
        self.memory = ContextMemory(
        max_episodes=50,
        gcs_bucket=None,   # ten sam bucket co w Memory_bank.ipynb
        gcs_prefix=""                   # opcjonalnie; usu≈Ñ lub zostaw ""
        )
        # Parser oparty na Pydantic ‚Äì oczekuje czystego JSON zgodnego ze schematem
        self.parser = StructuredResponseParser()
        
        # Wczytaj konfiguracjƒô
        self._load_config(config_file)
        
        # Stan debaty
        self.iteration_count = 0
        self.max_iterations = 5
        self.current_context = {}
        self.final_plan = None
        self._forced_speaker: Optional[str] = None
        # Inicjalizuj agent√≥w AutoGen
        self.enable_sanity_ping = False
        process_log(f"[CFG] enable_sanity_ping={self.enable_sanity_ping}")
        self._initialize_autogen_agents()
        self._secret_cache = {}
        
        process_log(f"=== AutoGen MOA Orchestrator initialized for mission: {mission[:100]}... ===")
    
    
    #nowe helpery
    
    @staticmethod
    def _valid_memory_json(js: dict) -> bool:
        if not isinstance(js, dict):
            return False
        required = ("recommended_strategies", "common_pitfalls", "examples", "notes")
        if any(k not in js for k in required):
            return False
        return (
            isinstance(js["recommended_strategies"], list) and
            isinstance(js["common_pitfalls"], list) and
            isinstance(js["examples"], list) and
            isinstance(js["notes"], str)
        )

    
    def _extract_name_and_len(self, msg):
        """
        Zwraca (name, content_len, type_name) z wiadomo≈õci w wielu mo≈ºliwych formatach:
        - dict z kluczami name/content/sender/role
        - list/tuple w stylu [role, name, content] lub [name, content]
        - obiekt z atrybutami .name/.content
        - plain string
        """
        try:
            tname = type(msg).__name__
            # 1) dict
            if isinstance(msg, dict):
                name = msg.get("name") or msg.get("sender") or msg.get("role") or None
                content = msg.get("content") or msg.get("text") or ""
                return name, len(content or ""), tname

            # 2) list/tuple ‚Äì najczƒôstsze warianty
            if isinstance(msg, (list, tuple)):
                if len(msg) >= 3:
                    # [role, name, content]
                    name = str(msg[1])
                    content = str(msg[2])
                elif len(msg) == 2:
                    # [name, content] lub [role, content]
                    name = str(msg[0])
                    content = str(msg[1])
                elif len(msg) == 1:
                    name = None
                    content = str(msg[0])
                else:
                    name, content = None, ""
                return name, len(content or ""), tname

            # 3) obiekt z atrybutami
            if hasattr(msg, "name") or hasattr(msg, "content"):
                name = getattr(msg, "name", None)
                content = getattr(msg, "content", "") or ""
                return name, len(content or ""), tname

            # 4) fallback: traktuj jako string
            s = str(msg)
            return None, len(s), tname

        except Exception:
            # ostateczny fallback
            return None, 0, "unknown"

        
        
        
    #liczenie uzycia pamieci
    
    def _log_memory_alignment(self, plan_text: str, phase: str):
        """
        Miƒôkka telemetria: sprawdza, na ile 'plan_text' (JSON/tekst Aggregatora)
        pokrywa siƒô z seedowanƒÖ pamiƒôciƒÖ (self._last_seeded_memory).
        Nie modyfikuje prompt√≥w ani toku debaty ‚Äì tylko loguje.
        """
        import json, re

        mem = getattr(self, "_last_seeded_memory", None)
        if not mem or not plan_text:
            process_log(f"[MEMORY ALIGNMENT] skipped (mem_or_plan_missing) phase={phase}")
            return

        # --- 1) wyciƒÖgnij czysty tekst planu (je≈õli to JSON w stringu)
        txt = plan_text
        # spr√≥buj znale≈∫ƒá najwiƒôkszy blok JSON (ostro≈ºny regex)
        try:
            match = re.search(r"\{[\s\S]*\}", plan_text)
            if match:
                txt = match.group(0)
        except Exception:
            pass

        # --- 2) zbuduj 'work_text' do prostych dopasowa≈Ñ (lowercase)
        work_text = txt.lower()

        # --- 3) ≈∫r√≥d≈Ço pamiƒôci
        strategies = [s.strip().lower() for s in (mem.get("recommended_strategies") or []) if s and isinstance(s, str)]
        pitfalls   = [p.strip().lower() for p in (mem.get("common_pitfalls")       or []) if p and isinstance(p, str)]
        examples   = mem.get("examples") or []

        # --- 4) proste dopasowania substring (bez magii ‚Äì chodzi o telemetriƒô, nie scoring naukowy)
        def _hitcount(items, text):
            hits = []
            for it in items:
                if len(it) >= 5 and it in text:  # mini-pr√≥g, ≈ºeby ‚Äûcsv‚Äù itp. nie ≈Çapaƒá
                    hits.append(it)
            return hits

        used_strats   = _hitcount(strategies, work_text)
        addressed_pts = _hitcount(pitfalls,   work_text)

        # --- 5) examples: sprawdzimy, czy wystƒÖpi≈Çy ID lub URI w planie
        ex_total = len(examples)
        ex_hits = 0
        for ex in examples:
            mid = str(ex.get("mission_id","")).strip().lower()
            uri = str(ex.get("plan_uri","")).strip().lower()
            if (mid and mid in work_text) or (uri and uri in work_text):
                ex_hits += 1

        # --- 6) prosty score 0..1 (wagi mo≈ºesz zmieniƒá)
        import math
        def frac(num, den): 
            return (num/den) if den else 0.0

        s_frac = frac(len(used_strats),   len(strategies))
        p_frac = frac(len(addressed_pts), len(pitfalls))
        e_frac = frac(ex_hits,            ex_total)

        score = round(0.5 * s_frac + 0.3 * p_frac + 0.2 * e_frac, 3)

        # --- 7) log + opcjonalna integracja z EXPLAINABILITY (je≈õli jest)
        process_log(
            "[MEMORY ALIGNMENT] phase=%s score=%.3f "
            "strategies_used=%d/%d pitfalls_addressed=%d/%d examples_ref=%d/%d" % (
                phase, score,
                len(used_strats), len(strategies),
                len(addressed_pts), len(pitfalls),
                ex_hits, ex_total
            )
        )
        try:
            if "EXPLAINABILITY" in globals() and hasattr(EXPLAINABILITY, "on_memory_alignment"):
                EXPLAINABILITY.on_memory_alignment({
                    "phase": phase, "score": score,
                    "strategies_used": used_strats,
                    "pitfalls_addressed": addressed_pts,
                    "examples_hits": ex_hits,
                    "examples_total": ex_total
                })
        except Exception as e:
            process_log(f"[MEMORY ALIGNMENT][WARN] explainability hook failed: {type(e).__name__}: {e}")

        # --- 8) (opcjonalnie) kr√≥tki, niewp≈ÇywajƒÖcy podglƒÖd w transkrypcie
        try:
            if True:
            # if getattr(self.config, "show_memory_alignment_preview", False):
                preview = {
                    "phase": phase, "score": score,
                    "strategies_used": len(used_strats), "strategies_total": len(strategies),
                    "pitfalls_addressed": len(addressed_pts), "pitfalls_total": len(pitfalls),
                    "examples_ref": ex_hits, "examples_total": ex_total
                }
                # zapisz do historii ‚Äì jako wiadomo≈õƒá Orchestratora (nie zmienia m√≥wcy)
                self.groupchat.messages.append({
                    "role": "assistant", "name": "Orchestrator",
                    "content": f"[MEMORY ALIGNMENT PREVIEW] {preview}"
                })
        except Exception as e:
            process_log(f"[MEMORY ALIGNMENT][DEBUG] preview append failed: {type(e).__name__}: {e}")
    
    
    #koniec liczenia
    
    
    def _make_memory_message_once(self):
        """LIGHT -> walidacja -> MID fallback -> lokalny fallback (bez LLM). Zwraca pojedynczƒÖ wiadomo≈õƒá do historii czatu."""
        user_msg = {"role":"user","content": f"mission={self.mission}"}

        # 1) LIGHT (PRIMARY)
        if self.memory_analyst_agent:
            try:
                out = self.memory_analyst_agent.generate_reply(messages=[user_msg])
                try: js = json.loads(out)
                except Exception: js = None
                if self._valid_memory_json(js) and (len(js["recommended_strategies"])>=2 or len(js["examples"])>=1):
                    self._last_seeded_memory = js
                    process_log("[MEMORY][LIGHT] Valid memory JSON produced")
                    return {"name":"Memory_Analyst","role":"assistant","content": out}
            except Exception as e:
                log_exc(f"[MEMORY] LIGHT failed:", e)

            # 2) MID (FALLBACK)
            if self.memory_analyst_fallback_model:
                try:
                    mid_llm = self._build_llm_config(self.memory_analyst_fallback_model)
                    tmp = autogen.ConversableAgent(
                        name="Memory_Analyst_MID",
                        llm_config=mid_llm,
                        system_message=MOAPrompts.get_memory_analyst_prompt(),
                        human_input_mode="NEVER"
                    )
                    out = tmp.generate_reply(messages=[user_msg])
                    try: js = json.loads(out)
                    except Exception: js = None
                    if self._valid_memory_json(js):
                        self._last_seeded_memory = js
                        process_log("[MEMORY][MID] Valid memory JSON produced")
                        return {"name":"Memory_Analyst","role":"assistant","content": out}
                except Exception as e:
                    log_exc(f"[MEMORY] MID failed:", e)

        # 3) Fallback bez LLM ‚Äî lokalny + (opcjonalnie) Vertex
        try:
            ctx_local = self.memory.get_relevant_context(self.mission) if self.memory else {}
            get_v = getattr(self.memory, "get_vertex_context", None)
            ctx_vertex = get_v(self.mission) if callable(get_v) else {}
            tips = list(dict.fromkeys(
                (ctx_local.get("recommended_strategies") or []) +
                (ctx_vertex.get("recommended_strategies") or [])
            ))[:6]
            examples = (ctx_vertex.get("examples") or [])[:3]
            minimal = {
                "recommended_strategies": tips,
                "common_pitfalls": ctx_local.get("common_pitfalls") or [],
                "examples": [{"mission_id": e.get("mission_id",""), "plan_uri": e.get("plan_uri","")} for e in examples],
                "notes": ""
            }
            self._last_seeded_memory = minimal
            return {"name":"Memory_Analyst","role":"assistant","content": json.dumps(minimal, ensure_ascii=False)}
        except Exception as e:
            log_exc(f"[MEMORY] local fallback failed:",e)
            return {"name":"Memory_Analyst","role":"assistant","content": '{"recommended_strategies":[],"common_pitfalls":[],"examples":[],"notes":""}'}
    
    #koniec nowych helperow
    
    
    
    def reset(self):
        # ... resetuje liczniki ...
        # U≈ºywa wbudowanej metody .reset() do wyczyszczenia historii ka≈ºdego agenta
        all_agents = [self.user_proxy, *self.proposer_agents, self.aggregator_agent, self.critic_agent]
        for agent in all_agents:
            if agent:
                agent.reset()
    
    def _get_api_key_from_gcp_secret_manager(self, model_cfg: Dict) -> str | None:
        """
        Czyta klucz z GCP Secret Manager.
        Oczekuje: model_cfg["secret_manager"] = {"project_id": "...", "secret_id": "...", "version": "latest"|"1"|...}
        Zwraca: string lub None (gdy brak/nieudane).
        """
        sm = model_cfg.get("secret_manager") or {}
        project_id = (sm.get("project_id") or "").strip()
        secret_id  = (sm.get("secret_id") or "").strip()
        version    = (sm.get("version") or "latest").strip()

        if not project_id or not secret_id:
            return None

        cache_key = (project_id, secret_id, version)
        if cache_key in self._secret_cache:
            return self._secret_cache[cache_key]

        try:
            client = secretmanager.SecretManagerServiceClient()
            name = f"projects/{project_id}/secrets/{secret_id}/versions/{version}"
            resp = client.access_secret_version(name=name)
            value = resp.payload.data.decode("utf-8")
            # cache in-memory (nie logujemy!)
            self._secret_cache[cache_key] = value
            return value
        except Exception as e:
            # Nie loguj warto≈õci sekretu. Mo≈ºesz zalogowaƒá TYLKO metadane.
            from process_logger import log as process_log
            process_log(f"[SECRETS] Failed to read {secret_id}@{project_id}/{version}: {type(e).__name__}: {e}")
            return None
    
    
    #pamiec:
    
    def _build_memory_analyst_message(self) -> str:
        ctx_local = self.memory.get_relevant_context(self.mission) if self.memory else {}
        get_v = getattr(self.memory, "get_vertex_context", None)
        ctx_vertex = get_v(self.mission) if callable(get_v) else {}


        tips = []
        tips += ctx_local.get("recommended_strategies", []) or []
        tips += ctx_vertex.get("recommended_strategies", []) or []


        seen, dedup = set(), []
        for t in tips:
            if t not in seen:
                seen.add(t)
                dedup.append(t)
        tips = dedup[:8]


        examples = (ctx_vertex.get("examples") or [])[:3]
        lines = ["# MemoryAnalyst Summary"]
        if tips:
            lines.append("## Recommended strategies:")
            lines += [f"- {t}" for t in tips]
        if examples:
            lines.append("\n## Example plans:")
            for e in examples:
                mid = e.get("mission_id") or "unknown"
                puri = e.get("plan_uri") or ""
                lines.append(f"- {mid}: {puri}")


        return "\n".join(lines) if len(lines) > 1 else "# MemoryAnalyst Summary\n(no relevant memory found)"
    
    
    #Raport:
    def _ensure_dir(self, path: str):
        os.makedirs(path, exist_ok=True)

    def _now_stamp(self) -> str:
        return time.strftime("%Y%m%d_%H%M%S", time.localtime())

    def _extract_llm_hint(self, text: str) -> Optional[str]:
        """Prosta heurystyka do rozpoznawania typowych problem√≥w LLM-a."""
        if not text:
            return None
        
        #poprawka:
        if not isinstance(text, str):
            try:
                import json as _json
                text = _json.dumps(text, ensure_ascii=False)
            except Exception:
                text = str(text)
        
        
        #koniec_poprawki
        
        
        
        t = text.lower()
        hints = {
            "quota/rate_limit": ["rate limit", "too many requests", "quota", "insufficient_quota"],
            "context_length": ["maximum context length", "token limit", "context window", "too many tokens"],
            "safety": ["safety", "blocked", "content filter"],
            "auth/api": ["invalid api key", "unauthorized", "forbidden", "permission"],
            "timeout": ["timeout", "timed out", "deadline exceeded"]
        }
        for label, kws in hints.items():
            if any(k in t for k in kws):
                return label
        return None

    def _write_failure_report(
        self,
        reason: str,
        stage: str,
        aggregator_raw: Optional[str],
        critic_raw: Optional[str],
        exception: Optional[BaseException] = None,
        parsed_aggregator: Optional[Dict[str, Any]] = None
    ) -> str:
        
        
        #poprawka:
        def _safe_str(x):
            if x is None:
                return ""
            if isinstance(x, str):
                return x
            try:
                import json as _json
                return _json.dumps(x, ensure_ascii=False)
            except Exception:
                return str(x)
        
        
        
        #koniec poprawki:
        
        """Zapisuje raport awaryjny JSON + MD i zwraca ≈õcie≈ºkƒô do pliku JSON."""
        self._ensure_dir("reports")
        ts = self._now_stamp()
        jpath = f"reports/failure_report_{ts}.json"
        mpath = f"reports/failure_report_{ts}.md"

        agg_hint = self._extract_llm_hint(aggregator_raw or "")
        crit_hint = self._extract_llm_hint(critic_raw or "")

        report = {
            "timestamp": ts,
            "mission": self.mission,
            "stage": stage,  # np. "aggregator", "groupchat", "critic"
            "reason": reason,  # np. "AGGREGATOR_NO_VALID_JSON", "EXCEPTION_DURING_DEBATE"
            "aggregator_model": getattr(self, "aggregator_config", {}).get("model", {}),
            "critic_model": getattr(self, "critic_config", {}).get("model", {}),
            "aggregator_output_excerpt": _safe_str(aggregator_raw or "")[:4000],
            "critic_output_excerpt": _safe_str(critic_raw or "")[:4000],
            "aggregator_llm_hint": agg_hint,
            "critic_llm_hint": crit_hint,
            "parsed_aggregator": parsed_aggregator,
            "exception": None if not exception else {
                "type": type(exception).__name__,
                "message": str(exception),
                "traceback": traceback.format_exc()
            }
        }

        with open(jpath, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # kr√≥tkie MD dla ludzi
        with open(mpath, "w", encoding="utf-8") as f:
            f.write(f"# Failure Report ({ts})\n\n")
            f.write(f"**Mission:** {self.mission}\n\n")
            f.write(f"**Stage:** {stage}\n\n")
            f.write(f"**Reason:** {reason}\n\n")
            if agg_hint:
                f.write(f"**Aggregator LLM hint:** `{agg_hint}`\n\n")
            if crit_hint:
                f.write(f"**Critic LLM hint:** `{crit_hint}`\n\n")
            if exception:
                f.write(f"**Exception:** `{type(exception).__name__}: {exception}`\n\n")
            f.write("## Last Aggregator Output (excerpt)\n\n")
            f.write("```\n" +  _safe_str(aggregator_raw or "")[:4000] + "\n```\n\n")
            f.write("## Last Critic Output (excerpt)\n\n")
            f.write("```\n" +  _safe_str(critic_raw or "")[:4000] + "\n```\n")

        
        process_log(f"[FAILSAFE] Saved failure report: {jpath}")

        return jpath

    def _get_last_message_from(self, groupchat, agent_name: str) -> Optional[str]:
        """Zwraca tekst ostatniej wiadomo≈õci danego agenta z obiektu GroupChat."""
        try:
            msgs = getattr(groupchat, "messages", [])
            for m in reversed(msgs):
                if (m.get("name") or m.get("role")) == agent_name:
                    return m.get("content") or ""
        except Exception:
            pass
        return None
    
    
    # ========== UNIVERSAL JSON REPAIR ==========

    MAX_REPAIR_ATTEMPTS = 2
    REPAIR_JSON_SUFFIX = "\n\nZWR√ìƒÜ TYLKO I WY≈ÅƒÑCZNIE JSON, bez komentarzy, bez dodatkowego tekstu."

    def _schema_example_for(self, role: str) -> str:
        if role == "proposer":
            return (
                '{\n'
                '  "thought_process": ["Krok 1...", "Krok 2..."],\n'
                '  "plan": {\n'
                '    "entry_point": "Start_Node",\n'
                '    "nodes": [ {"name":"Start_Node","implementation":"load_data"} ],\n'
                '    "edges": [ {"from":"Start_Node","to":"Next_Node","condition":"on_success"} ]\n'
                '  },\n'
                '  "confidence": 0.80\n'
                '}'
            )
        if role == "aggregator":
            return (
                '{\n'
                '  "thought_process": ["Agregujƒô elementy X i Y..."],\n'
                '  "final_plan": {\n'
                '    "entry_point": "Start_Node",\n'
                '    "nodes": [ {"name":"Start_Node","implementation":"load_data"} ],\n'
                '    "edges": [ {"from":"Start_Node","to":"Next_Node","condition":"on_success"} ]\n'
                '  },\n'
                '  "confidence_score": 0.90\n'
                '}'
            )
        if role == "critic":
            return (
                '{\n'
                '  "critique_summary": {\n'
                '    "verdict": "ZATWIERDZONY",\n'
                '    "statement": "Uzasadnienie...",\n'
                '    "key_strengths": ["..."],\n'
                '    "identified_weaknesses": [{"weakness":"...", "severity":"Low", "description":"..."}]\n'
                '  },\n'
                '  "quality_metrics": {\n'
                '    "Complexity_Score_C": 3.1,\n'
                '    "Robustness_Score_R": 50,\n'
                '    "Innovation_Score_I": 100,\n'
                '    "Completeness_Score": 100,\n'
                '    "Overall_Quality_Q": 84.07\n'
                '  },\n'
                '  "final_synthesized_plan": {\n'
                '    "entry_point": "Start_Node",\n'
                '    "nodes": [ {"name":"Start_Node","implementation":"load_data"} ],\n'
                '    "edges": [ {"from":"Start_Node","to":"Next_Node","condition":"on_success"} ]\n'
                '  }\n'
                '}'
            )
        return "{}"

    def _try_parse_by_role(self, role: str, text: str):
        try:
            if role == "proposer":
                parsed = self.parser.parse_agent_response(text)
            elif role == "aggregator":
                parsed = self.parser.parse_aggregator_response(text)
            elif role == "critic":
                parsed = self.parser.parse_critic_response(text)
            else:
                return None, f"Unknown role: {role}"
            if parsed:
                return parsed, None
            return None, "Parser returned None"
        except Exception as e:
            return None, f"{type(e).__name__}: {e}"

    def _repair_prompt_for(self, role: str, err_msg: str) -> str:
        return (
            f"Twoja poprzednia odpowied≈∫ NIE SPE≈ÅNIA wymaganego schematu dla roli '{role}'.\n"
            f"B≈ÇƒÖd/diagnoza parsera: {err_msg}\n\n"
            f"Wymagana struktura JSON (minimalny przyk≈Çad):\n{self._schema_example_for(role)}\n"
            f"{REPAIR_JSON_SUFFIX}"
        )

    def _force_one_turn(self, agent, manager) -> str:
        self._forced_speaker = agent.name
        try:
            manager.step()  # je≈õli Twoja wersja AG2 nie wspiera .step(), u≈ºyj run(max_round=1)
        except Exception:
            pass
        return self._get_last_message_from(manager.groupchat, agent.name) or ""

    def _auto_repair_and_parse(self, role: str, agent, manager, last_text: str):
        parsed, err = self._try_parse_by_role(role, last_text or "")
        if parsed:
            return parsed
        
        for attempt in range(1, MAX_REPAIR_ATTEMPTS + 1):
            repair_msg = self._repair_prompt_for(role, err or "Invalid JSON")
            manager.groupchat.messages.append({
                "role": "user",
                "name": "Orchestrator",
                "content": repair_msg
            })
            process_log(f"[REPAIR][{role}] attempt {attempt}: requesting strictly JSON output.")
            repaired_text = self._force_one_turn(agent, manager)
            parsed, err2 = self._try_parse_by_role(role, repaired_text or "")
            if parsed:
                return parsed
            err = err2
        return None
    
    
    
    def _load_config(self, config_file: str):
        """Wczytuje konfiguracjƒô agent√≥w"""
        with open(config_file, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
    
    
    def _is_final_plan_message(self, m: dict) -> bool:
        """Ko≈Ñczymy TYLKO na odpowiedzi CRITICA, gdy ko≈Ñczy siƒô markerem."""
        content = (m.get("content") or "").strip()
        name = (m.get("name") or "").lower()
        role = (m.get("role") or "").lower()
        return role == "assistant" and content.endswith("PLAN_ZATWIERDZONY") and "critic" in name
    
    

    def custom_speaker_selection_logic(self, last_speaker, groupchat):
        """
        Proposers ‚Üí Aggregator ‚Üí Critic. Je≈õli Critic nie zatwierdzi, nowa iteracja od pierwszego Proposera.
        Por√≥wnujemy po NAZWACH z historii wiadomo≈õci (AutoGen mo≈ºe podawaƒá inne instancje agent√≥w).
        """
        msgs = groupchat.messages
        
        for msg in msgs:
            if "PLAN_ZATWIERDZONY" in msg.get("content", ""):
                raise StopIteration("Plan zatwierdzony - ko≈Ñczymy debatƒô")
        
        last_name = (msgs[-1].get("name") or "").lower() if msgs else ""
        last_content = (msgs[-1].get("content") or "")

        if last_name and last_content:
            # Znajd≈∫ prompt kt√≥ry wywo≈Ça≈Ç tƒô odpowied≈∫
            # To bƒôdzie przedostatnia wiadomo≈õƒá lub poczƒÖtkowy bootstrap
            prompt_for_last = ""
            if len(msgs) >= 2:
                prompt_for_last = msgs[-2].get("content", "")
            elif len(msgs) == 1:
                # Pierwsza odpowied≈∫ - prompt to bootstrap z run_full_debate_cycle
                prompt_for_last = getattr(self, '_initial_bootstrap', '')

            # Zapisz wyja≈õnialno≈õƒá
            EXPLAINABILITY.on_response_received(
                prompt=prompt_for_last,
                response=last_content,
                agent_name=last_name
            )
        
        
        
        # --- [DODANE] Memory Analyst ma tylko jednƒÖ wypowied≈∫ na starcie ---
        ma_name = (getattr(self, "memory_analyst_agent", None).name or "").lower() \
                  if getattr(self, "memory_analyst_agent", None) else ""
        if last_name == ma_name:
            # Po jednorazowym komunikacie pamiƒôci przechodzimy do pierwszego Proposera
            return self.proposer_agents[0]

        # ‚ù∂ Po bootstrapie (ostatni by≈Ç Orchestrator ‚Üí wybieramy pierwszego proposera)
        if last_name == (self.user_proxy.name or "").lower() and last_content.strip():
            return self.proposer_agents[0]

        # ‚ù∑ Po Aggregatorze ‚Üí czas na Critica
        if last_name == (self.aggregator_agent.name or "").lower():
            try:
                # zapamiƒôtaj ostatni tekst planu do p√≥≈∫niejszego u≈ºytku (np. post-approval)
                self._last_aggregated_plan_text = last_content
                # policz adhezjƒô tu≈º po agregacji
                self._log_memory_alignment(last_content, phase="post_aggregation")
            except Exception as e:
                process_log(f"[MEMORY ALIGNMENT][WARN] post_aggregation failed: {type(e).__name__}: {e}")
            return self.critic_agent

        # ‚ù∏ Po Criticu ‚Üí zatwierdzenie albo nowa iteracja
        if last_name == (self.critic_agent.name or "").lower():
            self._save_iteration_to_memory(last_content, self.iteration_count)
            if "PLAN_ZATWIERDZONY" in last_content:
                try:
                    plan_text = getattr(self, "_last_aggregated_plan_text", "") or last_content
                    self._log_memory_alignment(plan_text, phase="post_approval")
                except Exception as e:
                    process_log(f"[MEMORY ALIGNMENT][WARN] post_approval failed: {type(e).__name__}: {e}")
                return None
                
                
                
                
                # return None
            # nowa iteracja
            self.iteration_count += 1
            if self.iteration_count >= self.max_iterations:
                process_log(f"[FAILSAFE] OsiƒÖgniƒôto maksymalnƒÖ liczbƒô iteracji ({self.max_iterations}). Koniec debaty.")
                return None
            process_log(f"===== ROZPOCZYNAM ITERACJƒò DEBATY NR {self.iteration_count + 1} =====")
            self._update_context_from_last_critique(last_content)
            return self.proposer_agents[0]

        # ‚ùπ WewnƒÖtrz puli proposer√≥w ‚Äì leƒá kolejno po nazwach
        proposer_names = [p.name.lower() for p in self.proposer_agents]
        if last_name in proposer_names:
            idx = proposer_names.index(last_name)
            if idx < len(self.proposer_agents) - 1:
                return self.proposer_agents[idx + 1]
            return self.aggregator_agent  # po ostatnim proposerze m√≥wi Aggregator

        # ‚ù∫ Domy≈õlnie ‚Äì zacznij od pierwszego proposera
        return self.proposer_agents[0]
    
    
    def _save_iteration_to_memory(self, critic_response: str, iteration: int):
        """Zapisuje dane z iteracji do pamiƒôci"""
        try:
            
            if not self.memory:
                return
            
            # Parse odpowiedzi krytyka
            parsed = self.parser.parse_critic_response(critic_response)
            if not parsed:
                process_log(f"[MEMORY] Nie mogƒô sparsowaƒá odpowiedzi krytyka w iteracji {iteration}")
                return

            # WyciƒÖgnij kluczowe dane
            score = parsed.get("quality_metrics", {}).get("Overall_Quality_Q", 0)
            weaknesses = parsed.get("critique_summary", {}).get("identified_weaknesses", [])
            verdict = parsed.get("critique_summary", {}).get("verdict", "")

            # Stw√≥rz feedback string
            feedback_data = {
                "score": score,
                "verdict": verdict,
                "weaknesses": [w.get("weakness", "") for w in weaknesses if isinstance(w, dict)],
                "iteration": iteration
            }

            # ZAPISZ DO PAMIƒòCI
            self.memory.add_iteration_feedback(
                iteration=iteration,
                feedback=json.dumps(feedback_data),
                timestamp=datetime.now()
            )

            process_log(f"[MEMORY] Zapisano iteracjƒô {iteration}: score={score}, verdict={verdict}")

        except Exception as e:
            process_log(f"[MEMORY ERROR] B≈ÇƒÖd zapisu iteracji {iteration}: {e}")

    
    def _initialize_autogen_agents(self):
        """Inicjalizuje agent√≥w AutoGen dla debaty ‚Äî minimalistycznie i niezawodnie."""
        # Atrybuty ZAWSZE istniejƒÖ
        self.proposer_agents = []
        self.aggregator = None
        self.critic = None
        self.aggregator_agent = None
        self.critic_agent = None
        self.memory_analyst_agent = None
        self.memory_analyst_fallback_model = None
        
        
        # User Proxy ‚Äì nigdy nie ko≈Ñczy rozmowy
        self.user_proxy = autogen.ConversableAgent( # <-- POPRAWKA
        name="Orchestrator",
        human_input_mode="NEVER",
        llm_config=False,  # Ten agent nie potrzebuje LLM, tylko rozpoczyna rozmowƒô
        system_message="You are the orchestrator who starts the debate and then observes."
        )
        self.user_proxy.silent = False
        process_log("[INIT] UserProxy initialized")

        # Proposerzy
        for agent_config in self.config['agents']:
            rn = agent_config['role_name'].lower()
            if 'aggregator' in rn or 'critic' in rn or 'memory analyst' in rn:
                continue
            role = AgentRole(
                role_name=agent_config['role_name'],
                expertise_areas=agent_config['expertise_areas'],
                thinking_style=agent_config['thinking_style']
            )
            prompt = self._build_proposer_prompt(role)
            ag = autogen.ConversableAgent(
                name=agent_config['role_name'].replace(" ", "_"),
                llm_config=self._build_llm_config(agent_config['model']),
                system_message=prompt,
                human_input_mode="NEVER"
            )
            ag.silent = False
            self.proposer_agents.append(ag)
            process_log(f"[INIT] Proposer initialized: {ag.name}")

        #memory agent
        
        # --- Memory Analyst (lekki agent, m√≥wi tylko raz na starcie; prompt z MOAPrompts) ---
        mem_cfg = next((a for a in self.config['agents'] if 'memory analyst' in a['role_name'].lower()), None)
        if mem_cfg:
            try:
                mem_llm = self._build_llm_config(mem_cfg['model'])
                self.memory_analyst_agent = autogen.ConversableAgent(
                    name="Memory_Analyst",
                    llm_config=mem_llm,
                    system_message=MOAPrompts.get_memory_analyst_prompt(),
                    human_input_mode="NEVER"
                )
                self.memory_analyst_agent.silent = False
                self.memory_analyst_fallback_model = mem_cfg.get("fallback")  # dict lub None
                process_log("[INIT] Memory Analyst initialized")
            except Exception as e:
                process_log(f"[INIT][WARN] Memory Analyst init failed: {e}")
        else:
            process_log("[INIT] Memory Analyst not configured")
        
        
        
        #koniec memory agent
        
            
        # Aggregator
        aggregator_config = next((a for a in self.config['agents'] if 'aggregator' in a['role_name'].lower()), None)
        if aggregator_config:
            self.aggregator = autogen.ConversableAgent(
                name="Master_Aggregator",
                llm_config=self._build_llm_config(aggregator_config['model']),
                system_message=MOAPrompts.get_aggregator_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=lambda m: False,
            )
        else:
            self.aggregator = autogen.ConversableAgent(
                name="Master_Aggregator",
                llm_config={"config_list": [{"model": "dummy", "api_type": "dummy"}]},
                system_message=MOAPrompts.get_aggregator_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=lambda m: False,
            )
        self.aggregator.silent = False
        self.aggregator_agent = self.aggregator
        process_log("[INIT] Aggregator initialized")

        # Critic
        critic_config = next((a for a in self.config['agents'] if 'critic' in a['role_name'].lower()), None)
        if critic_config:
            self.critic = autogen.ConversableAgent(
                name="Quality_Critic",
                llm_config=self._build_llm_config(critic_config['model']),
                system_message=self._build_critic_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=self._is_final_plan_message,
            )
        else:
            self.critic = autogen.ConversableAgent(
                name="Quality_Critic",
                llm_config={"config_list": [{"model": "dummy", "api_type": "dummy"}]},
                system_message=self._build_critic_prompt(),
                human_input_mode="NEVER",
                is_termination_msg=self._is_final_plan_message,
            )
        self.critic.silent = False
        self.critic_agent = self.critic
        process_log("[INIT] Critic initialized")

        process_log(f"Initialized {len(self.proposer_agents)} proposers, 1 aggregator, 1 critic using AutoGen")
    
    
    
    def reset(self):
        """
        Minimalny reset: czy≈õci historiƒô agent√≥w i liczniki sesji,
        bez dotykania cache'u LLM i bez zmian w konfiguracji.
        """
        agents = []

        # Zbierz agent√≥w, je≈õli istniejƒÖ (nie zak≈Çadamy, ≈ºe wszystkie sƒÖ zainicjalizowane)
        if getattr(self, "user_proxy", None):           agents.append(self.user_proxy)
        if getattr(self, "proposer_agents", None):      agents.extend(self.proposer_agents)
        if getattr(self, "aggregator_agent", None):     agents.append(self.aggregator_agent)
        if getattr(self, "critic_agent", None):         agents.append(self.critic_agent)
        if getattr(self, "memory_analyst_agent", None): agents.append(self.memory_analyst_agent)

        # Czy≈õƒá historie czatu agent√≥w (je≈õli wspierajƒÖ .reset())
        for ag in agents:
            try:
                if hasattr(ag, "reset") and callable(ag.reset):
                    ag.reset()
            except Exception as e:
                process_log(f"[RESET][WARN] {getattr(ag,'name','<agent>')}: {e}")

        # Zresetuj licznik iteracji i bootstrap
        self.iteration_count = 0
        self._initial_bootstrap = ""

        # (opcjonalnie) miƒôkko wyczy≈õƒá lokalny kontekst pamiƒôci, je≈õli ma takie API
        try:
            mem = getattr(self, "memory", None)
            if mem and hasattr(mem, "reset") and callable(mem.reset):
                mem.reset()
            elif mem and hasattr(mem, "clear") and callable(mem.clear):
                mem.clear()
        except Exception as e:
            process_log(f"[RESET][WARN] memory: {e}")

        process_log("[RESET] Orchestrator state cleared")
    
    
    
    
    def _runtime_env_snapshot(self) -> dict:
        # tylko presence, bez warto≈õci
        def present(k): return bool(os.getenv(k))
        return {
            "VERTEXAI_PROJECT": present("VERTEXAI_PROJECT") or present("GOOGLE_CLOUD_PROJECT") or present("GCP_PROJECT"),
            "VERTEXAI_LOCATION": present("VERTEXAI_LOCATION") or present("GOOGLE_CLOUD_REGION"),
            "ANTHROPIC_API_KEY": present("ANTHROPIC_API_KEY"),
            "OPENAI_API_KEY": present("OPENAI_API_KEY"),
        }

    def _agent_signature(self, agent) -> dict:
        llm = getattr(agent, "llm_config", {})
        # wyciƒÖgamy pierwszy wpis z config_list dla kr√≥tkiego podpisu
        vendor = None; model = None
        try:
            entry = (llm.get("config_list") or [{}])[0]
            if "google" in entry:
                vendor = "google"; model = entry["google"].get("model")
            elif "anthropic" in entry:
                vendor = "anthropic"; model = entry["anthropic"].get("model")
            elif "openai" in entry:
                vendor = "openai"; model = entry["openai"].get("model")
            else:
                vendor = (entry.get("api_type") or "unknown")
                model = entry.get("model")
        except Exception:
            pass
        return {"name": getattr(agent, "name", "?"), "vendor": vendor, "model": model}

    def _sanity_ping_agent(self, agent) -> None:
        tmp_user = UserProxyAgent(
            "sanity_user", human_input_mode="NEVER", max_consecutive_auto_reply=1,
            is_termination_msg=lambda m: True, code_execution_config=False
        )
        try:
            tmp_user.initiate_chat(agent, message="Odpowiedz dok≈Çadnie s≈Çowem: PONG")
        except Exception as e:
            sig = self._agent_signature(agent)
            snap = self._runtime_env_snapshot()
            raise RuntimeError(
                f"[SANITY PING FAILED] agent={sig} | env={snap} | err={type(e).__name__}: {e}"
            ) from e
    
    
    
    
    
    
    def _build_llm_config(self, model_config: dict) -> dict:
        """
        Buduje llm_config dla AutoGen na bazie agents_config.json,
        u≈ºywajƒÖc config_api.basic_config_agent (ten sam format co w solo).
        Google => Vertex/ADC (bez api_key), Anthropic/OpenAI => klucze z ENV/SM.
        """
        from config_api import basic_config_agent, PROJECT_ID as DEFAULT_PROJECT_ID, LOCATION as DEFAULT_LOCATION

        # --- helpery ---
        DEFAULT_MODEL_BY_PROVIDER = {
            "google": "gemini-2.5-pro",
            "anthropic": "claude-3-7-sonnet",
            "openai": "gpt-4o-mini",
        }

        def _map_provider_to_api_type(provider: str) -> str:
            p = (provider or "google").strip().lower()
            return {
                "google": "google", "gemini": "google", "vertex": "google",
                "anthropic": "anthropic",
                "openai": "openai", "azure_openai": "openai",
            }.get(p, p)

        def _validate_provider_model_pair(api_type: str, model: str) -> None:
            m = (model or "").lower()
            if api_type == "google" and not m.startswith("gemini"):
                raise ValueError(f"Model '{model}' nie pasuje do providera 'google' (Vertex/Gemini).")
            if api_type == "anthropic" and not m.startswith("claude"):
                raise ValueError(f"Model '{model}' nie pasuje do 'anthropic'.")
            if api_type == "openai" and not ("gpt" in m or m.startswith("o")):
                raise ValueError(f"Model '{model}' nie wyglƒÖda na model OpenAI.")

        # 1) provider -> api_type
        api_type = _map_provider_to_api_type(model_config.get("provider"))

        # 2) model + sanity
        agent_name = model_config.get("model_name") or DEFAULT_MODEL_BY_PROVIDER.get(api_type, "gemini-2.5-pro")
        _validate_provider_model_pair(api_type, agent_name)

        # 3) projekt/region tylko dla Google/Vertex
        project_id = model_config.get("project_id") or DEFAULT_PROJECT_ID
        location   = model_config.get("location")   or DEFAULT_LOCATION
        if api_type == "google" and not project_id:
            raise RuntimeError("Vertex/Gemini: brak project_id. Ustaw VERTEXAI_PROJECT/GOOGLE_CLOUD_PROJECT albo podaj 'project_id' w agents_config.json.")

        # 4) api_key tylko dla nie-Google
        api_key_arg = None if api_type == "google" else model_config.get("api_key")

        # 5) wo≈Çamy Tw√≥j builder
        flat_list = basic_config_agent(
            agent_name = agent_name,
            api_type   = api_type,
            location   = (location if api_type == "google" else None),   # <-- KLUCZOWA ZMIANA
            project_id = (project_id if api_type == "google" else None), # <-- KLUCZOWA ZMIANA
            api_key    = api_key_arg,
        )
        if not isinstance(flat_list, list) or not flat_list:
            raise ValueError("basic_config_agent powinien zwr√≥ciƒá niepustƒÖ listƒô.")

        entry = dict(flat_list[0])  # kopia, ≈ºeby m√≥c czy≈õciƒá

        # 6) sanity: dla nie-Google WYTNJIJ project/location (gdyby kiedy≈õ zn√≥w wpad≈Çy)
        if api_type != "google":
            entry.pop("project_id", None)
            entry.pop("location", None)

        # 7) finalny llm_config
        return {
            "config_list": [entry],
            "temperature": float(model_config.get("temperature", 0.0)),
            "seed": 42,
            "cache_seed": 42,
        }
       
    
    def _build_proposer_prompt(self, role: AgentRole) -> str:
        """Buduje prompt dla proposera z kontekstem"""
        base_prompt = MOAPrompts.get_proposer_prompt(role, self.mission, self.node_library)
        
        # Dodaj dynamiczny kontekst
        if self.current_context:
            context_injection = self._build_context_injection()
            base_prompt = base_prompt + "\n\n" + context_injection
            # return base_prompt + "\n\n" + context_injection
        
        base_prompt = EXPLAINABILITY.on_prompt_build(base_prompt, role.role_name)
        return base_prompt
    
    def _build_critic_prompt(self) -> str:
        """Buduje prompt dla krytyka"""
        base_prompt = MOAPrompts.get_critic_prompt()

        # Dodaj specjalnƒÖ instrukcjƒô o frazie ko≈ÑczƒÖcej i nowej strukturze JSON
        additional_instruction = """

        ## CRITICAL OUTPUT STRUCTURE
        - If you REJECT the plan, provide your standard critique with weaknesses and suggestions.
        - If you APPROVE the plan, your JSON response MUST contain a top-level key named `plan_approved`. Inside this key, you MUST place the complete, final, synthesized plan object. The other keys (like critique_summary) should still be present.

        Example of an APPROVED response structure:
        ```json
        {
          "critique_summary": {
            "verdict": "ZATWIERDZONY",
            "statement": "Plan jest doskona≈Çy, spe≈Çnia wszystkie wymagania.",
            ...
          },
          "plan_approved": {
            "entry_point": "Start_Node",
            "nodes": [ ... ],
            "edges": [ ... ]
          },
          ...
        }
        ```

        ## GOLDEN TERMINATION RULE
        If you approve the plan, you MUST end your ENTIRE response with the exact phrase on a new line, after the JSON block:
        PLAN_ZATWIERDZONY
        """
        return EXPLAINABILITY.on_prompt_build(base_prompt + additional_instruction, "Quality_Critic")
        # return base_prompt + additional_instruction
    
    def _build_context_injection(self) -> str:
        """Buduje wstrzykniƒôcie kontekstu"""
        parts = []
        
        if self.current_context.get('recommended_strategies'):
            parts.append("## üí° RECOMMENDED STRATEGIES (from memory):")
            for strategy in self.current_context['recommended_strategies']:
                parts.append(f"‚Ä¢ {strategy}")
        
        if self.current_context.get('common_pitfalls'):
            parts.append("\n## ‚ö†Ô∏è COMMON PITFALLS TO AVOID:")
            for pitfall in self.current_context['common_pitfalls']:
                parts.append(f"‚Ä¢ {pitfall}")
        
        if self.current_context.get('last_feedback'):
            parts.append(f"\n## üìù LAST FEEDBACK:\n{self.current_context['last_feedback']}")
        
        return "\n".join(parts)
    

    def run_full_debate_cycle(self):
        from autogen import GroupChat, GroupChatManager
        import json, os, traceback
        from datetime import datetime
        self.reset()
        # Lazy-guard: je≈õli kto≈õ zawo≈Ça przed init
        for must in ("user_proxy", "proposer_agents", "aggregator_agent", "critic_agent"):
            if not hasattr(self, must) or getattr(self, must) is None:
                self._initialize_autogen_agents()
                break

        # Szybkie asserty z czytelnym komunikatem
        if not self.proposer_agents:
            raise RuntimeError("Brak proposer√≥w. Sprawd≈∫ agents_config.json (role bez 'aggregator'/'critic').")
        if not self.aggregator_agent:
            raise RuntimeError("Brak agregatora. Sprawd≈∫ agents_config.json (rola 'Aggregator').")
        if not self.critic_agent:
            raise RuntimeError("Brak krytyka. Sprawd≈∫ agents_config.json (rola 'Critic').")

        max_rounds = len(self.proposer_agents) + 2

        # Bootstrap misji ‚Äì bez 'PLAN_ZATWIERDZONY' w tre≈õci, ≈ºeby manager nie ko≈Ñczy≈Ç po 1 msg
        bootstrap = (
            f"## MISJA\n{self.mission}\n\n"
    "Zaproponuj kompletny PLAN w formacie JSON {entry_point, nodes[], edges[]}.\n"
    "Rola: Proposerzy proponujƒÖ swoje wersje planu. Nastƒôpnie Aggregator scala je w jednƒÖ, sp√≥jnƒÖ propozycjƒô. "
    "Na ko≈Ñcu, Quality_Critic oceni finalny, zagregowany plan."
        )

        self._initial_bootstrap = bootstrap
        
        
        #nowe wczytywanie pamieci
        memory_msg = None
        try:
            memory_msg = self._make_memory_message_once()
            if memory_msg and isinstance(memory_msg, dict) and memory_msg.get("content"):
                process_log("[MEMORY] Seeded memory message into history")
            else:
                process_log("[MEMORY][WARN] No usable memory message produced; seeding skipped")
                memory_msg = None
        except Exception as e:
            process_log(f"[MEMORY][ERROR] Could not build memory message: {type(e).__name__}: {e}")
            memory_msg = None
        
        #koniec 
        
        
        
        
        agents = (
        ([self.memory_analyst_agent] if self.memory_analyst_agent else []) +
        [*self.proposer_agents, self.aggregator_agent, self.critic_agent]
        )
        
        # Uczestnicy ‚Äì tylko agenci
        # agents = [*self.proposer_agents, self.aggregator_agent, self.critic_agent]

        turns_per_iteration = len(self.proposer_agents) + 2 
        max_rounds = self.max_iterations * turns_per_iteration + 5 # Dodajemy bufor bezpiecze≈Ñstwa

        start_messages = []
        if memory_msg:
            start_messages.append(memory_msg)

        
        
        gc = GroupChat(
            agents=agents,
            messages = start_messages,
            max_round=max_rounds, # U≈ºywamy nowej, dynamicznie obliczonej warto≈õci
            speaker_selection_method=self.custom_speaker_selection_logic)
        
        self.groupchat = gc
        
        try:
            for i, m in enumerate(gc.messages[:3]):  # poka≈º do 3 pierwszych
                n, l, tn = self._extract_name_and_len(m)
                process_log(f"[MEMORY][DEBUG] m{i}: type={tn} name={n} len={l}")
        except Exception as e:
            process_log(f"[MEMORY][DEBUG] inspect failed: {type(e).__name__}: {e}")
        
        
        
        manager = GroupChatManager(
            groupchat=gc,
            llm_config=self.aggregator_agent.llm_config,
            human_input_mode="NEVER",
            system_message=MOAPrompts.get_aggregator_prompt(),
            is_termination_msg=self._is_final_plan_message
        )

        try:
            # Start rozmowy ‚Äì to uruchamia ca≈ÇƒÖ maszynkƒô
            self.user_proxy.initiate_chat(manager, message=bootstrap, max_turns=max_rounds)
        except StopIteration:
            # To jest OK - plan zosta≈Ç zatwierdzony
            process_log("[SUCCESS] Debata zako≈Ñczona przez StopIteration - plan zatwierdzony")
        try:
            # Szukamy finalnej odpowiedzi
            final_plan_message_content = None
            messages = manager.groupchat.messages
            for msg in reversed(messages):
                
                content_str = str(msg.get("content", ""))
                # U≈ºywamy Twojej nowej, precyzyjnej funkcji sprawdzajƒÖcej
                if "PLAN_ZATWIERDZONY" in content_str:
                    final_plan_message_content = msg.get("content")
                    break
                    

            # Je≈õli znaleziono zatwierdzonƒÖ wiadomo≈õƒá, sparsuj jƒÖ
            if final_plan_message_content:
                process_log("[SUCCESS] Krytyk zatwierdzi≈Ç plan. Rozpoczynam parsowanie...")
                try:
                    parsed_critic_response = self.parser.parse_critic_response(final_plan_message_content)

                    # TUTAJ WKLEJ NOWY KOD (zamiast linii 67-75):
                    if parsed_critic_response:
                        # Szukaj planu w r√≥≈ºnych mo≈ºliwych miejscach
                        final_plan = None

                        # Lista mo≈ºliwych kluczy
                        possible_keys = [
                            "plan_approved",
                            "final_synthesized_plan", 
                            "final_plan",
                            "synthesized_plan",
                            "approved_plan",
                            "plan"
                        ]

                        for key in possible_keys:
                            if key in parsed_critic_response:
                                candidate = parsed_critic_response[key]
                                # Sprawd≈∫ czy to wyglƒÖda jak plan (ma entry_point i nodes)
                                if isinstance(candidate, dict) and "entry_point" in candidate and "nodes" in candidate:
                                    final_plan = candidate
                                    process_log(f"[SUCCESS] Znaleziono plan pod kluczem: '{key}'")
                                    break

                        if final_plan:
                            self.final_plan = final_plan
                            self._save_successful_plan()
                            
                            
                            
                            # Zbierz stan orchestratora
                            orchestrator_state = {
                                "iteration_count": self.iteration_count,
                                "execution_time": (datetime.now() - start_time).total_seconds() if 'start_time' in locals() else 0,
                                "total_tokens": getattr(self, 'token_counter', 0),
                                "api_calls": getattr(self, 'api_call_counter', 0)
                            }
                            
                            # try:
                                # Je≈õli masz obliczony final_score z jako≈õci planu ‚Äì mo≈ºesz go tutaj podaƒá, inaczej None
                            #     mission_id = save_mission_to_gcs(
                            #         bucket_name="memory_rag_for_agents",
                            #         base_prefix="missions",
                            #         mission=self.mission,
                            #         final_plan=self.final_plan,
                            #         all_messages=manager.groupchat.messages,
                            #         orchestrator_state=orchestrator_state,
                            #         approved=True,
                            #         final_score=None,
                            #     )
                            #     process_log(f"[ORCHESTRATOR] Mission completed and saved as: {mission_id}")
                            # except Exception as err:
                            #     # jedno wywo≈Çanie loguje zar√≥wno nag≈Ç√≥wek, jak i pe≈Çny traceback
                            #     log_exc("[MEMORY:GCS][ERROR] Nie uda≈Ço siƒô zapisaƒá misji", err)
                            
                            if self.memory:
                                # Zapisz KOMPLETNƒÑ misjƒô
                                mission_id = self.memory.save_complete_mission(
                                mission=self.mission,
                                final_plan=self.final_plan,
                                all_messages=manager.groupchat.messages,
                                orchestrator_state=orchestrator_state
                                )

                                process_log(f"[ORCHESTRATOR] Mission completed and saved as: {mission_id}")
    
                                try:
                                    ndjson_uris = export_local_by_filename_date(
                                        input_dir="memory/missions",                # folder z lokalnymi plikami mission_*.json
                                        output_root_gcs="gs://external_memory/missions",
                                        pattern="*.json",
                                    )
                                    process_log(f"[EXPORT] Wyeksportowano {len(ndjson_uris)} misji do GCS")
                                except Exception as e:
                                    # w razie b≈Çƒôdu logujemy pe≈Çen traceback
                                    log_exc("[EXPORT][ERROR] Eksport misji do GCS nie powi√≥d≈Ç siƒô", e)
                            
                            return self.final_plan
                        else:
                            # Je≈õli nie znaleziono planu w ≈ºadnym kluczu
                            raise RuntimeError(f"Nie znaleziono planu w odpowiedzi. Dostƒôpne klucze: {list(parsed_critic_response.keys())}")
                    else:
                        raise RuntimeError("Parser zwr√≥ci≈Ç None - nie uda≈Ço siƒô sparsowaƒá JSON")
            
                except Exception as parse_error:
                    #poprawka
                    log_exc("[ERROR] Nie uda≈Ço siƒô sparsowaƒá odpowiedzi krytyka", parse_error)
                    
                    #koniec poprawki
                    
                    
                    # Sytuacja awaryjna: nie uda≈Ço siƒô sparsowaƒá odpowiedzi krytyka
                    # process_log(f"[ERROR] Nie uda≈Ço siƒô sparsowaƒá odpowiedzi krytyka: {parse_error}")
                    
                    
                    # Zapisz raport z surowƒÖ odpowiedziƒÖ do analizy
                    self._write_failure_report(
                        reason="CRITIC_RESPONSE_PARSE_FAILURE",
                        stage="post-debate_parsing",
                        aggregator_raw=None, # Nieistotne na tym etapie
                        critic_raw=final_plan_message_content,
                        exception=parse_error
                    )
                    
                    return None # Zwracamy None w przypadku b≈Çƒôdu parsowania
                
                
                explainability_report = EXPLAINABILITY.generate_final_report()
                process_log(f"[ORCHESTRATOR] Explainability report generated: {explainability_report['debate_id']}")
            else:
                explainability_report = EXPLAINABILITY.generate_final_report()
                process_log(f"[ORCHESTRATOR] Explainability report generated: {explainability_report['debate_id']}")
                # Je≈õli pƒôtla siƒô zako≈Ñczy≈Ça i nie znaleziono zatwierdzonej wiadomo≈õci
                raise RuntimeError("Debata zako≈Ñczona, ale krytyk nigdy nie zwr√≥ci≈Ç wiadomo≈õci z 'PLAN_ZATWIERDZONY'.")

        except Exception as e:
            # Raport diagnostyczny
            tb = traceback.format_exc()
            os.makedirs("reports", exist_ok=True)
            path = f"reports/failure_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(path, "w", encoding="utf-8") as f:
                json.dump({"error_type": type(e).__name__,
                           "error_message": str(e),
                           "stacktrace": tb}, f, ensure_ascii=False, indent=2)
            process_log(f"[FAILSAFE] Saved failure report: {path}")
            process_log(tb)
            
            explainability_report = EXPLAINABILITY.generate_final_report()
            process_log(f"[ORCHESTRATOR] Explainability report generated: {explainability_report['debate_id']}")
            
            return None
    
   

    
        
    
    def _update_context_from_last_critique(self, critique_message: str):
        """Aktualizuje kontekst na podstawie krytyki"""
        # Parsuj krytykƒô
        if not self.memory:
            return
        
        
        parsed = self.parser.parse_critic_response(critique_message)
        
        if parsed:
            feedback = f"Score: {parsed.get('score', 'N/A')}. "
            feedback += f"Weaknesses: {', '.join(parsed.get('weaknesses', []))}. "
            feedback += f"Improvements: {', '.join(parsed.get('improvements', []))}"
            
            self.current_context['last_feedback'] = feedback
            
            # Zapisz do pamiƒôci
            self.memory.add_iteration_feedback(
                iteration=self.iteration_count,
                feedback=feedback,
                timestamp=datetime.now()
            )
        
        # Od≈õwie≈º kontekst z pamiƒôci
        self.current_context = self.memory.get_relevant_context(self.mission)
        
        process_log(f"Context updated for iteration {self.iteration_count}")
    
    def _extract_final_plan(self, messages: List[Dict]):
        """Wyodrƒôbnia zatwierdzony plan z historii wiadomo≈õci"""
        # Szukaj od ko≈Ñca
        for msg in reversed(messages):
            content = msg.get("content", "")
            name = msg.get("name", "")
            
            # Je≈õli krytyk zatwierdzi≈Ç
            if name == "Quality_Critic" and "PLAN_ZATWIERDZONY" in content:
                # Znajd≈∫ ostatni plan od agregatora
                for prev_msg in reversed(messages):
                    if prev_msg.get("name") == "Master_Aggregator":
                        parsed = self.parser.parse_agent_response(prev_msg.get("content", ""))
                        if parsed:
                            self.final_plan = parsed.get("final_plan", parsed.get("plan"))
                            break
                break
        
        process_log(f"Final plan extracted: {self.final_plan is not None}")
    
    def _save_successful_plan(self):
        """Zapisuje udany plan do pamiƒôci i pliku"""
        if not self.final_plan:
            return
        if self.memory:
           
        # Zapisz do pamiƒôci
            self.memory.add_successful_plan(
                plan=self.final_plan,
                mission=self.mission,
                metadata={
                    'iterations': self.iteration_count,
                    'agents_count': len(self.proposer_agents)
                }
            )
        
        # Zapisz do pliku
        output = {
            "mission": self.mission,
            "final_plan": self.final_plan,
            "metadata": {
                "iterations": self.iteration_count,
                "timestamp": datetime.now().isoformat(),
                "autogen_debate": True
            }
        }
        
        os.makedirs("outputs", exist_ok=True)
        output_file = f"outputs/autogen_plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Plan saved to: {output_file}")
        process_log(f"Successful plan saved to {output_file}")
        
        
    def _debug_dump_transcript(self, groupchat, tail: int = 30):
        """Wypisz ostatnie ~N wiadomo≈õci debaty, ≈ºeby by≈Ço je widaƒá w notebooku."""
        from process_logger import log as process_log
        try:
            msgs = getattr(groupchat, "messages", [])[-tail:]
            process_log("----- TRANSCRIPT (tail) -----")
            for m in msgs:
                role = m.get("role") or m.get("name") or "?"
                name = m.get("name") or ""
                content = m.get("content") or ""
                head = (content[:400] + "...") if len(content) > 400 else content
                process_log(f"{role} {name}: {head}")
            process_log("----- END TRANSCRIPT -----")
        except Exception as e:
            process_log(f"[TRANSCRIPT_DUMP_FAIL] {type(e).__name__}: {e}")


--- FILE: autogen_vertex_mcp_system_claude.py ---

# autogen_vertex_mcp_system.py
"""
AutoGen Multi-Agent System with Vertex AI Search as MCP Tool
Agents learn from mission memory without prompt injection
"""

import asyncio
import json
import logging
from typing import Any, Dict, List, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime

import autogen
from autogen import AssistantAgent, UserProxyAgent
from google.cloud import discoveryengine_v1 as de
from google.cloud import storage
from google.api_core.client_options import ClientOptions

# ======================== Configuration ========================

TOOL_DEFINITIONS = [
    {
        "type": "function",
        "function": {
            "name": "search_mission_memory",
            "description": "Search the Vertex AI mission memory for past missions based on a query.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The search query (e.g., 'CSV pipeline error handling').",
                    },
                    "filters": {
                        "type": "string",
                        "description": "Optional filters (e.g., 'status:SUCCESS').",
                    },
                    "top_k": {
                        "type": "integer",
                        "description": "Number of results to return (default 5).",
                    },
                },
                "required": ["query"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_mission_plan",
            "description": "Retrieve the complete execution plan (nodes and edges) for a specific mission ID.",
            "parameters": {
                "type": "object",
                "properties": {
                    "mission_id": {
                        "type": "string",
                        "description": "The unique ID of the mission.",
                    }
                },
                "required": ["mission_id"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "analyze_patterns",
            "description": "Analyze successful patterns across high-scoring missions.",
            "parameters": {
                "type": "object",
                "properties": {
                    "min_score": {
                        "type": "number",
                        "description": "Minimum score threshold (default 90.0).",
                    }
                },
                "required": [],
            },
        },
    },
]


@dataclass
class SystemConfig:
    """Central configuration"""

    vertex_search_config: str = (
        "projects/815755318672/locations/us/collections/default_collection/dataStores/external-memory-connector_1756845276280_gcs_store/servingConfigs/default_config"
    )
    vertex_location: str = "us"

    # AutoGen config - mo≈ºesz zmieniƒá na Gemini je≈õli masz setup
    autogen_config: Dict[str, Any] = field(
        default_factory=lambda: {
            "config_list": [
                {
                    "model": "gpt-4-turbo-preview",  # lub "gemini-1.5-pro" je≈õli u≈ºywasz Vertex AI
                    "api_key": "your-api-key",  # lub vertex ai credentials
                    "temperature": 0.7,
                }
            ],
            "timeout": 120,
            "cache_seed": 42,
        }
    )

    def get_llm_config_with_tools(self) -> Dict[str, Any]:
        config = self.autogen_config.copy()
        # Wstrzykujemy definicje narzƒôdzi tutaj (u≈ºyj "tools" dla nowoczesnych API)
        config["tools"] = TOOL_DEFINITIONS
        return config

    max_search_results: int = 5
    enable_learning: bool = True


# ======================== Vertex AI Search Tool ========================


class VertexSearchTool:
    """Vertex AI Search as a tool for AutoGen agents"""

    def __init__(self, config: SystemConfig):
        self.config = config
        self.storage_client = storage.Client()
        self.search_client = de.SearchServiceClient(
            client_options=ClientOptions(
                api_endpoint=f"{config.vertex_location}-discoveryengine.googleapis.com"
            )
        )
        self._cache = {}

    def search_mission_memory(
        self, query: str, filters: Optional[str] = None, top_k: int = 5
    ) -> str:
        """
        Search mission memories. Returns JSON string.
        This is the function that AutoGen agents will call.
        """
        try:
            # Check cache
            cache_key = f"{query}_{filters}_{top_k}"
            if cache_key in self._cache:
                return json.dumps(self._cache[cache_key])

            # Build request
            req = de.SearchRequest(
                serving_config=self.config.vertex_search_config,
                query=query,
                page_size=min(top_k, self.config.max_search_results),
                filter=filters if filters else None,
            )

            # Execute search
            results = []
            for r in self.search_client.search(request=req):
                doc = r.document
                sdata = doc.struct_data or {}

                results.append(
                    {
                        "mission_id": sdata.get("mission_id", ""),
                        "score": sdata.get("final_score", 0),
                        "approved": sdata.get("approved", False),
                        "nodes_count": sdata.get("nodes_count", 0),
                        "edges_count": sdata.get("edges_count", 0),
                        "has_optimization": sdata.get("has_optimization", False),
                        "has_rollback": sdata.get("has_rollback", False),
                        "has_retry": sdata.get("has_retry", False),
                        # KLUCZOWA POPRAWKA: Konwertujemy RepeatedComposite na listƒô Pythona
                        "tags": list(sdata.get("tags", [])),
                        # KLUCZOWA POPRAWKA: Konwertujemy Struct/Map na s≈Çownik Pythona
                        "links": dict(sdata.get("links", {})),
                    }
                )

            response = {
                "status": "success",
                "query": query,
                "results": results,
                "count": len(results),
            }

            # Cache result
            self._cache[cache_key] = response

            return json.dumps(response, ensure_ascii=False)

        except Exception as e:
            error_response = {
                "status": "error",
                "error": str(e),
                "query": query,
                "results": [],
            }
            return json.dumps(error_response)

    def get_mission_plan(self, mission_id: str) -> str:
        """
        Get complete plan for a mission. Returns JSON string.
        """
        try:
            # First find the mission
            search_result = self.search_mission_memory(
                query=f"mission_id:{mission_id}", top_k=1
            )
            search_data = json.loads(search_result)

            if not search_data.get("results"):
                return json.dumps({"status": "not_found", "mission_id": mission_id})

            links = search_data["results"][0].get("links", {})
            plan_uri = links.get("plan_uri")

            if not plan_uri or not plan_uri.startswith("gs://"):
                return json.dumps({"status": "no_plan", "mission_id": mission_id})

            # Fetch from GCS
            bucket_name, _, path = plan_uri[5:].partition("/")
            blob = self.storage_client.bucket(bucket_name).blob(path)
            plan_content = blob.download_as_text(encoding="utf-8")
            plan_data = json.loads(plan_content)

            return json.dumps(
                {"status": "success", "mission_id": mission_id, "plan": plan_data}
            )

        except Exception as e:
            return json.dumps(
                {"status": "error", "error": str(e), "mission_id": mission_id}
            )

    def analyze_patterns(self, min_score: float = 90.0) -> str:
        """
        Analyze successful patterns. Returns JSON string.
        """
        try:
            # Search for high-scoring approved missions
            results_json = self.search_mission_memory(
                query="approved:true", filters=f"final_score >= {min_score}", top_k=20
            )
            results_data = json.loads(results_json)

            if not results_data.get("results"):
                return json.dumps({"status": "no_data", "patterns": {}})

            results = results_data["results"]
            total = len(results)

            patterns = {
                "optimization_rate": sum(
                    1 for r in results if r.get("has_optimization")
                )
                / total,
                "rollback_rate": sum(1 for r in results if r.get("has_rollback"))
                / total,
                "retry_rate": sum(1 for r in results if r.get("has_retry")) / total,
                "avg_nodes": sum(r.get("nodes_count", 0) for r in results) / total,
                "avg_edges": sum(r.get("edges_count", 0) for r in results) / total,
                "avg_score": sum(r.get("score", 0) for r in results) / total,
                "total_analyzed": total,
            }

            # Find common tags
            tag_counts = {}
            for r in results:
                for tag in r.get("tags", []):
                    tag_counts[tag] = tag_counts.get(tag, 0) + 1

            patterns["common_tags"] = sorted(
                tag_counts.items(), key=lambda x: x[1], reverse=True
            )[:5]

            return json.dumps({"status": "success", "patterns": patterns})

        except Exception as e:
            return json.dumps({"status": "error", "error": str(e), "patterns": {}})


# ======================== AutoGen Agents Setup ========================


class MissionPlanningTeam:
    """AutoGen multi-agent team for mission planning"""

    def __init__(self, config: SystemConfig):
        self.config = config
        self.vertex_tool = VertexSearchTool(config)
        self.setup_agents()

    def setup_agents(self):
        """Setup AutoGen agents with tools"""

        llm_config_with_tools = self.config.get_llm_config_with_tools()

        tool_implementation_map = {
            "search_mission_memory": self.vertex_tool.search_mission_memory,
            "get_mission_plan": self.vertex_tool.get_mission_plan,
            "analyze_patterns": self.vertex_tool.analyze_patterns,
        }

        # Memory Analyst Agent - analizuje pamiƒôƒá misji
        self.memory_analyst = AssistantAgent(
            name="MemoryAnalyst",
            system_message="""You are a memory analyst. Your ONLY function is to retrieve data using tools.
            You MUST immediately call a tool. 
            Do NOT respond with text, explanations, thought processes, or acknowledgments. 
            
            Instructions:
            1. Call 'search_mission_memory' immediately.
            
            RESPOND ONLY WITH A TOOL CALL.
            """,
            llm_config=llm_config_with_tools,
        )

        # Graph Designer Agent - projektuje grafy wykonania
        self.graph_designer = AssistantAgent(
            name="GraphDesigner",
            system_message="""You are a graph execution plan designer. Your role is to:
            1. Design execution graphs with nodes and edges
            2. Incorporate patterns learned from successful missions
            3. Ensure robustness with error handling, rollback, and optimization
            
            Create plans in this JSON format:
            {
                "entry_point": "StartNode",
                "nodes": [
                    {"name": "NodeName", "implementation": "function", "params": {}}
                ],
                "edges": [
                    {"from": "Node1", "to": "Node2", "condition": "on_success"}
                ]
            }
            
            Use insights from MemoryAnalyst to improve your designs.""",
            llm_config=self.config.autogen_config,
        )

        # Quality Critic Agent - ocenia i ulepsza plany
        self.quality_critic = AssistantAgent(
            name="QualityCritic",
            system_message="""You are a quality critic. Your role is to:
            1. Evaluate proposed plans against historical success patterns
            2. Identify missing robustness features (rollback, retry, optimization)
            3. Suggest improvements based on data from mission memory
            4. Calculate confidence scores
            
            Be constructive but critical. Use data to support your assessments.""",
            llm_config=llm_config_with_tools,
        )

        self.executor = UserProxyAgent(
            name="Executor",
            system_message="Execute the tools requested by agents and report the results verbatim.",
            # W≈ÅƒÑCZAMY wykonanie kodu (wymagane do uruchomienia funkcji Python)
            code_execution_config={
                "use_docker": False,
                "work_dir": "tool_execution_logs",
            },
            human_input_mode="NEVER",
            # Rejestrujemy implementacje funkcji TUTAJ
            function_map=tool_implementation_map,
            llm_config=False,  # Wykonawca nie potrzebuje LLM do my≈õlenia
        )

    async def create_mission_plan(self, mission_prompt: str) -> Dict[str, Any]:
        """
        Create a mission plan using the multi-agent team
        """
        logging.info(f"Starting mission planning for: {mission_prompt}")

        # Create group chat
        groupchat = autogen.GroupChat(
            agents=[
                self.executor,  # U≈ºywamy Executora zamiast Coordinatora
                self.memory_analyst,
                self.graph_designer,
                self.quality_critic,
            ],
            messages=[],
            max_round=15,
            speaker_selection_method="auto",
        )

        manager = autogen.GroupChatManager(
            groupchat=groupchat, llm_config=self.config.autogen_config
        )

        # Start the planning process
        initial_message = f"""
        Create an execution plan for this mission: {mission_prompt}
        
        Process:
        1. MemoryAnalyst: Search for similar successful missions
        2. MemoryAnalyst: Get FULL DETAILS (complete plans) of top 2-3 similar missions
        3. MemoryAnalyst: Analyze the exact structure - what nodes, edges, conditions they use
        4. GraphDesigner: Create a plan based on ACTUAL successful plan structures
        5. QualityCritic: Compare new plan with successful ones and suggest improvements
        
        MemoryAnalyst, start by finding similar missions and then GET THEIR COMPLETE PLANS.
        """

        # Initiate chat
        await self.executor.a_initiate_chat(
            manager, message=initial_message, clear_history=True
        )

        # Extract the final plan from conversation
        final_plan = self._extract_plan_from_messages(groupchat.messages)

        # Get learning context
        learning_context = self._extract_learning_context(groupchat.messages)

        return {
            "mission_prompt": mission_prompt,
            "final_plan": final_plan,
            "learning_context": learning_context,
            "conversation_rounds": len(groupchat.messages),
            "timestamp": datetime.now().isoformat(),
        }

    def _extract_plan_from_messages(self, messages: List[Dict]) -> Dict[str, Any]:
        """Extract the final plan from agent messages"""
        # Look for JSON plans in reverse order (latest first)
        for msg in reversed(messages):
            if msg.get("name") == "GraphDesigner":
                content = msg.get("content", "")
                try:
                    # Try to find JSON in the content
                    import re

                    json_match = re.search(
                        r'\{.*"entry_point".*"nodes".*"edges".*\}', content, re.DOTALL
                    )
                    if json_match:
                        return json.loads(json_match.group())
                except:
                    continue

        # Return empty plan if not found
        return {"entry_point": "", "nodes": [], "edges": []}

    def _extract_learning_context(self, messages: List[Dict]) -> Dict[str, Any]:
        """Extract what was learned from memory"""
        context = {
            "searched_queries": [],
            "analyzed_missions": [],
            "identified_patterns": {},
            "applied_improvements": [],
        }

        for msg in messages:
            if msg.get("name") == "MemoryAnalyst":
                content = msg.get("content", "")
                # Extract search queries and results
                if "search_mission_memory" in content:
                    context["searched_queries"].append(content[:100])
                if "mission_id" in content:
                    # Extract mission IDs mentioned
                    import re

                    ids = re.findall(r"mission_\w+", content)
                    context["analyzed_missions"].extend(ids[:5])

            elif msg.get("name") == "QualityCritic":
                if "improvement" in msg.get("content", "").lower():
                    context["applied_improvements"].append(msg.get("content", "")[:200])

        return context


# ======================== Main Execution ========================


class MissionExecutor:
    """Main system orchestrator"""

    def __init__(self, config: Optional[SystemConfig] = None):
        self.config = config or SystemConfig()
        self.team = MissionPlanningTeam(self.config)
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )

    async def execute_mission(self, mission_prompt: str) -> Dict[str, Any]:
        """Execute a mission with learning from memory"""

        logging.info("=" * 60)
        logging.info(f"MISSION: {mission_prompt}")
        logging.info("=" * 60)

        # Create plan using multi-agent team
        result = await self.team.create_mission_plan(mission_prompt)

        # Log what was learned
        if result.get("learning_context", {}).get("analyzed_missions"):
            logging.info(
                f"Learned from missions: {result['learning_context']['analyzed_missions']}"
            )

        logging.info(
            f"Plan created with {len(result['final_plan'].get('nodes', []))} nodes"
        )

        return result

    def run(self, mission_prompt: str) -> Dict[str, Any]:
        """Synchronous wrapper for execute_mission"""
        return asyncio.run(self.execute_mission(mission_prompt))


# ======================== Usage Example ========================



--- FILE: config_api.py ---

import os
import logging
from enum import Enum
from google.cloud import secretmanager
import langchain
from langchain.cache import SQLiteCache


def get_secret(project_id: str, secret_id: str, version_id: str = "latest") -> str:
    """Pobiera warto≈õƒá sekretu z Google Secret Manager."""
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"
    response = client.access_secret_version(request={"name": name})

    return response.payload.data.decode("UTF-8")


class ApiType(Enum):
    GOOGLE = "google"
    ANTHROPIC = "anthropic"
    OPENAI = "openai"

    def __str__(self):
        return self.value


LOCATION = "us-central1"
PROJECT_ID = "dark-data-discovery"

# ---------AGENTS--------:
MAIN_AGENT = "gemini-2.5-pro"
API_TYPE_GEMINI = str(ApiType.GOOGLE)

CRITIC_MODEL = "claude-3-7-sonnet-20250219"
ARCHITECT_MODEL = "claude-opus-4-1-20250805"
CODE_MODEL = "claude-sonnet-4-20250514"
QUICK_SMART_MODEL = "gemini-2.5-flash"

GPT_MODEL = "gpt-4o"  # U≈ºywamy gpt-4o jako odpowiednika "gpt-5"
API_TYPE_OPENAI = str(ApiType.OPENAI)

API_TYPE_SONNET = str(ApiType.ANTHROPIC)

LANGCHAIN_API_KEY = get_secret(PROJECT_ID, "LANGCHAIN_API_KEY")
ANTHROPIC_API_KEY = get_secret(PROJECT_ID, "ANTHROPIC_API_KEY")
TAVILY_API_KEY = get_secret(PROJECT_ID, "TAVILY_API_KEY")
OPENAI_API_KEY = get_secret(PROJECT_ID, "OPENAI_API_KEY")

MEMORY_ENGINE_DISPLAY_NAME = "memory-gamma-way"

INPUT_FILE_PATH = "gs://super_model/data/structural_data/synthetic_fraud_dataset.csv"

MAX_CORRECTION_ATTEMPTS = 5


os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_PROJECT"] = "Projekt Multi-Agent-System Dynamic-graphs"
os.environ["ANTHROPIC_API_KEY"] = ANTHROPIC_API_KEY
os.environ["TAVILY_API_KEY"] = TAVILY_API_KEY
# os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

os.environ.setdefault("MOA_SANITY_PING", "0")
# ---cache-------
langchain.llm_cache = SQLiteCache(database_path=".langchain.db")


# FUNKCJA KONFIGURACYJNA AGENTOW AUTOGEN
def basic_config_agent(
    agent_name: str,
    api_type: str,
    location: str = None,
    project_id: str = None,
    api_key: str = None,
):
    try:
        configuration = {"model": agent_name}
        configuration.update({"api_type": api_type})
        if api_key:
            configuration["api_key"] = api_key
        if project_id:
            configuration["project_id"] = project_id
        if location:
            configuration["location"] = location

        logging.info(f"Model configuration: {configuration}")
        return [configuration]

    except Exception as e:
        logging.error(f"Failed to initialize Vertex AI or configure LLM: {e}")
        print(
            f"Error: Failed to initialize Vertex AI or configure LLM. Please check your project ID, region, and permissions. Details: {e}"
        )
        exit()



--- FILE: explainability_layer.py ---

import json
import re
from datetime import datetime
from typing import Any, Dict, List, Optional

from process_logger import log as process_log


class ExplainabilityLayer:
    """
    Dodaje wyja≈õnialno≈õƒá do istniejƒÖcego systemu MOA bez wiƒôkszych zmian
    """

    @staticmethod
    def inject_explainability_prompt(base_prompt: str, agent_role: str) -> str:
        """
        Dodaje instrukcje wyja≈õnialno≈õci do promptu - MODEL PRZEZ API TO ZROZUMIE
        """

        explainability_suffix = """

## EXPLAINABILITY REQUIREMENTS (MANDATORY)
You must include an additional JSON field "cognitive_trace" in your response with:

```json
"cognitive_trace": {
    "trigger_words": [/* actual words from prompt that influenced you */],
    "reasoning_chain": [/* your actual reasoning steps */],
    "confidence_per_decision": {
        /* your actual confidence values between 0-1 for each decision type */
        "node_selection": /* your confidence */,
        "edge_conditions": /* your confidence */,
        "overall_structure": /* your confidence */
    },
    "alternatives_considered": {
        "rejected_nodes": [/* nodes you considered but rejected with reasons */],
        "rejected_patterns": [/* patterns you didn't use and why */]
    },
    "key_influences": {
        "from_prompt": /* what specific instruction shaped this most */,
        "from_context": /* what contextual factor was crucial */,
        "from_role": /* how your assigned role affected this */
    },
    "uncertainty_points": [/* where you're least certain and why */],
    "word_choices": {
        /* actual words you chose and why */
    }
}
This is REQUIRED - include it after your main plan/response.
"""
        return base_prompt + explainability_suffix

    @staticmethod
    def extract_cognitive_trace(response: str) -> Optional[Dict]:
        """
        WyciƒÖga cognitive_trace z odpowiedzi modelu
        """
        try:
            # Szukaj cognitive_trace w odpowiedzi
            if isinstance(response, dict):
                return response.get("cognitive_trace")

            # Je≈õli string, parsuj JSON
            if isinstance(response, str):
                # Usu≈Ñ markdown je≈õli jest
                cleaned = re.sub(r"```json?\s*|\s*```", "", response)
                parsed = json.loads(cleaned)
                return parsed.get("cognitive_trace")
        except:
            # Fallback - spr√≥buj znale≈∫ƒá wzorzec
            pattern = r'"cognitive_trace":\s*\{[^}]+\}'
            match = re.search(pattern, str(response))
            if match:
                try:
                    return json.loads("{" + match.group() + "}")["cognitive_trace"]
                except:
                    pass
            return None

    @staticmethod
    def analyze_response_semantics(prompt: str, response: str, agent_name: str) -> Dict:
        """
        Analizuje zwiƒÖzki semantyczne miƒôdzy promptem a odpowiedziƒÖ
        """
        analysis = {
            "agent": agent_name,
            "timestamp": datetime.now().isoformat(),
            "prompt_length": len(prompt),
            "response_length": len(response),
            "semantic_markers": {},
        }

        # Kluczowe s≈Çowa z promptu kt√≥re pojawi≈Çy siƒô w odpowiedzi
        prompt_keywords = set(re.findall(r"\b[A-Za-z_]+\b", prompt.lower()))
        response_keywords = set(re.findall(r"\b[A-Za-z_]+\b", response.lower()))

        analysis["semantic_markers"]["keyword_overlap"] = list(prompt_keywords & response_keywords)[
            :20
        ]
        analysis["semantic_markers"]["new_concepts"] = list(response_keywords - prompt_keywords)[
            :20
        ]

        # Wykryj wzorce decyzyjne
        if "error" in response.lower():
            analysis["semantic_markers"]["error_handling_focus"] = True
        if "rollback" in response.lower():
            analysis["semantic_markers"]["rollback_strategy"] = True
        if "optimiz" in response.lower():
            analysis["semantic_markers"]["optimization_focus"] = True

        return analysis

    @staticmethod
    def create_explainability_report(all_analyses: List[Dict]) -> Dict:
        """
        Tworzy raport wyja≈õnialno≈õci dla ca≈Çej debaty
        """
        report = {
            "debate_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "total_turns": len(all_analyses),
            "agents_involved": list(set(a["agent"] for a in all_analyses if "agent" in a)),
            "cognitive_patterns": {},
            "decision_evolution": [],
            "key_influences": {},
            "uncertainty_map": {},
        }

        # Agreguj cognitive traces
        for analysis in all_analyses:
            if "cognitive_trace" in analysis:
                trace = analysis["cognitive_trace"]
                agent = analysis.get("agent", "unknown")

                if agent not in report["cognitive_patterns"]:
                    report["cognitive_patterns"][agent] = []

                report["cognitive_patterns"][agent].append(
                    {
                        "triggers": trace.get("trigger_words", []),
                        "confidence": trace.get("confidence_per_decision", {}),
                        "alternatives": trace.get("alternatives_considered", {}),
                    }
                )

                # Mapuj niepewno≈õci
                for uncertainty in trace.get("uncertainty_points", []):
                    if uncertainty not in report["uncertainty_map"]:
                        report["uncertainty_map"][uncertainty] = []
                    report["uncertainty_map"][uncertainty].append(agent)

        return report


class ExplainabilityHooks:
    """
    Hooki do wstrzykniƒôcia w istniejƒÖcy kod z minimalnƒÖ ingerencjƒÖ
    """

    def __init__(self):
        self.layer = ExplainabilityLayer()
        self.session_analyses = []

    def on_prompt_build(self, base_prompt: str, agent_role: str) -> str:
        """
        Hook wywo≈Çywany przy budowaniu promptu - DODAJ TO DO TWOJEJ METODY BUDOWANIA PROMPT√ìW
        """
        return self.layer.inject_explainability_prompt(base_prompt, agent_role)

    def on_response_received(self, prompt: str, response: str, agent_name: str) -> Dict:
        """
        Hook wywo≈Çywany po otrzymaniu odpowiedzi - DODAJ TO PO OTRZYMANIU ODPOWIEDZI Z API
        """
        # Ekstraktuj cognitive trace
        cognitive_trace = self.layer.extract_cognitive_trace(response)

        # Analiza semantyczna
        semantic_analysis = self.layer.analyze_response_semantics(prompt, response, agent_name)

        # Po≈ÇƒÖcz analizy
        full_analysis = {
            **semantic_analysis,
            "cognitive_trace": cognitive_trace,
            "raw_response_sample": (
                response[:500] if isinstance(response, str) else str(response)[:500]
            ),
        }

        # Zapisz do sesji
        self.session_analyses.append(full_analysis)

        # Loguj kluczowe informacje
        if cognitive_trace:
            process_log(
                f"[EXPLAINABILITY] {agent_name} confidence: {cognitive_trace.get('confidence_per_decision', {})}"
            )
            process_log(
                f"[EXPLAINABILITY] Key triggers: {cognitive_trace.get('trigger_words', [])[:5]}"
            )

        return full_analysis

    def generate_final_report(self) -> Dict:
        """
        Generuje ko≈Ñcowy raport wyja≈õnialno≈õci
        """
        report = self.layer.create_explainability_report(self.session_analyses)

        # Zapisz do pliku
        report_file = f"explainability_reports/report_{report['debate_id']}.json"
        import os

        os.makedirs("explainability_reports", exist_ok=True)

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        process_log(f"[EXPLAINABILITY] Report saved to: {report_file}")
        return report



--- FILE: exporter_missions_gcs.py ---

# exporter_missions_gcs.py
from __future__ import annotations
import json, os, glob
from pathlib import Path
from typing import List, Tuple
from datetime import datetime

from google.cloud import storage
from datetime import datetime, timezone
# Re-use logiki z Twojej biblioteki (identyczne wyliczenia i pola!)
from exporter_missions_lib import (
    _to_str_content,
    _extract_plan,
    _build_txt,
    _build_transcript,
    _build_metrics,
    _ndjson_line,
)

def _parse_gs_uri(uri: str) -> Tuple[str, str]:
    if not uri.startswith("gs://"):
        raise ValueError(f"output_root_gcs must start with gs://, got: {uri}")
    rest = uri[5:]
    bucket, _, prefix = rest.partition("/")
    return bucket, prefix.strip("/")

def _mission_id_from_snapshot(snap: dict, fallback_path: Path) -> str:
    mid = snap.get("memory_id") or snap.get("mission_id")
    if isinstance(mid, str) and mid:
        return mid
    base = fallback_path.stem
    if base.lower().startswith("mission_"):
        return base
    # ostateczny fallback
    stamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    return f"mission_{stamp}"

def export_local_by_filename_date(
    input_dir: str,
    output_root_gcs: str,
    pattern: str = "*.json",
    skip_existing: bool = True,
) -> List[str]:
    """
    Eksportuje lokalne misje do GCS w formacie zgodnym z exporter_missions_lib:
      missions/<mission_id>/<mission_id>.txt
      missions/<mission_id>/<mission_id>.plan.json
      missions/<mission_id>/<mission_id>.transcript.json
      missions/<mission_id>/<mission_id>.metrics.json
      missions/<mission_id>/<mission_id>.ndjson   (1 linia na misjƒô)

    Zwraca listƒô GCS URI do *.ndjson (po jednym na misjƒô).
    """
    input_dir_p = Path(input_dir).resolve()
    files = sorted(
        glob.glob(os.path.join(str(input_dir_p), pattern)),
        key=lambda p: os.path.basename(p).split("_")[1:3]  # sort: YYYYMMDD, HHMMSS
    )

    bucket_name, root_prefix = _parse_gs_uri(output_root_gcs)
    client = storage.Client()
    bucket = client.bucket(bucket_name)

    ndjson_uris: List[str] = []

    for p in files:
        src = Path(p)
        try:
            snap = json.loads(src.read_text(encoding="utf-8"))
        except Exception as e:
            print(f"[EXPORT][SKIP] {src.name}: cannot load JSON ({e})")
            continue

        mission_id = _mission_id_from_snapshot(snap, src)
        plan = _extract_plan(snap)
        txt = _build_txt(snap, plan)
        transcript = _build_transcript(snap)
        metrics = _build_metrics(snap, plan)

        # ≈öcie≈ºki w GCS ‚Äì jak w Twojej bibliotece (katalog per misja)
        base_prefix = f"{root_prefix}/{mission_id}".strip("/")
        txt_path     = f"{base_prefix}/{mission_id}.txt"
        plan_path    = f"{base_prefix}/{mission_id}.plan.json"
        trans_path   = f"{base_prefix}/{mission_id}.transcript.json"
        metrics_path = f"{base_prefix}/{mission_id}.metrics.json"
        ndjson_path  = f"{base_prefix}/{mission_id}.ndjson"

        txt_uri     = f"gs://{bucket_name}/{txt_path}"
        plan_uri    = f"gs://{bucket_name}/{plan_path}"
        trans_uri   = f"gs://{bucket_name}/{trans_path}"
        metrics_uri = f"gs://{bucket_name}/{metrics_path}"
        ndjson_uri  = f"gs://{bucket_name}/{ndjson_path}"

        
        if skip_existing:
            
            metrics_blob = bucket.blob(metrics_path)
            ndjson_blob  = bucket.blob(ndjson_path)
            if metrics_blob.exists(client) or ndjson_blob.exists(client):
                print(f"[EXPORT][SKIP] {mission_id} ju≈º istnieje w GCS (metrics/ndjson).")
                continue
        
        
        # 1) Upload artefakt√≥w
        bucket.blob(txt_path).upload_from_string(txt, content_type="text/plain; charset=utf-8")
        bucket.blob(plan_path).upload_from_string(
            json.dumps(plan, ensure_ascii=False, indent=2),
            content_type="application/json; charset=utf-8",
        )
        bucket.blob(trans_path).upload_from_string(
            json.dumps(transcript, ensure_ascii=False, indent=2),
            content_type="application/json; charset=utf-8",
        )
        bucket.blob(metrics_path).upload_from_string(
            json.dumps(metrics, ensure_ascii=False, indent=2),
            content_type="application/json; charset=utf-8",
        )

        # 2) Zbuduj liniƒô NDJSON (identyczna z TwojƒÖ funkcjƒÖ _ndjson_line)
        ndjson_line = _ndjson_line(
            mission_id=mission_id,
            txt_uri=txt_uri,
            plan_uri=plan_uri,
            transcript_uri=trans_uri,
            metrics_uri=metrics_uri,
            metrics=metrics,
        )

        # 3) Upload pojedynczego pliku NDJSON dla misji
        bucket.blob(ndjson_path).upload_from_string(
            ndjson_line + "\n",
            content_type="application/x-ndjson; charset=utf-8",
        )
        
        
        #zapisywanie indeksow
        
        
         # helper do slug√≥w (zachowuje PL znaki, ogranicza d≈Çugo≈õƒá)
        def _slug_u(text: str) -> str:
            t = (text or "").strip().lower()
            t = re.sub(r"\s+", "-", t)                         # spacje -> '-'
            t = re.sub(r"[^0-9A-Za-zƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ªƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º\-]+", "-", t)  # tylko sensowne znaki
            t = re.sub(r"-{2,}", "-", t).strip("-")
            return t[:120]

        # 1) Timestamp i identyfikatory
        ts_dt   = datetime.now(timezone.utc)                   # je≈õli wolisz bez timezone: datetime.utcnow()
        ts_file = ts_dt.strftime("%Y%m%d_%H%M%S")              # do nazwy pliku
        ts_iso  = ts_dt.strftime("%Y-%m-%dT%H:%M:%SZ")         # do pola timestamp (ISO8601Z)

        # mission_id pe≈Çny vs skr√≥cony (jak w Twoim za≈ÇƒÖczniku)
        mid_full  = mission_id                                 # np. "mission_20250829_212413_92ed8ebc"
        mid_short = mission_id.replace("mission_", "")         # np. "20250829_212413_92ed8ebc"
        tail8     = mid_short[-8:] if len(mid_short) >= 8 else mid_short

        
        
        # --- SAFE BINDINGS: u≈ºywaj *_val zamiast go≈Çych nazw ---
        _locals = locals()
        _md = _locals.get("metadata") if isinstance(_locals.get("metadata"), dict) else {}

        approved_val     = _locals.get("approved", _md.get("approved", True))
        final_score_val  = _locals.get("final_score", _md.get("final_score"))
        nodes_count_val  = _locals.get("nodes_count", _md.get("nodes_count"))
        edges_count_val  = _locals.get("edges_count", _md.get("edges_count"))
        mission_type_val = _locals.get("mission_type", _md.get("mission_type", "general"))
        lang_val         = _locals.get("lang", _md.get("lang", "pl"))

        # tags jako lista
        _tags = _locals.get("tags", _md.get("tags", []))
        tags_list = list(_tags) if isinstance(_tags, (list, tuple)) else ([] if _tags is None else [str(_tags)])

        # flags jako dict + fallback z metadanych (has_* bywa trzymane p≈Çasko)
        flags_val = _locals.get("flags", {})
        if not isinstance(flags_val, dict):
            flags_val = {}
        flags_val = {
            "has_retry":        bool(flags_val.get("has_retry",        _md.get("has_retry", False))),
            "has_rollback":     bool(flags_val.get("has_rollback",     _md.get("has_rollback", False))),
            "has_optimization": bool(flags_val.get("has_optimization", _md.get("has_optimization", False))),
        }
        
        
        
        
        
        
        # 2) ≈πr√≥d≈Ço tytu≈Çu do display_id z metadanych (BEZ u≈ºycia 'mission')
        _display_src = ""
        try:
            if isinstance(metadata, dict):
                _display_src = (
                    metadata.get("mission_prompt")
                    or metadata.get("mission")
                    or metadata.get("title")
                    or ""
                )
        except NameError:
            _display_src = ""

        display_base = _slug_u(_display_src) if isinstance(_display_src, str) and _display_src.strip() else ""
        display_id   = f"{ts_file}-{display_base}-{tail8}" if display_base else f"{ts_file}-{tail8}"

        # 3) Lekki .txt jako content.uri (tak jak w za≈ÇƒÖczniku)
        txt_name = f"{display_id}.txt"
        # UWAGA: 'root_prefix' to katalog dnia (ten sam, w kt√≥rym lƒÖdujƒÖ artefakty tej misji)
        txt_path = f"{root_prefix}/{txt_name}"
        
        txt_body = (
        f"mission_id: {mid_full}\n"
        f"timestamp:  {ts_iso}\n"
        f"approved:   {bool(approved_val)}\n"
        f"final_score:{final_score_val if final_score_val is not None else 'null'}\n"
        f"tags:       {', '.join(tags_list) if tags_list else ''}\n"
    )
        
        
        
        bucket.blob(txt_path).upload_from_string(txt_body, content_type="text/plain; charset=utf-8")
        txt_uri = f"gs://{bucket_name}/{txt_path}"

        # 4) Sk≈Çadamy dokument NDJSON 1:1 jak w za≈ÇƒÖczniku
        tags_list = list(tags) if isinstance(tags, (list, tuple)) else ([] if tags is None else [str(tags)])
        has_retry = bool(flags.get("has_retry")) if isinstance(flags, dict) else False
        has_rb    = bool(flags.get("has_rollback")) if isinstance(flags, dict) else False
        has_opt   = bool(flags.get("has_optimization")) if isinstance(flags, dict) else False

        
        
        doc = {
        "id": mid_short,
        "structData": {
            "mission_id": mid_full,
            "timestamp": ts_iso,
            "mission_type": mission_type_val,
            "tags": tags_list,
            "outcome": "Success" if approved_val else ("Partial" if (final_score_val not in (None, 0)) else "Failure"),
            "final_score": float(final_score_val) if final_score_val is not None else None,
            "approved": bool(approved_val),
            "nodes_count": int(nodes_count_val) if nodes_count_val is not None else None,
            "edges_count": int(edges_count_val) if edges_count_val is not None else None,
            "has_retry": flags_val["has_retry"],
            "has_rollback": flags_val["has_rollback"],
            "has_optimization": flags_val["has_optimization"],
            "lang": lang_val,
            "display_id": display_id,
            "links": {
                "txt_uri": txt_uri,
                "plan_uri": plan_uri,
                "transcript_uri": transcript_uri,
                "metrics_uri": meta_uri,
            },
        },
        "content": {
            "mimeType": "text/plain",
            "uri": txt_uri,
        },
        }
        
        

        # 5) Jednowierszowy plik NDJSON do folderu index/
        index_dir  = f"{root_prefix}/index"    # je≈õli chcesz top-level: index_dir = "index"
        index_path = f"{index_dir}/metadata_{ts_file}.ndjson"
        bucket.blob(index_path).upload_from_string(
            json.dumps(doc, ensure_ascii=False) + "\n",
            content_type="application/x-ndjson; charset=utf-8",
        )
        print(f"[INDEX] NDJSON -> gs://{bucket_name}/{index_path}")
        
        
        
        #koniec zapisu indeksu

        print(f"[EXPORT] {mission_id} -> {ndjson_uri}")
        ndjson_uris.append(ndjson_uri)

    return ndjson_uris



--- FILE: exporter_missions_lib.py ---

# exporter_missions_lib.py
# Eksporter misji ‚Äì czysta funkcja do u≈ºycia w notebooku lub pipeline

from __future__ import annotations
import json, re, hashlib
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

FENCE_RE = re.compile(r"^```(?:json)?\s*|\s*```$", flags=re.IGNORECASE | re.MULTILINE)


def _strip_fences(text: str) -> str:
    return FENCE_RE.sub("", text or "").strip()


def _to_str_content(content: Any) -> str:
    if content is None:
        return ""
    s = (
        json.dumps(content, ensure_ascii=False)
        if isinstance(content, (dict, list))
        else str(content)
    )
    return _strip_fences(s)


def _iso_utc(ts: str | None) -> str:
    if not ts:
        return datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
    t = ts.replace(" ", "T")
    if "." in t:
        t = t.split(".")[0]
    return t if t.endswith("Z") else t + "Z"


def _hash(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()[:12]


def _ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def _extract_plan(snapshot: Dict[str, Any]) -> Dict[str, Any]:
    if isinstance(snapshot.get("final_plan"), dict):
        return snapshot["final_plan"]
    for it in snapshot.get("iterations") or []:
        for key in ("aggregator", "critic"):
            block = it.get(key, {}) or {}
            try:
                data = json.loads(_to_str_content(block.get("content", "")))
                if isinstance(data, dict) and "final_plan" in data:
                    return data["final_plan"]
                if isinstance(data, dict) and "plan_approved" in data:
                    return data["plan_approved"]
            except Exception:
                pass
    return {"entry_point": "", "nodes": [], "edges": []}


def _count_nodes_edges(plan: Dict[str, Any]) -> Tuple[int, int]:
    return len(plan.get("nodes") or []), len(plan.get("edges") or [])


def _approved(snapshot: Dict[str, Any]) -> bool:
    verdicts = [str(snapshot.get("verdict", ""))]
    for it in snapshot.get("iterations") or []:
        critic = it.get("critic", {}) or {}
        verdicts.append(str(critic.get("verdict", "")))
        if "zatwierdzony" in _to_str_content(critic.get("content", "")).lower():
            verdicts.append("ZATWIERDZONY")
    if "plan_zatwierdzony" in str(snapshot.get("decision_marker", "")).lower():
        verdicts.append("ZATWIERDZONY")
    return any("zatwierdzony" in v.lower() for v in verdicts)


def _derive_flags(snapshot: Dict[str, Any], plan: Dict[str, Any]) -> Dict[str, bool]:
    blob = "\n".join(
        [
            _to_str_content(snapshot.get("mission_prompt", "")),
            _to_str_content(snapshot.get("llm_generated_summary", "")),
            _to_str_content(snapshot.get("aggregator_reasoning", "")),
            _to_str_content(snapshot.get("identified_patterns", "")),
            json.dumps(plan, ensure_ascii=False),
        ]
    ).lower()
    return {
        "has_retry": ("retry" in blob or "ponow" in blob),
        "has_rollback": ("rollback" in blob or "wycof" in blob),
        "has_optimization": ("optimiz" in blob or "optymal" in blob),
    }


def _build_txt(snapshot: Dict[str, Any], plan: Dict[str, Any]) -> str:
    mission_id = (
        snapshot.get("memory_id")
        or snapshot.get("mission_id")
        or f"mission_{_hash(json.dumps(snapshot, ensure_ascii=False))}"
    )
    timestamp = _iso_utc(snapshot.get("timestamp"))
    mtype = snapshot.get("mission_type", "")
    tags = ", ".join(snapshot.get("tags") or [])
    outcome = str(snapshot.get("outcome", ""))
    score = snapshot.get("final_score", snapshot.get("score", ""))
    verdict = "ZATWIERDZONY" if _approved(snapshot) else ""
    nodes_cnt, edges_cnt = _count_nodes_edges(plan)
    prompt = _to_str_content(snapshot.get("mission_prompt", ""))
    llm_summary = _to_str_content(snapshot.get("llm_generated_summary", ""))

    txt = []
    txt.append(f"# Mission: {prompt[:80] or '‚Äî'}")
    txt.append(f"ID: {mission_id}")
    txt.append(f"Timestamp: {timestamp}")
    txt.append(f"Type: {mtype}")
    txt.append(f"Tags: {tags}")
    txt.append(f"Outcome | Score | Verdict: {outcome} | {score} | {verdict}\n")
    txt.append("## Executive Summary")
    txt.append(llm_summary or "Brak skr√≥tu; patrz szczeg√≥≈Çy planu i ryzyka.\n")
    txt.append("## Final Plan (skr√≥t)")
    txt.append(f'Entry: {plan.get("entry_point","")}')
    node_names = [n.get("name") for n in plan.get("nodes", []) if isinstance(n, dict)]
    txt.append(f"Wƒôz≈Çy ({nodes_cnt}): " + ", ".join(node_names))
    return "\n".join(txt).strip() + "\n"


def _build_transcript(snapshot: Dict[str, Any]) -> Dict[str, Any]:
    def norm(m: Dict[str, Any]) -> Dict[str, Any]:
        m2 = dict(m)
        m2["content"] = _to_str_content(m.get("content", ""))
        return m2

    out = {
        "mission_id": snapshot.get("memory_id") or snapshot.get("mission_id"),
        "iterations": [],
        "full_transcript": [],
    }
    for it in snapshot.get("iterations") or []:
        it_out = {}
        for k, v in it.items():
            if k == "proposers":
                it_out[k] = [
                    {
                        "agent": p.get("agent"),
                        "content": _to_str_content(p.get("content", "")),
                    }
                    for p in (v or [])
                ]
            elif k in ("aggregator", "critic"):
                block = v or {}
                it_out[k] = {"content": _to_str_content(block.get("content", ""))}
            else:
                it_out[k] = v
        out["iterations"].append(it_out)
    out["full_transcript"] = [norm(m) for m in (snapshot.get("full_transcript") or [])]
    return out


def _build_metrics(snapshot: Dict[str, Any], plan: Dict[str, Any]) -> Dict[str, Any]:
    mission_id = snapshot.get("memory_id") or snapshot.get("mission_id")
    flags = _derive_flags(snapshot, plan)
    nodes_cnt, edges_cnt = _count_nodes_edges(plan)
    return {
        "mission_id": mission_id,
        "timestamp": _iso_utc(snapshot.get("timestamp")),
        "mission_type": snapshot.get("mission_type"),
        "tags": snapshot.get("tags") or [],
        "outcome": snapshot.get("outcome"),
        "final_score": snapshot.get("final_score", snapshot.get("score")),
        "approved": _approved(snapshot),
        "nodes_count": nodes_cnt,
        "edges_count": edges_cnt,
        **flags,
        "lang": snapshot.get("lang", "pl"),
    }


def _ndjson_line(
    mission_id: str,
    txt_uri: str,
    plan_uri: str,
    transcript_uri: str,
    metrics_uri: str,
    metrics: Dict[str, Any],
) -> str:
    return json.dumps(
        {
            "id": mission_id,
            "structData": {
                **metrics,
                "links": {
                    "plan_uri": plan_uri,
                    "transcript_uri": transcript_uri,
                    "metrics_uri": metrics_uri,
                },
            },
            "content": {"mimeType": "text/plain", "uri": txt_uri},
        },
        ensure_ascii=False,
    )


def process_file(
    src_json: Path, input_dir: Path, out_dir: Path, gcs_prefix: str
) -> List[str]:
    rel = src_json.relative_to(input_dir)
    snap = json.loads(src_json.read_text(encoding="utf-8"))
    mission_id = (
        snap.get("memory_id")
        or snap.get("mission_id")
        or f"mission_{_hash(json.dumps(snap, ensure_ascii=False))}"
    )
    plan = _extract_plan(snap)
    txt = _build_txt(snap, plan)
    transcript = _build_transcript(snap)
    metrics = _build_metrics(snap, plan)
    rel_dir = rel.parent
    base = out_dir / rel_dir / mission_id
    _ensure_dir(base)
    (base / f"{mission_id}.txt").write_text(txt, encoding="utf-8")
    (base / f"{mission_id}.plan.json").write_text(
        json.dumps(plan, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    (base / f"{mission_id}.transcript.json").write_text(
        json.dumps(transcript, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    (base / f"{mission_id}.metrics.json").write_text(
        json.dumps(metrics, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    gcs_base = f"{gcs_prefix}/{rel_dir.as_posix()}/{mission_id}"
    return [
        _ndjson_line(
            mission_id,
            f"{gcs_base}/{mission_id}.txt",
            f"{gcs_base}/{mission_id}.plan.json",
            f"{gcs_base}/{mission_id}.transcript.json",
            f"{gcs_base}/{mission_id}.metrics.json",
            metrics,
        )
    ]


def export_missions(
    input_dir: str,
    out_dir: str,
    gcs_prefix: str,
    pattern="**/*.json",
    ndjson_out="metadata.ndjson",
):
    input_dir = Path(input_dir).resolve()
    out_dir = Path(out_dir).resolve()
    _ensure_dir(out_dir)
    files = sorted(input_dir.glob(pattern))
    if not files:
        print("Brak plik√≥w JSON do przetworzenia.")
        return
    lines: List[str] = []
    for f in files:
        try:
            lines.extend(process_file(f, input_dir, out_dir, gcs_prefix))
        except Exception as e:
            print(f"[WARN] Pomi≈Ñ {f}: {e}")
    ndjson_path = out_dir / ndjson_out
    ndjson_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
    print(f"OK. Artefakty w: {out_dir}\nNDJSON: {ndjson_path}")



--- FILE: extended_llm_wrapper.py ---

"""
Rozszerzony wrapper LLM z bardziej realistycznymi dummy responses dla r√≥≈ºnych r√≥l
"""

import json
import random
from typing import Dict, Any


class ExtendedLLMWrapper:
    """
    Rozszerzona wersja wrappera z r√≥≈ºnorodnymi odpowiedziami dla demo
    """

    @staticmethod
    def generate_dummy_response(model_name: str, prompt: str) -> str:
        """Generuje r√≥≈ºne odpowiedzi w zale≈ºno≈õci od typu agenta"""

        # Sprawd≈∫ typ agenta na podstawie nazwy modelu lub promptu
        if "causal" in model_name.lower() or "Causal" in prompt:
            return ExtendedLLMWrapper._causal_analyst_response()
        elif "creative" in model_name.lower() or "Creative" in prompt:
            return ExtendedLLMWrapper._creative_planner_response()
        elif "risk" in model_name.lower() or "Risk" in prompt:
            return ExtendedLLMWrapper._risk_analyst_response()
        elif "aggregator" in model_name.lower() or "Aggregator" in prompt:
            return ExtendedLLMWrapper._aggregator_response(prompt)
        elif "critic" in model_name.lower() or "Critic" in prompt:
            return ExtendedLLMWrapper._critic_response(prompt)
        else:
            return ExtendedLLMWrapper._default_response()

    @staticmethod
    def _causal_analyst_response() -> str:
        """Odpowied≈∫ analityka przyczynowego"""
        response = {
            "thought_process": [
                "Analizujƒô potencjalne relacje przyczynowe w przep≈Çywie danych",
                "Identyfikujƒô zmienne confounding i mediatory",
                "Projektujƒô DAG (Directed Acyclic Graph) dla workflow",
            ],
            "plan": {
                "entry_point": "validate_data",
                "nodes": [
                    {"name": "validate_data", "implementation": "validate_data"},
                    {"name": "check_quality", "implementation": "check_quality"},
                    {
                        "name": "discover_causality",
                        "implementation": "discover_causality",
                    },
                    {"name": "error_handler", "implementation": "error_handler"},
                    {"name": "validate_model", "implementation": "validate_model"},
                    {"name": "generate_report", "implementation": "generate_report"},
                ],
                "edges": [
                    {"from": "validate_data", "to": "check_quality"},
                    {"from": "check_quality", "to": "discover_causality"},
                    {
                        "from": "discover_causality",
                        "to": "validate_model",
                        "condition": "check_success",
                    },
                    {
                        "from": "discover_causality",
                        "to": "error_handler",
                        "condition": "check_error",
                    },
                    {"from": "error_handler", "to": "discover_causality"},
                    {"from": "validate_model", "to": "generate_report"},
                ],
            },
            "confidence": 0.85,
            "key_innovations": [
                "Dodanie pƒôtli retry dla discover_causality",
                "Walidacja jako≈õci przed analizƒÖ przyczynowƒÖ",
            ],
            "risk_mitigation": {
                "data_quality": "Podw√≥jna walidacja przed analizƒÖ",
                "algorithm_failure": "Error handler z retry mechanism",
            },
        }
        return json.dumps(response)

    @staticmethod
    def _creative_planner_response() -> str:
        """Odpowied≈∫ kreatywnego planera"""
        response = {
            "thought_process": [
                "My≈õlƒô nieszablonowo - co gdyby pipeline sam siƒô optymalizowa≈Ç?",
                "Inspiracja z natury: mr√≥wki znajdujƒÖ optymalnƒÖ ≈õcie≈ºkƒô",
                "Dodajƒô element adaptacyjno≈õci i uczenia siƒô",
            ],
            "plan": {
                "entry_point": "load_data",
                "nodes": [
                    {"name": "load_data", "implementation": "load_data"},
                    {"name": "clean_data", "implementation": "clean_data"},
                    {
                        "name": "optimize_performance",
                        "implementation": "optimize_performance",
                    },
                    {
                        "name": "discover_causality",
                        "implementation": "discover_causality",
                    },
                    {"name": "train_model", "implementation": "train_model"},
                    {"name": "notify_user", "implementation": "notify_user"},
                ],
                "edges": [
                    {"from": "load_data", "to": "clean_data"},
                    {"from": "clean_data", "to": "optimize_performance"},
                    {"from": "optimize_performance", "to": "discover_causality"},
                    {"from": "discover_causality", "to": "train_model"},
                    {"from": "train_model", "to": "notify_user"},
                ],
            },
            "confidence": 0.75,
            "key_innovations": [
                "Samooptymalizacja pipeline'u",
                "Proaktywne powiadomienia u≈ºytkownika",
                "Adaptacyjne dostosowanie do typu danych",
            ],
            "risk_mitigation": {
                "performance": "Continuous optimization",
                "user_experience": "Real-time notifications",
            },
        }
        return json.dumps(response)

    @staticmethod
    def _risk_analyst_response() -> str:
        """Odpowied≈∫ analityka ryzyka"""
        response = {
            "thought_process": [
                "Identyfikujƒô wszystkie mo≈ºliwe punkty awarii",
                "Analizujƒô cascading failures",
                "Projektujƒô redundancjƒô i fallback paths",
            ],
            "plan": {
                "entry_point": "validate_data",
                "nodes": [
                    {"name": "validate_data", "implementation": "validate_data"},
                    {"name": "clean_data", "implementation": "clean_data"},
                    {"name": "check_quality", "implementation": "check_quality"},
                    {
                        "name": "discover_causality",
                        "implementation": "discover_causality",
                    },
                    {"name": "error_handler", "implementation": "error_handler"},
                    {"name": "rollback", "implementation": "rollback"},
                    {"name": "validate_model", "implementation": "validate_model"},
                    {"name": "generate_report", "implementation": "generate_report"},
                ],
                "edges": [
                    {"from": "validate_data", "to": "clean_data"},
                    {"from": "clean_data", "to": "check_quality"},
                    {
                        "from": "check_quality",
                        "to": "discover_causality",
                        "condition": "quality_ok",
                    },
                    {
                        "from": "check_quality",
                        "to": "rollback",
                        "condition": "quality_fail",
                    },
                    {
                        "from": "discover_causality",
                        "to": "validate_model",
                        "condition": "success",
                    },
                    {
                        "from": "discover_causality",
                        "to": "error_handler",
                        "condition": "error",
                    },
                    {
                        "from": "error_handler",
                        "to": "rollback",
                        "condition": "cannot_recover",
                    },
                    {
                        "from": "error_handler",
                        "to": "discover_causality",
                        "condition": "can_retry",
                    },
                    {"from": "validate_model", "to": "generate_report"},
                    {"from": "rollback", "to": "generate_report"},
                ],
            },
            "confidence": 0.90,
            "key_innovations": [
                "Comprehensive error handling",
                "Multiple fallback paths",
                "Quality gates at critical points",
            ],
            "risk_mitigation": {
                "data_corruption": "Rollback mechanism",
                "algorithm_failure": "Multiple retry with degradation",
                "quality_issues": "Early detection and abort",
            },
        }
        return json.dumps(response)

    @staticmethod
    def _aggregator_response(prompt: str) -> str:
        """Odpowied≈∫ agregatora - synteza propozycji"""
        # Sprawd≈∫ iteracjƒô je≈õli jest w prompcie
        iteration = 1
        if "ITERATION:" in prompt:
            try:
                iteration = int(prompt.split("ITERATION:")[1].split("/")[0].strip())
            except:
                pass

        response = {
            "thought_process": [
                "Analizujƒô si≈Çy ka≈ºdej propozycji",
                "Identyfikujƒô synergie miƒôdzy podej≈õciami",
                "≈ÅƒÖczƒô najlepsze elementy w sp√≥jnƒÖ ca≈Ço≈õƒá",
            ],
            "final_plan": {
                "entry_point": "validate_data",
                "nodes": [
                    {"name": "validate_data", "implementation": "validate_data"},
                    {"name": "clean_data", "implementation": "clean_data"},
                    {"name": "check_quality", "implementation": "check_quality"},
                    {
                        "name": "optimize_performance",
                        "implementation": "optimize_performance",
                    },
                    {
                        "name": "discover_causality",
                        "implementation": "discover_causality",
                    },
                    {"name": "error_handler", "implementation": "error_handler"},
                    {"name": "rollback", "implementation": "rollback"},
                    {"name": "train_model", "implementation": "train_model"},
                    {"name": "validate_model", "implementation": "validate_model"},
                    {"name": "generate_report", "implementation": "generate_report"},
                    {"name": "notify_user", "implementation": "notify_user"},
                ],
                "edges": [
                    {"from": "validate_data", "to": "clean_data"},
                    {"from": "clean_data", "to": "check_quality"},
                    {
                        "from": "check_quality",
                        "to": "optimize_performance",
                        "condition": "quality_ok",
                    },
                    {
                        "from": "check_quality",
                        "to": "rollback",
                        "condition": "quality_fail",
                    },
                    {"from": "optimize_performance", "to": "discover_causality"},
                    {
                        "from": "discover_causality",
                        "to": "train_model",
                        "condition": "success",
                    },
                    {
                        "from": "discover_causality",
                        "to": "error_handler",
                        "condition": "error",
                    },
                    {
                        "from": "error_handler",
                        "to": "rollback",
                        "condition": "max_retries",
                    },
                    {
                        "from": "error_handler",
                        "to": "discover_causality",
                        "condition": "can_retry",
                    },
                    {"from": "train_model", "to": "validate_model"},
                    {"from": "validate_model", "to": "generate_report"},
                    {"from": "generate_report", "to": "notify_user"},
                ],
            },
            "synthesis_reasoning": "Po≈ÇƒÖczy≈Çem solidnƒÖ obs≈Çugƒô b≈Çƒôd√≥w od Risk Analyst, innowacyjnƒÖ optymalizacjƒô od Creative Planner, i rygorystycznƒÖ walidacjƒô od Causal Analyst",
            "component_sources": {
                "Causal Analyst": ["validate_data", "check_quality", "validate_model"],
                "Creative Planner": ["optimize_performance", "notify_user"],
                "Risk Analyst": ["error_handler", "rollback", "conditional_edges"],
            },
            "confidence_score": 0.80 + iteration * 0.05,  # Ro≈õnie z iteracjami
            "improvements": [
                "Dodanie cache dla powtarzalnych operacji",
                "Implementacja progressive enhancement",
                "Monitoring w czasie rzeczywistym",
            ],
        }
        return json.dumps(response)

    @staticmethod
    def _critic_response(prompt: str) -> str:
        """Odpowied≈∫ krytyka - ocena planu"""
        # Sprawd≈∫ iteracjƒô
        iteration = 1
        if "ITERATION:" in prompt:
            try:
                iteration = int(prompt.split("ITERATION:")[1].split("/")[0].strip())
            except:
                pass

        # Dostosuj ocenƒô do iteracji
        base_score = 60 + iteration * 8
        approved = base_score >= 75 or iteration >= 4

        response = {
            "approved": approved,
            "score": min(base_score + random.randint(-5, 10), 95),
            "strengths": [
                "Comprehensive error handling",
                "Good balance between robustness and efficiency",
                "Clear separation of concerns",
                "Innovative optimization approach",
            ][
                : 2 + iteration
            ],  # Wiƒôcej mocnych stron w p√≥≈∫niejszych iteracjach
            "weaknesses": [
                "Missing parallelization opportunities",
                "No caching mechanism",
                "Limited monitoring capabilities",
                "Could benefit from more granular error types",
            ][
                iteration - 1 :
            ],  # Mniej s≈Çabo≈õci w p√≥≈∫niejszych iteracjach
            "feedback": f"Plan shows {'significant' if iteration > 2 else 'good'} improvement. {'Ready for deployment.' if approved else 'Further refinement needed.'}",
            "improvements": (
                [
                    "Add parallel processing for independent steps",
                    "Implement result caching",
                    "Add detailed logging and monitoring",
                    "Consider adding A/B testing capability",
                ][iteration - 1 :]
                if not approved
                else []
            ),
        }

        # KLUCZOWA ZMIANA: Dodaj frazƒô "PLAN_ZATWIERDZONY" je≈õli zatwierdzamy
        response_json = json.dumps(response)

        if approved:
            # Dodaj magicznƒÖ frazƒô PO JSONie
            response_json += "\n\nPLAN_ZATWIERDZONY"

        return response_json

    @staticmethod
    def _default_response() -> str:
        """Domy≈õlna odpowied≈∫"""
        response = {
            "thought_process": ["Analyzing task", "Creating plan"],
            "plan": {
                "entry_point": "load_data",
                "nodes": [
                    {"name": "load_data", "implementation": "load_data"},
                    {"name": "process", "implementation": "clean_data"},
                    {"name": "output", "implementation": "generate_report"},
                ],
                "edges": [
                    {"from": "load_data", "to": "process"},
                    {"from": "process", "to": "output"},
                ],
            },
            "confidence": 0.7,
        }
        return json.dumps(response)


# ZastƒÖp oryginalnƒÖ klasƒô LLMWrapper
import llm_wrapper

original_call = llm_wrapper.LLMWrapper.__call__


def enhanced_call(self, prompt: str) -> str:
    """Rozszerzone wywo≈Çanie z lepszymi dummy responses"""
    if self.provider == "dummy":
        return ExtendedLLMWrapper.generate_dummy_response(self.model_name, prompt)
    else:
        return original_call(self, prompt)


# Monkey-patch oryginalnej klasy
llm_wrapper.LLMWrapper.__call__ = enhanced_call



--- FILE: llm_wrapper.py ---

"""
Wrapper modeli LLM. Umo≈ºliwia ≈ÇatwƒÖ zamianƒô ≈∫r√≥d≈Ça modelu (np. OpenAI, lokalny model itp.).
W tym przyk≈Çadzie implementujemy klasƒô `LLMWrapper`, kt√≥ra w trybie demonstracyjnym
generuje sztucznƒÖ odpowied≈∫. Aby u≈ºyƒá prawdziwego modelu (np. GPT‚Äë5), nale≈ºy
uzupe≈Çniƒá implementacjƒô wywo≈Çania API w metodzie `__call__`.
"""

import os
import json


class LLMWrapper:
    def __init__(
        self,
        provider: str,
        model_name: str,
        api_key_env: str = None,
        temperature: float = 0.5,
    ):
        """
        :param provider: dostawca modelu, np. "openai" lub "dummy" dla demonstracji
        :param model_name: nazwa modelu u dostawcy
        :param api_key_env: nazwa zmiennej ≈õrodowiskowej z kluczem API
        :param temperature: parametr kreatywno≈õci dla modeli typu GPT
        """
        self.provider = provider
        self.model_name = model_name
        self.temperature = temperature
        self.api_key = os.environ.get(api_key_env) if api_key_env else None

    def __call__(self, prompt: str) -> str:
        """
        Zwraca odpowied≈∫ modelu na dany prompt. W wersji demonstracyjnej,
        je≈õli provider to "dummy", generuje prosty plan w formacie JSON.
        W przeciwnym razie wymaga zaimplementowania wywo≈Çania API.
        """
        if self.provider == "dummy":
            # Zwr√≥ƒá przyk≈Çadowy JSON jako ciƒÖg znak√≥w
            response = {
                "thought_process": ["Analiza zadania", "Propozycja rozwiƒÖzania"],
                "plan": {
                    "entry_point": "start",
                    "nodes": [
                        {"name": "start", "implementation": "init_task"},
                        {"name": "finish", "implementation": "end_task"},
                    ],
                    "edges": [{"from": "start", "to": "finish"}],
                },
                "confidence": 0.85,
            }
            return json.dumps(response)
        elif self.provider == "openai":
            # Przyk≈Çad wywo≈Çania OpenAI ChatCompletion ‚Äì wymaga biblioteki openai i klucza API
            try:
                import openai  # zaimportuj wewnƒÖtrz, aby uniknƒÖƒá zale≈ºno≈õci dla dummy
            except ImportError:
                raise RuntimeError(
                    "Biblioteka openai nie jest zainstalowana. Zainstaluj jƒÖ lub u≈ºyj provider='dummy'."
                )
            if not self.api_key:
                raise RuntimeError(
                    "Brak klucza API. Ustaw zmiennƒÖ ≈õrodowiskowƒÖ lub przeka≈º api_key_env."
                )
            openai.api_key = self.api_key
            # Buduj listƒô wiadomo≈õci zgodnie z API ChatCompletion
            messages = [
                {"role": "system", "content": "You are an advanced planning agent."},
                {"role": "user", "content": prompt},
            ]
            response = openai.ChatCompletion.create(
                model=self.model_name, messages=messages, temperature=self.temperature
            )
            return response.choices[0].message["content"]
        else:
            raise NotImplementedError(
                f"Provider '{self.provider}' nie jest obs≈Çugiwany."
            )



--- FILE: memory_helpers.py ---

# memory_helpers.py

from datetime import datetime
import json
import uuid
from typing import Any, Dict, List, Optional, Tuple

from google.cloud import storage


# ---------- Generatory i serializacja ----------

def gen_mission_id() -> str:
    """Generuje unikalny identyfikator misji."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    short = uuid.uuid4().hex[:6]
    return f"mission_{timestamp}_{short}"


def safe_json_dumps(obj: Any) -> str:
    """Bezpieczne serializowanie do JSON; w razie b≈Çƒôd√≥w serializuje repr."""
    try:
        return json.dumps(obj, ensure_ascii=False, indent=2)
    except Exception:
        return json.dumps(str(obj), ensure_ascii=False, indent=2)


def count_nodes_edges(plan: Dict) -> Tuple[int, int]:
    """Zlicza liczbƒô wƒôz≈Ç√≥w i krawƒôdzi w finalnym planie."""
    nodes = plan.get("nodes", []) if isinstance(plan, dict) else []
    edges = plan.get("edges", []) if isinstance(plan, dict) else []
    return (len(nodes), len(edges))


def infer_flags_and_tags(plan: Dict, transcript: List[str]) -> Tuple[Dict[str, bool], List[str]]:
    """Wykrywa flagi (retry, rollback, optimize) i generuje listƒô tag√≥w."""
    joined = " ".join(str(x).lower() for x in transcript)[:50000]
    flags = {
        "has_retry": any(k in joined for k in ["retry", "pon√≥w", "ponowienie", "backoff"]),
        "has_rollback": "rollback" in joined,
        "has_optimization": any(k in joined for k in ["optimiz", "optymaliz"]),
    }
    tags: List[str] = []
    if flags["has_retry"]:
        tags.append("retry")
    if flags["has_rollback"]:
        tags.append("rollback")
    if flags["has_optimization"]:
        tags.append("optimize")
    return flags, tags


# ---------- G≈Ç√≥wna funkcja zapisu ----------

def save_mission_to_gcs(
    bucket_name: str,
    base_prefix: str,
    mission: str,
    final_plan: Dict,
    all_messages: List[Dict],
    orchestrator_state: Dict,
    approved: bool = True,
    final_score: Optional[float] = None
) -> str:
    
    
    print(">>> save_mission_to_gcs CALLED")
    print(">>> module:", __name__)
    print(">>> file:  ", __file__)
    logging.warning("save_mission_to_gcs CALLED from %s", __file__)
    """
    Zapisuje plan, transkrypt i metadane do Google Cloud Storage w strukturze:
      gs://{bucket}/{base_prefix}/{mission_id}/plan.json
      gs://{bucket}/{base_prefix}/{mission_id}/transcript.jsonl
      gs://{bucket}/{base_prefix}/{mission_id}/metadata.json

    Zwraca mission_id.
    """
    # 1. Id i ≈õcie≈ºki
    mission_id = gen_mission_id()
    base_path = f"{base_prefix}/{mission_id}"
    plan_path = f"{base_path}/plan.json"
    transcript_path = f"{base_path}/transcript.jsonl"
    meta_path = f"{base_path}/metadata.json"

    plan_uri = f"gs://{bucket_name}/{plan_path}"
    transcript_uri = f"gs://{bucket_name}/{transcript_path}"
    meta_uri = f"gs://{bucket_name}/{meta_path}"

    # 2. Przygotuj dane
    nodes_count, edges_count = count_nodes_edges(final_plan)

    # Zamie≈Ñ transkrypt na JSONL (po jednej linii na wiadomo≈õƒá)
    transcript_lines: List[str] = []
    transcript_texts: List[str] = []
    for m in all_messages:
        mm = dict(m)
        c = mm.get("content")
        transcript_texts.append(c if isinstance(c, str) else safe_json_dumps(c))
        # Serializacja do JSONL ‚Äì konwertuj content na string je≈õli to np. dict
        if not isinstance(c, (str, dict)):
            mm["content"] = str(c)
        transcript_lines.append(safe_json_dumps(mm))

    flags, tags = infer_flags_and_tags(final_plan, transcript_texts)

    metadata = {
        "mission_id": mission_id,
        "mission_prompt": mission,
        "approved": approved,
        "final_score": float(final_score) if final_score is not None else None,
        "nodes_count": nodes_count,
        "edges_count": edges_count,
        "has_optimization": flags["has_optimization"],
        "has_rollback": flags["has_rollback"],
        "has_retry": flags["has_retry"],
        "tags": tags,
        "orchestrator_state": orchestrator_state or {},
        "timestamp": datetime.now().isoformat(),
        "links": {
            "plan_uri": plan_uri,
            "transcript_uri": transcript_uri,
            "metadata_uri": meta_uri,
        },
        "preview": {
            "entry_point": (final_plan or {}).get("entry_point"),
        },
    }

    # 3. Zapis do GCS
    client = storage.Client()
    bucket = client.bucket(bucket_name)

    bucket.blob(plan_path).upload_from_string(
        safe_json_dumps(final_plan), content_type="application/json; charset=utf-8"
    )
    bucket.blob(transcript_path).upload_from_string(
        "\n".join(transcript_lines), content_type="application/x-ndjson; charset=utf-8"
    )
    bucket.blob(meta_path).upload_from_string(
        safe_json_dumps(metadata), content_type="application/json; charset=utf-8"
    )
    
    
    
    #zapisywanie indeksow
    ts_dt = datetime.now(timezone.utc)
    ts_file = ts_dt.strftime("%Y%m%d_%H%M%S")
    ts_iso  = ts_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

    # preview.txt (content.uri)
    preview_txt = (
        f"mission_id: {mission_id}\n"
        f"timestamp:  {ts_iso}\n"
        f"approved:   {bool(approved)}\n"
        f"final_score:{final_score if final_score is not None else 'null'}\n"
        f"tags:       {', '.join(tags) if tags else ''}\n"
    )
    preview_path = f"{base_path}/preview.txt"
    bucket.blob(preview_path).upload_from_string(
        preview_txt, content_type="text/plain; charset=utf-8"
    )
    preview_uri = f"gs://{bucket_name}/{preview_path}"

    # dokument NDJSON (1 linia)
    ndjson_doc = {
        "id": f"{ts_file}_{(mission_id[-8:] if len(mission_id) >= 8 else mission_id)}",
        "structData": {
            "mission_id": mission_id,
            "timestamp": ts_iso,
            "mission_type": "general",
            "tags": tags,
            "outcome": "Success" if approved else "Partial" if final_score else "Failure",
            "final_score": float(final_score) if final_score is not None else None,
            "approved": bool(approved),
            "nodes_count": int(nodes_count),
            "edges_count": int(edges_count),
            "has_retry": bool(flags.get("has_retry")) if isinstance(flags, dict) else False,
            "has_rollback": bool(flags.get("has_rollback")) if isinstance(flags, dict) else False,
            "has_optimization": bool(flags.get("has_optimization")) if isinstance(flags, dict) else False,
            "lang": "pl",
            "display_id": f"{ts_file}-{mission_id}",
            "links": {
                "txt_uri": preview_uri,
                "plan_uri": plan_uri,
                "transcript_uri": transcript_uri,
                "metrics_uri": meta_uri,
                "metadata_uri": meta_uri,
            },
        },
        "content": {
            "mimeType": "text/plain",
            "uri": preview_uri,
        },
    }

    # zapis NDJSON pod {base_prefix}/index/
    index_dir  = f"{base_prefix}/index"
    index_path = f"{index_dir}/metadata_{ts_file}.ndjson"
    bucket.blob(index_path).upload_from_string(
        json.dumps(ndjson_doc, ensure_ascii=False) + "\n",
        content_type="application/x-ndjson; charset=utf-8",
    )

    # twardy log z pe≈Çnym URI
    ndjson_uri = f"gs://{bucket_name}/{index_path}"
    print("NDJSON ->", ndjson_uri)
    logging.warning("NDJSON wrote to %s", ndjson_uri)

    # ma≈Çy kanarek, ≈ºeby ≈Çatwo z≈Çapaƒá prefiks (ten sam katalog co NDJSON)
    canary_path = f"{index_dir}/_canary_{ts_file}.txt"
    bucket.blob(canary_path).upload_from_string(
        f"ok {ts_iso} mission_id={mission_id}",
        content_type="text/plain; charset=utf-8",
    )
    print("CANARY ->", f"gs://{bucket_name}/{canary_path}")
    
    #koniec zapisu indeksow

    logging.getLogger(__name__).info(f"[MEMORY:GCS] Saved mission {mission_id} at {plan_uri}")
    return mission_id



--- FILE: memory_system.py ---

"""
System pamiƒôci kontekstowej z uczeniem siƒô z poprzednich iteracji
"""

from typing import Dict, List, Any, Optional
from datetime import datetime
import json
import numpy as np
from collections import deque
import os

# Zewnƒôtrzne biblioteki do obliczania podobie≈Ñstwa tekstu
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Lokalny logger procesu
from process_logger import log as process_log


import re, unicodedata
from autogen_vertex_mcp_system_claude import SystemConfig, VertexSearchTool


def _to_text_safe(x) -> str:
    if x is None:
        return ""
    if isinstance(x, str):
        return x
    try:
        return json.dumps(x, ensure_ascii=False)
    except Exception:
        return str(x)


def _slugify(text: str, maxlen: int = 60) -> str:
    text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode()
    text = re.sub(r"[^a-zA-Z0-9]+", "-", text).strip("-").lower()
    return text[:maxlen] or "mission"


def _gcs_upload_json(bucket_name: str, blob_name: str, obj: dict):
    """Wrzuca JSON do GCS pod gs://{bucket_name}/{blob_name}"""
    try:
        from google.cloud import storage
    except ImportError:
        raise RuntimeError(
            "Brak pakietu google-cloud-storage. Zainstaluj: pip install google-cloud-storage"
        )
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    blob.cache_control = "no-cache"
    blob.upload_from_string(
        data=json.dumps(obj, ensure_ascii=False, indent=2),
        content_type="application/json",
    )


class ContextMemory:
    def __init__(
        self,
        max_episodes: int = 100,
        gcs_bucket: str | None = None,
        gcs_prefix: str = "",
    ):
        # Existing
        self.episodes = deque(maxlen=max_episodes)
        self.learned_patterns = {}
        self.successful_strategies = []

        # NOWE - Pe≈Çne dane misji
        self.full_mission_records = []  # Bez limitu - wszystko zapisujemy
        self.mission_index = {}  # Szybkie wyszukiwanie po ID

        self._load_persistent_memory()
        self.gcs_bucket = gcs_bucket
        self.gcs_prefix = (gcs_prefix or "").strip().strip("/")
        self.use_gcs = bool(self.gcs_bucket)

        self.iteration_feedback = []
        self.last_feedback = ""
        
        
    #poprawki
    
        
    def add_iteration_feedback(self, iteration: int, feedback: str, timestamp: datetime):
        """
        Zapisuje feedback z iteracji do pamiƒôci operacyjnej + lekki zapis lokalny.
        Orchestrator wo≈Ça to po ka≈ºdej odpowiedzi Critica.
        """
        try:
            entry = {
                "iteration": int(iteration),
                "feedback": str(feedback),
                "timestamp": timestamp.isoformat(),
            }
            # w RAM
            if not hasattr(self, "iteration_feedback"):
                self.iteration_feedback = []
            self.iteration_feedback.append(entry)
            # do szybkiego kontekstu
            self.last_feedback = entry["feedback"]

            # lekki zapis lokalny (bez zale≈ºno≈õci od pe≈Çnego mission_id)
            os.makedirs("memory/iterations", exist_ok=True)
            with open(f"memory/iterations/iter_{iteration:03d}.json", "w", encoding="utf-8") as f:
                json.dump(entry, f, ensure_ascii=False, indent=2)

            process_log(f"[MEMORY] add_iteration_feedback(iter={iteration}) ok")
        except Exception as e:
            process_log(f"[MEMORY ERROR] add_iteration_feedback failed: {e}")

    def get_relevant_context(self, mission: str) -> Dict[str, Any]:
        """
        Zwraca wstrzykniƒôcie kontekstu dla prompt√≥w (to wo≈Ça orchestrator).
        Minimalnie: rekomendacje, pu≈Çapki, ostatni feedback.
        Wersja bez TF-IDF: heurystyki + ostatnie sukcesy.
        """
        context = {
            "recommended_strategies": [],
            "common_pitfalls": [],
            "last_feedback": getattr(self, "last_feedback", ""),
        }

        # Heurystyka po tre≈õci misji (szybkie tagi)
        m = (mission or "").lower()
        if "csv" in m or "etl" in m or "pipeline" in m:
            context["recommended_strategies"].append("Add robust error handling with retry & dead-letter queue.")
            context["common_pitfalls"].append("Schema drift unchecked; missing data validation gates.")
        if "continuous" in m or "online" in m or "adapt" in m:
            context["recommended_strategies"].append("Introduce drift detection + gated retraining with rollback.")
            context["common_pitfalls"].append("No cap on retraining loops; missing convergence/abort criteria.")
        if "causal" in m or "przyczyn" in m:
            context["recommended_strategies"].append("Add causal shift analysis before blind retraining.")

        # Na bazie ostatnich udanych plan√≥w zapamiƒôtanych w tym procesie
        best_practices = []
        if hasattr(self, "successful_strategies") and self.successful_strategies:
            best_practices = self.successful_strategies[-5:]
        for bp in best_practices:
            tip = bp.get("tip") or bp.get("note")
            if tip:
                context["recommended_strategies"].append(tip)

        # Deduplicate i przytnij, ≈ºeby prompt by≈Ç zwiƒôz≈Çy
        context["recommended_strategies"] = list(dict.fromkeys(context["recommended_strategies"]))[:6]
        context["common_pitfalls"] = list(dict.fromkeys(context["common_pitfalls"]))[:6]

        return context

    
    
    #przeszukiwanie pamieci
    
    def get_vertex_context(self, mission: str, min_score: float = 80.0, top_k: int = 5) -> dict:
        ctx = {"recommended_strategies": [], "common_pitfalls": [], "examples": []}
        if VertexSearchTool is None or SystemConfig is None:
            return ctx
        try:
            cfg = SystemConfig()
            vst = VertexSearchTool(cfg)
            raw = vst.search_mission_memory(query=mission, top_k=top_k)
            results = (json.loads(raw) or {}).get("results", [])
            for r in results:
                try:
                    score = float(r.get("score") or 0)
                except Exception:
                    score = 0.0
                if score < min_score:
                    continue
                tags = r.get("tags") or []
                links = r.get("links") or {}
                if "retry" in tags:
                    ctx["recommended_strategies"].append("Use retry with backoff + DLQ.")
                if "rollback" in tags:
                    ctx["recommended_strategies"].append("Add rollback path for irreversible ops.")
                if "optimize" in tags:
                    ctx["recommended_strategies"].append("Add optimize_performance guarded loop.")
                if links.get("plan_uri"):
                    ctx["examples"].append({
                    "mission_id": r.get("mission_id"),
                    "plan_uri": links["plan_uri"],
                    })
            # dedup + limit
            dedup = []
            seen = set()
            for t in ctx["recommended_strategies"]:
                if t not in seen:
                    seen.add(t)
                    dedup.append(t)
            ctx["recommended_strategies"] = dedup[:6]
            return ctx
        except Exception as e:
            process_log(f"[MEMORY] Vertex ctx skipped: {e}")
            return {"recommended_strategies": [], "common_pitfalls": [], "examples": []}
    
    #koniec poprawki
    
    
    
    def add_successful_plan(self, plan: Dict[str, Any], mission: str, metadata: Dict[str, Any]):
        """
        Zapisuje ‚Äûudany plan‚Äù lokalnie (folder per misja) oraz aktualizuje proste ‚Äûbest practices‚Äù.
        To wo≈Ça orchestrator po wyciƒÖgniƒôciu finalnego planu.
        """
        try:
            os.makedirs("memory/success", exist_ok=True)

            # id misji z misji (sp√≥jne z save_complete_mission)
            from datetime import datetime as _dt
            import hashlib as _h
            ts = _dt.now().strftime("%Y%m%d_%H%M%S")
            h = _h.md5(mission.encode("utf-8")).hexdigest()[:8]
            mission_id = f"mission_{ts}_{h}"

            # folder per misja
            mission_dir = os.path.join("memory", "success", mission_id)
            os.makedirs(mission_dir, exist_ok=True)

            # zapis planu i meta
            with open(os.path.join(mission_dir, f"{mission_id}.plan.json"), "w", encoding="utf-8") as f:
                json.dump(plan, f, ensure_ascii=False, indent=2)
            payload = {
                "mission": mission,
                "metadata": metadata or {},
                "saved_at": _dt.now().isoformat(),
            }
            with open(os.path.join(mission_dir, f"{mission_id}.meta.json"), "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)

            # bardzo proste ‚Äûbest practices‚Äù (do wykorzystania przez get_relevant_context)
            node_types = {str(n.get("implementation") or n.get("name") or "").lower()
                          for n in (plan.get("nodes") or [])}
            tiplist = []
            if "error_handler" in node_types:
                tiplist.append("Always include a dedicated error_handler with routing to notify/report.")
            if "rollback" in node_types:
                tiplist.append("Keep rollback path for any irreversible change.")
            if "validate_data" in node_types:
                tiplist.append("Gate the main path by validate_data with measurable thresholds.")
            if "validate_model" in node_types and "train_model" in node_types:
                tiplist.append("Retrain only after drift detection; compare and rollback on regression.")
            if "optimize_performance" in node_types:
                tiplist.append("Use optimize_performance with loop guards to avoid infinite cycles.")

            if tiplist:
                self.successful_strategies.append({"mission_id": mission_id, "tip": tiplist[0]})

            process_log(f"[MEMORY] add_successful_plan saved under {mission_dir}")
        except Exception as e:
            process_log(f"[MEMORY ERROR] add_successful_plan failed: {e}")

    
    
    #koniec poprawek
        
        
        
        
    def _clean_agent_content(
        self, content: str
    ) -> any:  # Zmieniamy typ zwracany na 'any'
        """
        Usuwa bloki kodu markdown i parsuje wewnƒôtrzny JSON,
        je≈õli to mo≈ºliwe.
        """
        if not isinstance(content, str):
            return content

        cleaned_content = content.strip()

        # Krok 1: Usu≈Ñ bloki kodu markdown (tak jak poprzednio)
        pattern = r"```(?:json)?\s*(.*?)\s*```"
        match = re.search(pattern, cleaned_content, re.DOTALL)
        if match:
            cleaned_content = match.group(1).strip()

        # === NOWY, KLUCZOWY KROK ===
        # Krok 2: Spr√≥buj sparsowaƒá string jako JSON
        try:
            # Je≈õli siƒô uda, zwr√≥ƒá prawdziwy obiekt (s≈Çownik)
            return json.loads(cleaned_content)
        except json.JSONDecodeError:
            # Je≈õli to nie jest JSON, zwr√≥ƒá po prostu oczyszczony tekst
            return cleaned_content

    def _gcs_path(self, relative: str) -> str:
        """Buduje pe≈ÇnƒÖ ≈õcie≈ºkƒô do pliku w GCS z uwzglƒôdnieniem prefiksu"""
        if self.gcs_prefix:
            return f"{self.gcs_prefix}/{relative}"
        return relative

    def _learn_from_success(self, mission_record: Dict):
        """Ekstraktuje i zapisuje PRAWDZIWE wzorce z udanej misji"""

        # 1. Zapisz wzorzec sukcesu dla tego typu misji
        pattern_key = f"success_pattern_{mission_record['mission_type']}"

        if pattern_key not in self.learned_patterns:
            self.learned_patterns[pattern_key] = {
                "occurrences": 0,
                "examples": [],
                "common_elements": {},
                "avg_score": 0,
                "best_practices": [],
            }

        # 2. Aktualizuj statystyki
        pattern = self.learned_patterns[pattern_key]
        pattern["occurrences"] += 1
        current_score = mission_record.get("final_score", 0)
        pattern["avg_score"] = (
            pattern["avg_score"] * (pattern["occurrences"] - 1) + current_score
        ) / pattern["occurrences"]

        # 3. Znajd≈∫ kluczowe elementy sukcesu
        success_elements = []

        # Sprawd≈∫ co by≈Ço w tym planie
        plan = mission_record.get("final_plan", {})
        nodes = plan.get("nodes", [])

        # Zapisz kt√≥re wƒôz≈Çy by≈Çy u≈ºyte
        node_types = [n.get("implementation") for n in nodes]

        if "error_handler" in node_types:
            success_elements.append("comprehensive_error_handling")
        if "rollback" in node_types:
            success_elements.append("rollback_mechanism")
        if "validate_data" in node_types:
            success_elements.append("data_validation")
        if "optimize_performance" in node_types:
            success_elements.append("performance_optimization")

        # 4. Znajd≈∫ unikalne innowacje z tej misji
        if "Adaptive_Router" in str(nodes):
            success_elements.append("adaptive_routing")

        # 5. Zapisz jako best practice je≈õli score > 90
        if current_score > 90:
            best_practice = {
                "mission_id": mission_record["memory_id"],
                "score": current_score,
                "key_success_factors": success_elements,
                "node_count": len(nodes),
                "complexity": mission_record["performance_metrics"].get(
                    "convergence_rate", 0
                ),
            }
            pattern["best_practices"].append(best_practice)

        # 6. Zaktualizuj common_elements (co wystƒôpuje najczƒô≈õciej)
        for element in success_elements:
            if element not in pattern["common_elements"]:
                pattern["common_elements"][element] = 0
            pattern["common_elements"][element] += 1

        # 7. Dodaj przyk≈Çad
        pattern["examples"].append(
            {
                "mission_prompt": mission_record["mission_prompt"],
                "success_factors": success_elements,
                "score": current_score,
            }
        )

        process_log(
            f"[MEMORY] Learned from success: {pattern_key}, "
            f"occurrences={pattern['occurrences']}, "
            f"avg_score={pattern['avg_score']:.2f}"
        )

    def export_temporal_report(self, filepath: str = "memory/temporal_patterns.json"):
        """Eksportuje raport wzorc√≥w czasowych"""
        patterns = self.analyze_temporal_patterns()

        report = {
            "generated_at": datetime.now().isoformat(),
            "total_missions": len(self.full_mission_records),
            "patterns": patterns,
            "insights": [],
        }

        # Znajd≈∫ najlepszy/najgorszy czas
        best_day = max(
            patterns["by_weekday"].items(), key=lambda x: x[1].get("avg_score", 0)
        )
        worst_day = min(
            patterns["by_weekday"].items(), key=lambda x: x[1].get("avg_score", 100)
        )

        report["insights"].append(
            f"Best day: {best_day[0]} (avg: {best_day[1]['avg_score']:.1f})"
        )
        report["insights"].append(
            f"Worst day: {worst_day[0]} (avg: {worst_day[1]['avg_score']:.1f})"
        )

        with open(filepath, "w") as f:
            json.dump(report, f, indent=2)

        return report

    # ------
    def save_complete_mission(
        self,
        mission: str,
        final_plan: Dict,
        all_messages: List[Dict],
        orchestrator_state: Dict,
    ) -> str:
        """
        Zapisuje KOMPLETNY rekord misji do OSOBNEGO pliku JSON
        """
        from datetime import datetime
        import hashlib

        # Generuj unikalne ID
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        mission_hash = hashlib.md5(mission.encode()).hexdigest()[:8]
        mission_id = f"mission_{timestamp}_{mission_hash}"

        cleaned_messages = []
        for msg in all_messages:
            new_msg = msg.copy()  # Kopiujemy, aby nie modyfikowaƒá orygina≈Çu
            if "content" in new_msg:
                new_msg["content"] = self._clean_agent_content(new_msg["content"])
            cleaned_messages.append(new_msg)

        # Ekstraktuj kluczowe informacje z transcript
        iterations_data = self._extract_iterations_from_transcript(cleaned_messages)

        # Klasyfikuj misjƒô i tagi
        mission_type = self._classify_mission(mission)
        tags = self._extract_tags(mission, final_plan)

        # Znajd≈∫ krytyczne momenty w debacie
        critical_moments = self._identify_critical_moments(all_messages)

        # Przygotuj pe≈Çny rekord
        mission_record = {
            # === METADATA ===
            "memory_id": mission_id,
            "timestamp": datetime.now().isoformat(),
            "mission_prompt": mission,
            "mission_type": mission_type,
            "tags": tags,
            # === OUTCOME ===
            "outcome": "Success" if final_plan else "Failed",
            "total_iterations": orchestrator_state.get("iteration_count", 0),
            "total_messages": len(all_messages),
            "time_taken_seconds": orchestrator_state.get("execution_time", 0),
            # === FINAL ARTIFACTS ===
            "final_plan": final_plan,
            "final_score": self._extract_final_score(all_messages),
            # === ITERATION DETAILS ===
            "iterations": iterations_data,
            # === KEY INSIGHTS ===
            "critique_evolution": self._track_critique_evolution(iterations_data),
            "aggregator_reasoning": self._extract_aggregator_reasoning(all_messages),
            "proposer_contributions": self._analyze_proposer_contributions(
                all_messages
            ),
            # === LEARNING DATA ===
            "llm_generated_summary": self._generate_mission_summary(
                all_messages, final_plan
            ),
            "identified_patterns": self._extract_patterns_from_debate(all_messages),
            "success_factors": self._identify_success_factors(
                final_plan, iterations_data
            ),
            "failure_points": self._identify_failure_points(iterations_data),
            # === CRITICAL MOMENTS ===
            "critical_moments": critical_moments,
            "turning_points": self._identify_turning_points(iterations_data),
            # === FULL TRANSCRIPT ===
            "full_transcript": cleaned_messages,
            # === METRICS ===
            "performance_metrics": {
                "token_usage": orchestrator_state.get("total_tokens", 0),
                "api_calls": orchestrator_state.get("api_calls", 0),
                "convergence_rate": self._calculate_convergence_rate(iterations_data),
            },
        }

        # NOWA CZƒò≈öƒÜ - Zapisz do osobnego pliku
        # --- Zapis misji: GCS je≈õli skonfigurowany, inaczej lokalnie ---
        if getattr(self, "use_gcs", False) and getattr(self, "gcs_bucket", None):
            # Czytelna nazwa: YYYY/MM/DD/{YYYYMMDD_HHMMSS}-{slug}-{hash}.json
            slug = _slugify(mission)  # helper poza klasƒÖ
            ts_date, ts_time = timestamp.split("_")  # np. 20250829, 212413
            y, m, d = ts_date[:4], ts_date[4:6], ts_date[6:8]
            mission_blob_rel = (
                f"missions/{y}/{m}/{d}/{ts_date}_{ts_time}-{slug}-{mission_hash}.json"
            )
            mission_blob = self._gcs_path(mission_blob_rel)  # helper w klasie

            try:
                _gcs_upload_json(
                    self.gcs_bucket, mission_blob, mission_record
                )  # helper poza klasƒÖ
                process_log(
                    f"[MEMORY] Saved mission to GCS: gs://{self.gcs_bucket}/{mission_blob}"
                )
            except Exception as e:
                process_log(f"[MEMORY ERROR] Failed to save mission to GCS: {e}")

            # Lekki indeks: 1 plik per misja (≈Çatwe listowanie prefixem)
            index_entry = {
                "mission_id": mission_id,
                "gcs_path": f"gs://{self.gcs_bucket}/{mission_blob}",
                "timestamp": mission_record.get("timestamp"),
                "mission_prompt": mission_record.get("mission_prompt", "")[:100],
                "mission_type": mission_record.get("mission_type"),
                "final_score": mission_record.get("final_score"),
                "tags": mission_record.get("tags", []),
                "outcome": mission_record.get("outcome"),
            }
            index_blob = self._gcs_path(f"index/{mission_id}.json")
            try:
                _gcs_upload_json(self.gcs_bucket, index_blob, index_entry)
            except Exception as e:
                process_log(f"[MEMORY ERROR] Failed to save index to GCS: {e}")

        else:
            # dotychczasowy zapis lokalny + lokalny indeks
            mission_dir = "memory/missions"
            os.makedirs(mission_dir, exist_ok=True)
            mission_file = os.path.join(mission_dir, f"{mission_id}.json")

            try:
                with open(mission_file, "w", encoding="utf-8") as f:
                    json.dump(mission_record, f, ensure_ascii=False, indent=2)
                process_log(f"[MEMORY] Saved mission to file: {mission_file}")
            except Exception as e:
                process_log(f"[MEMORY ERROR] Failed to save mission file: {e}")

            self._update_mission_index(mission_id, mission_file, mission_record)

        # Nadal zapisz do pamiƒôci runtime je≈õli potrzebne
        self.full_mission_records.append(mission_record)
        self.mission_index[mission_id] = len(self.full_mission_records) - 1

        if final_plan:
            self._learn_from_success(mission_record)

        # Aktualizuj wzorce czasowe co 5 misji
        if len(self.full_mission_records) % 5 == 0:
            patterns = self.analyze_temporal_patterns()
            process_log(
                f"[MEMORY] Temporal patterns update: {len(patterns['by_weekday'])} weekdays analyzed"
            )

        # Persist tylko patterns i strategies
        self._persist_lightweight_memory()

        process_log(f"[MEMORY] Saved complete mission: {mission_id}")
        return mission_id

    def _update_mission_index(self, mission_id: str, file_path: str, record: Dict):
        """Aktualizuje lekki indeks wszystkich misji"""
        index_file = "memory/mission_index.json"

        try:
            if os.path.exists(index_file):
                with open(index_file, "r", encoding="utf-8") as f:
                    index = json.load(f)
            else:
                index = {"missions": [], "metadata": {}}
        except:
            index = {"missions": [], "metadata": {}}

        # Dodaj wpis do indeksu
        index_entry = {
            "mission_id": mission_id,
            "file_path": file_path,
            "timestamp": record.get("timestamp"),
            "mission_prompt": record.get("mission_prompt", "")[:100],
            "mission_type": record.get("mission_type"),
            "final_score": record.get("final_score"),
            "tags": record.get("tags", []),
            "outcome": record.get("outcome"),
        }

        index["missions"].append(index_entry)
        index["metadata"]["last_updated"] = datetime.now().isoformat()
        index["metadata"]["total_missions"] = len(index["missions"])

        with open(index_file, "w", encoding="utf-8") as f:
            json.dump(index, f, ensure_ascii=False, indent=2)

    def _persist_lightweight_memory(self):
        """Zapisuje tylko patterns i strategies (bez pe≈Çnych rekord√≥w misji)"""
        data = {
            "patterns": self.learned_patterns,
            "strategies": self.successful_strategies,
            "metadata": {
                "last_updated": datetime.now().isoformat(),
                "mission_count": len(self.full_mission_records),
            },
        }

        if self.use_gcs:
            try:
                blob = self._gcs_path("learned_strategies.json")
                _gcs_upload_json(self.gcs_bucket, blob, data)
                process_log(
                    f"[MEMORY] Saved lightweight memory to GCS: gs://{self.gcs_bucket}/{blob}"
                )
                return
            except Exception as e:
                process_log(
                    f"[MEMORY ERROR] Failed to save lightweight memory to GCS: {e}"
                )

        # fallback/local
        os.makedirs("memory", exist_ok=True)
        memory_file = "memory/learned_strategies.json"
        try:
            with open(memory_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö† Nie uda≈Ço siƒô zapisaƒá pamiƒôci: {e}")

    def load_specific_mission(self, mission_id: str) -> Optional[Dict]:
        """≈Åaduje konkretnƒÖ misjƒô z pliku"""
        mission_file = f"memory/missions/{mission_id}.json"

        if os.path.exists(mission_file):
            try:
                with open(mission_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                process_log(f"[MEMORY ERROR] Cannot load mission {mission_id}: {e}")

        return None

    def search_missions(self, query: str, limit: int = 10) -> List[Dict]:
        """Przeszukuje indeks misji bez ≈Çadowania wszystkich plik√≥w"""
        index_file = "memory/mission_index.json"

        if not os.path.exists(index_file):
            return []

        with open(index_file, "r", encoding="utf-8") as f:
            index = json.load(f)

        results = []
        query_lower = query.lower()

        for entry in index["missions"]:
            # Proste wyszukiwanie tekstowe
            if (
                query_lower in entry.get("mission_prompt", "").lower()
                or query_lower in entry.get("mission_type", "").lower()
                or any(query_lower in tag.lower() for tag in entry.get("tags", []))
            ):

                results.append(entry)
                if len(results) >= limit:
                    break

        return results

    # ------

    #     def save_complete_mission(self,
    #                             mission: str,
    #                             final_plan: Dict,
    #                             all_messages: List[Dict],
    #                             orchestrator_state: Dict) -> str:
    #         """
    #         Zapisuje KOMPLETNY rekord misji z wszystkimi danymi
    #         """
    #         from datetime import datetime
    #         import hashlib

    #         # Generuj unikalne ID
    #         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    #         mission_hash = hashlib.md5(mission.encode()).hexdigest()[:8]
    #         mission_id = f"mission_{timestamp}_{mission_hash}"

    #         # Ekstraktuj kluczowe informacje z transcript
    #         iterations_data = self._extract_iterations_from_transcript(all_messages)

    #         # Klasyfikuj misjƒô i tagi
    #         mission_type = self._classify_mission(mission)
    #         tags = self._extract_tags(mission, final_plan)

    #         # Znajd≈∫ krytyczne momenty w debacie
    #         critical_moments = self._identify_critical_moments(all_messages)

    #         # Przygotuj pe≈Çny rekord
    #         mission_record = {
    #             # === METADATA ===
    #             "memory_id": mission_id,
    #             "timestamp": datetime.now().isoformat(),
    #             "mission_prompt": mission,
    #             "mission_type": mission_type,
    #             "tags": tags,

    #             # === OUTCOME ===
    #             "outcome": "Success" if final_plan else "Failed",
    #             "total_iterations": orchestrator_state.get("iteration_count", 0),
    #             "total_messages": len(all_messages),
    #             "time_taken_seconds": orchestrator_state.get("execution_time", 0),

    #             # === FINAL ARTIFACTS ===
    #             "final_plan": final_plan,
    #             "final_score": self._extract_final_score(all_messages),

    #             # === ITERATION DETAILS ===
    #             "iterations": iterations_data,

    #             # === KEY INSIGHTS ===
    #             "critique_evolution": self._track_critique_evolution(iterations_data),
    #             "aggregator_reasoning": self._extract_aggregator_reasoning(all_messages),
    #             "proposer_contributions": self._analyze_proposer_contributions(all_messages),

    #             # === LEARNING DATA ===
    #             "llm_generated_summary": self._generate_mission_summary(all_messages, final_plan),
    #             "identified_patterns": self._extract_patterns_from_debate(all_messages),
    #             "success_factors": self._identify_success_factors(final_plan, iterations_data),
    #             "failure_points": self._identify_failure_points(iterations_data),

    #             # === CRITICAL MOMENTS ===
    #             "critical_moments": critical_moments,
    #             "turning_points": self._identify_turning_points(iterations_data),

    #             # === FULL TRANSCRIPT ===
    #             "full_transcript": all_messages,  # Kompletny zapis

    #             # === METRICS ===
    #             "performance_metrics": {
    #                 "token_usage": orchestrator_state.get("total_tokens", 0),
    #                 "api_calls": orchestrator_state.get("api_calls", 0),
    #                 "convergence_rate": self._calculate_convergence_rate(iterations_data)
    #             }
    #         }

    #         # Zapisz do pamiƒôci
    #         self.full_mission_records.append(mission_record)
    #         self.mission_index[mission_id] = len(self.full_mission_records) - 1

    #         if final_plan:  # Je≈õli misja siƒô uda≈Ça
    #             self._learn_from_success(mission_record)

    #         if len(self.full_mission_records) % 5 == 0:
    #             patterns = self.analyze_temporal_patterns()
    #             process_log(f"[MEMORY] Temporal patterns update: {len(patterns['by_weekday'])} weekdays analyzed")

    #         # Persist immediately
    #         self._persist_full_memory()

    #         process_log(f"[MEMORY] Saved complete mission: {mission_id}")
    #         return mission_id

    def _extract_iterations_from_transcript(self, messages: List[Dict]) -> List[Dict]:
        """Ekstraktuje dane ka≈ºdej iteracji z transkryptu"""
        iterations = []
        current_iteration = {"proposers": [], "aggregator": None, "critic": None}

        for msg in messages:
            role = msg.get("name", "").lower()

            if "proposer" in role or "analyst" in role or "planner" in role:
                current_iteration["proposers"].append(
                    {
                        "agent": msg.get("name"),
                        "content": msg.get("content"),
                        "key_ideas": self._extract_key_ideas(msg.get("content", "")),
                    }
                )

            elif "aggregator" in role:
                current_iteration["aggregator"] = {
                    "content": msg.get("content"),
                    "synthesis": self._extract_synthesis(msg.get("content", "")),
                }

            elif "critic" in role:
                current_iteration["critic"] = {
                    "content": msg.get("content"),
                    "verdict": self._extract_verdict(msg.get("content", "")),
                    "score": self._extract_score(msg.get("content", "")),
                    "weaknesses": self._extract_weaknesses(msg.get("content", "")),
                }

                # Koniec iteracji - zapisz i zacznij nowƒÖ
                if current_iteration["proposers"]:
                    iterations.append(current_iteration)
                    current_iteration = {
                        "proposers": [],
                        "aggregator": None,
                        "critic": None,
                    }

        return iterations

    def _generate_mission_summary(self, messages: List[Dict], final_plan: Dict) -> str:
        """Generuje BOGATE podsumowanie misji"""
        summary_parts = []

        # 1. Liczba iteracji i czas
        iteration_count = sum(
            1 for m in messages if "critic" in m.get("name", "").lower()
        )
        summary_parts.append(f"Misja zako≈Ñczona w {iteration_count} iteracji.")

        # 2. Kluczowe innowacje (szukaj w transkrypcie)
        innovations = set()
        for msg in messages:

            content_str = str(msg.get("content", ""))
            content = content_str.lower()

            if "adaptive" in content or "adaptacyjny" in content:
                innovations.add("adaptive routing")
            if "rollback" in content:
                innovations.add("rollback mechanism")
            if "optimiz" in content or "optymali" in content:
                innovations.add("optimization")

        if innovations:
            summary_parts.append(f"Zastosowano: {', '.join(innovations)}.")

        # 3. Analiza struktury planu
        if final_plan:
            nodes = final_plan.get("nodes", [])
            edges = final_plan.get("edges", [])

            # Policz typy ≈õcie≈ºek
            success_paths = len(
                [e for e in edges if e.get("condition") == "on_success"]
            )
            failure_paths = len(
                [e for e in edges if e.get("condition") == "on_failure"]
            )

            summary_parts.append(
                f"Struktura: {len(nodes)} wƒôz≈Ç√≥w, "
                f"{success_paths} ≈õcie≈ºek sukcesu, "
                f"{failure_paths} ≈õcie≈ºek obs≈Çugi b≈Çƒôd√≥w."
            )

            # Znajd≈∫ kluczowe wƒôz≈Çy
            key_nodes = []
            for node in nodes:
                impl = node.get("implementation", "")
                if impl in [
                    "error_handler",
                    "rollback",
                    "validate_data",
                    "optimize_performance",
                ]:
                    key_nodes.append(impl)

            if key_nodes:
                summary_parts.append(
                    f"Kluczowe komponenty: {', '.join(set(key_nodes))}."
                )

        # 4. Ko≈Ñcowy verdykt
        for msg in reversed(messages):
            if "critic" in msg.get("name", "").lower() and "ZATWIERDZONY" in msg.get(
                "content", ""
            ):
                summary_parts.append("Plan zatwierdzony przez krytyka bez zastrze≈ºe≈Ñ.")
                break

        return " ".join(summary_parts)

    def _extract_tags(self, mission: str, final_plan: Dict) -> List[str]:
        """Automatycznie taguje misjƒô"""
        tags = []
        mission_lower = mission.lower()

        # Mission-based tags
        tag_keywords = {
            "error_handling": ["error", "b≈Çƒôd", "obs≈Çuga", "handler"],
            "optimization": ["optym", "performance", "wydajno≈õƒá"],
            "causality": ["causal", "przyczyn"],
            "validation": ["valid", "walidac"],
            "retry": ["retry", "ponow"],
            "rollback": ["rollback", "cofn"],
            "ml": ["model", "train", "uczenie"],
            "data": ["data", "dane", "csv", "pipeline"],
        }

        for tag, keywords in tag_keywords.items():
            if any(kw in mission_lower for kw in keywords):
                tags.append(tag)

        # Plan-based tags
        if final_plan:
            nodes_str = str(final_plan.get("nodes", []))
            if "error_handler" in nodes_str:
                tags.append("robust")
            if "optimize" in nodes_str:
                tags.append("optimized")

        return list(set(tags))  # Unique tags

    # ------------
    def _load_persistent_memory(self):
        """≈Åaduje pamiƒôƒá - patterns z g≈Ç√≥wnego pliku, misje z indeksu"""
        json_file = "memory/learned_strategies.json"

        # Za≈Çaduj patterns i strategies
        if os.path.exists(json_file):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                self.learned_patterns = data.get("patterns", {})
                self.successful_strategies = data.get("strategies", [])
            except Exception as e:
                print(f"‚ö† Nie uda≈Ço siƒô za≈Çadowaƒá pamiƒôci: {e}")

        # Za≈Çaduj listƒô misji z indeksu
        index_file = "memory/mission_index.json"
        if os.path.exists(index_file):
            try:
                with open(index_file, "r", encoding="utf-8") as f:
                    index = json.load(f)

                # Za≈Çaduj ostatnie 10 misji do pamiƒôci runtime
                recent_missions = index["missions"][-10:] if "missions" in index else []
                for entry in recent_missions:
                    if "file_path" in entry and os.path.exists(entry["file_path"]):
                        with open(entry["file_path"], "r", encoding="utf-8") as f:
                            record = json.load(f)
                            self.full_mission_records.append(record)
                            self.mission_index[entry["mission_id"]] = (
                                len(self.full_mission_records) - 1
                            )

                print(f"‚úî Za≈Çadowano {len(self.full_mission_records)} ostatnich misji")
            except Exception as e:
                print(f"‚ö† Problem z indeksem misji: {e}")
        else:
            print("üîç Tworzƒô nowƒÖ pamiƒôƒá (brak istniejƒÖcego indeksu)")

        os.makedirs("memory/missions", exist_ok=True)

    # ------------

    #     def _load_persistent_memory(self):
    #         """
    #         ≈Åaduje pamiƒôƒá z pliku JSON
    #         """
    #         json_file = "memory/learned_strategies.json"

    #         if os.path.exists(json_file):
    #             try:
    #                 with open(json_file, "r", encoding="utf-8") as f:
    #                     data = json.load(f)
    #                 self.learned_patterns = data.get("patterns", {})
    #                 self.successful_strategies = data.get("strategies", [])

    #                 # Za≈Çaduj te≈º nowe full_mission_records je≈õli istniejƒÖ
    #                 if "full_mission_records" in data:
    #                     self.full_mission_records = data["full_mission_records"]
    #                     # Odbuduj index
    #                     for i, record in enumerate(self.full_mission_records):
    #                         self.mission_index[record["memory_id"]] = i

    #                 print(f"‚úî Za≈Çadowano pamiƒôƒá: {len(self.successful_strategies)} strategies, {len(self.full_mission_records)} full records")
    #             except Exception as e:
    #                 print(f"‚ö† Nie uda≈Ço siƒô za≈Çadowaƒá pamiƒôci: {e}")
    #         else:
    #             print("üìù Tworzƒô nowƒÖ pamiƒôƒá (brak istniejƒÖcego pliku)")
    #             os.makedirs("memory", exist_ok=True)

    #     def _persist_memory(self):
    #         """
    #         Zapisuje pamiƒôƒá do pliku JSON
    #         """
    #         os.makedirs("memory", exist_ok=True)
    #         memory_file = "memory/learned_strategies.json"

    #         data = {
    #             "patterns": self.learned_patterns,
    #             "strategies": self.successful_strategies,
    #             "full_mission_records": self.full_mission_records  # NOWE!
    #         }

    #         try:
    #             with open(memory_file, "w", encoding="utf-8") as f:
    #                 json.dump(data, f, ensure_ascii=False, indent=2)
    #         except Exception as e:
    #             print(f"‚ö† Nie uda≈Ço siƒô zapisaƒá pamiƒôci: {e}")

    def _persist_full_memory(self):
        """Alias dla _persist_memory"""
        # self._persist_memory()
        self._persist_lightweight_memory()

    # def _extract_key_ideas(self, content: str) -> List[str]:
    #     """Ekstraktuje kluczowe pomys≈Çy z contentu"""
    #     # Prosta heurystyka - mo≈ºesz ulepszyƒá
    #     ideas = []
    #     if "error_handler" in content.lower():
    #         ideas.append("error_handling")
    #     if "rollback" in content.lower():
    #         ideas.append("rollback_mechanism")
    #     if "optimiz" in content.lower():
    #         ideas.append("optimization")
    #     return ideas

    
    
    def _extract_key_ideas(self, content):
        """
        Zwraca listƒô 'idei' (s≈Ç√≥w-kluczy) wykrytych w tre≈õci.
        Tre≈õƒá mo≈ºe byƒá stringiem albo dict-em (np. plan/odpowied≈∫ krytyka).
        """
        ideas = set()

        # Gdy dostajemy JSON-owy plan/krytykƒô (dict) ‚Äî wyciƒÖgnijmy sygna≈Çy bez tekstu
        if isinstance(content, dict):
            # 1) plan: nodes/edges ‚Üí dorzuƒá implementacje/nazwy
            nodes = content.get("nodes") \
                or content.get("final_synthesized_plan", {}).get("nodes") \
                or content.get("final_plan", {}).get("nodes") \
                or []
            if isinstance(nodes, list):
                for n in nodes:
                    if isinstance(n, dict):
                        impl = n.get("implementation") or n.get("name")
                        if impl:
                            ideas.add(str(impl).lower())

            # 2) krytyka: weaknesses ‚Üí dorzuƒá nazwy s≈Çabo≈õci
            cs = content.get("critique_summary") or {}
            for w in (cs.get("identified_weaknesses") or []):
                if isinstance(w, dict):
                    wk = w.get("weakness")
                    if wk:
                        ideas.add(str(wk).lower())

            # 3) przejd≈∫ do heurystyk tekstowych na zserializowanym stringu
            text = _to_text_safe(content)
        else:
            text = _to_text_safe(content)

        t = text.lower()

        # Heurystyki ‚Äî jak wcze≈õniej, ale dzia≈ÇajƒÖ na stringu
        if "error_handler" in t:
            ideas.add("error_handler")
        if "rollback" in t:
            ideas.add("rollback")
        if "retry" in t or "pon√≥w" in t or "ponowienie" in t or "backoff" in t:
            ideas.add("retry")
        if "optimiz" in t or "optymaliz" in t:
            ideas.add("optimize")
        if "validate" in t or "walidac" in t:
            ideas.add("validate_data")
        if "clean" in t or "czyszczen" in t:
            ideas.add("clean_data")
        if "load" in t or "wczyt" in t:
            ideas.add("load_data")

        return sorted(i for i in ideas if i)

    
    def _extract_synthesis(self, content: str) -> str:
        """Ekstraktuje syntezƒô z odpowiedzi aggregatora"""
        # Szukaj "synthesis_reasoning" w JSON
        try:
            data = json.loads(content) if isinstance(content, str) else content
            return data.get("synthesis_reasoning", "")
        except:
            return ""

    def _extract_verdict(self, content: str) -> str:
        """Ekstraktuje werdykt z odpowiedzi krytyka"""
        content_str = str(content)

        if "ZATWIERDZONY" in content_str:
            return "ZATWIERDZONY"
        return "ODRZUCONY"

    def _extract_score(self, content: str) -> float:
        """Ekstraktuje score z odpowiedzi krytyka"""
        content_str = str(content)
        try:
            import re

            score_match = re.search(r'"Overall_Quality_Q":\s*([\d.]+)', content_str)
            if score_match:
                return float(score_match.group(1))
        except:
            pass
        return 0.0

    def _extract_weaknesses(self, content: str) -> List[str]:
        """Ekstraktuje weaknesses z odpowiedzi krytyka"""
        weaknesses = []
        try:
            if isinstance(content, dict):
                data = content
            else:  # Je≈õli nie, spr√≥buj sparsowaƒá go jako JSON
                data = json.loads(str(content))

            weak_list = data.get("critique_summary", {}).get(
                "identified_weaknesses", []
            )
            for w in weak_list:
                if isinstance(w, dict):
                    weaknesses.append(w.get("weakness", ""))
                else:
                    weaknesses.append(str(w))
        except:
            pass
        return weaknesses

    def add_successful_plan(self, plan: Dict[str, Any], mission: str, metadata: Dict):
        """Zapisuje udany plan do pamiƒôci proceduralnej"""
        strategy = {
            "mission_type": self._classify_mission(mission),
            "plan_structure": self._extract_plan_structure(plan),
            "success_factors": metadata.get("success_factors", []),
            "performance_metrics": metadata.get("metrics", {}),
            "timestamp": datetime.now().isoformat(),
        }

        self.successful_strategies.append(strategy)
        self._persist_lightweight_memory()  # Zapisz od razu

        # Loguj dodanie udanego planu
        process_log(
            f"[MEMORY] Added successful plan for mission_type={strategy['mission_type']}, "
            f"nodes={strategy['plan_structure']['num_nodes']}"
        )

    def _classify_mission(self, mission: str) -> str:
        """Klasyfikuje typ misji"""
        mission_lower = mission.lower()

        if "przyczynow" in mission_lower or "causal" in mission_lower:
            return "causal_analysis"
        elif (
            "dane" in mission_lower or "data" in mission_lower or "csv" in mission_lower
        ):
            return "data_processing"
        elif "model" in mission_lower:
            return "model_validation"
        elif "optymali" in mission_lower:
            return "optimization"
        else:
            return "general"

    def _extract_plan_structure(self, plan: Dict) -> Dict:
        """Ekstraktuje strukturalne cechy planu"""
        return {
            "num_nodes": len(plan.get("nodes", [])),
            "num_edges": len(plan.get("edges", [])),
            "has_error_handling": any(
                "error" in str(node).lower() for node in plan.get("nodes", [])
            ),
            "has_validation": any(
                "valid" in str(node).lower() for node in plan.get("nodes", [])
            ),
            "graph_complexity": self._calculate_complexity(plan),
        }

    def _calculate_complexity(self, plan: Dict) -> float:
        """Oblicza z≈Ço≈ºono≈õƒá grafu"""
        nodes = len(plan.get("nodes", []))
        edges = len(plan.get("edges", []))

        if nodes == 0:
            return 0.0

        # Z≈Ço≈ºono≈õƒá cyklomatyczna aproksymowana
        return (edges - nodes + 2) / nodes

    def _identify_critical_moments(self, messages: List[Dict]) -> List[Dict]:
        """Identyfikuje krytyczne momenty w debacie"""
        critical = []
        for i, msg in enumerate(messages):
            content_str = str(msg.get("content", ""))  # Konwertujemy na string
            content = content_str.lower()

            # Moment krytyczny = du≈ºa zmiana w score lub verdict
            if "zatwierdzony" in content or "odrzucony" in content:
                critical.append(
                    {
                        "index": i,
                        "type": "verdict",
                        "agent": msg.get("name"),
                        "summary": "Decyzja krytyka",
                    }
                )
        return critical

    def _extract_final_score(self, messages: List[Dict]) -> float:
        """Znajduje finalny score z ostatniej odpowiedzi krytyka"""
        for msg in reversed(messages):
            if "critic" in msg.get("name", "").lower():
                score = self._extract_score(msg.get("content", ""))
                if score > 0:
                    return score
        return 0.0

    def _track_critique_evolution(self, iterations: List[Dict]) -> List[Dict]:
        """≈öledzi jak zmienia≈Ça siƒô krytyka miƒôdzy iteracjami"""
        evolution = []
        for i, iteration in enumerate(iterations):
            if iteration.get("critic"):
                evolution.append(
                    {
                        "iteration": i,
                        "score": iteration["critic"].get("score", 0),
                        "verdict": iteration["critic"].get("verdict", ""),
                        "main_issues": iteration["critic"].get("weaknesses", [])[:2],
                    }
                )
        return evolution

    def _extract_aggregator_reasoning(self, messages: List[Dict]) -> str:
        """WyciƒÖga reasoning agregatora"""
        for msg in reversed(messages):
            if "aggregator" in msg.get("name", "").lower():
                return self._extract_synthesis(msg.get("content", ""))
        return ""

    def _analyze_proposer_contributions(
        self, messages: List[Dict]
    ) -> Dict[str, List[str]]:
        """Analizuje wk≈Çad ka≈ºdego proposera"""
        contributions = {}
        for msg in messages:
            name = msg.get("name", "")
            if any(role in name.lower() for role in ["analyst", "planner", "proposer"]):
                if name not in contributions:
                    contributions[name] = []
                ideas = self._extract_key_ideas(msg.get("content", ""))
                contributions[name].extend(ideas)
        return contributions

    def _extract_patterns_from_debate(self, messages: List[Dict]) -> List[str]:
        """Ekstraktuje wzorce z ca≈Çej debaty"""
        patterns = []

        all_text = " ".join(str(m.get("content", "")) for m in messages).lower()

        # Szukaj powtarzajƒÖcych siƒô koncept√≥w
        all_text = " ".join(m.get("content", "") for m in messages).lower()

        if all_text.count("error_handler") > 3:
            patterns.append("Czƒôste odniesienia do obs≈Çugi b≈Çƒôd√≥w")
        if all_text.count("rollback") > 2:
            patterns.append("Rollback jako kluczowy element")
        if all_text.count("optimiz") > 2:
            patterns.append("Focus na optymalizacjƒô")

        return patterns

    def _identify_success_factors(
        self, final_plan: Dict, iterations: List[Dict]
    ) -> List[str]:
        """Identyfikuje co przyczyni≈Ço siƒô do sukcesu"""
        factors = []

        if final_plan:
            # Analiza struktury planu
            if any("error" in str(n).lower() for n in final_plan.get("nodes", [])):
                factors.append("Comprehensive error handling")
            if any("valid" in str(n).lower() for n in final_plan.get("nodes", [])):
                factors.append("Data validation steps")

            # Analiza iteracji
            if len(iterations) > 1:
                factors.append(f"Iterative improvement ({len(iterations)} rounds)")

        return factors

    def _identify_failure_points(self, iterations: List[Dict]) -> List[Dict]:
        """Identyfikuje gdzie by≈Çy problemy"""
        failures = []
        for i, iteration in enumerate(iterations):
            if iteration.get("critic", {}).get("verdict") == "ODRZUCONY":
                failures.append(
                    {
                        "iteration": i,
                        "issues": iteration["critic"].get("weaknesses", []),
                        "score": iteration["critic"].get("score", 0),
                    }
                )
        return failures

    def _identify_turning_points(self, iterations: List[Dict]) -> List[Dict]:
        """Znajduje punkty zwrotne w debacie"""
        turning_points = []
        prev_score = 0

        for i, iteration in enumerate(iterations):
            curr_score = iteration.get("critic", {}).get("score", 0)
            if curr_score - prev_score > 20:  # Du≈ºy skok w score
                turning_points.append(
                    {
                        "iteration": i,
                        "score_jump": curr_score - prev_score,
                        "reason": "Significant improvement",
                    }
                )
            prev_score = curr_score

        return turning_points

    def _calculate_convergence_rate(self, iterations: List[Dict]) -> float:
        """Oblicza jak szybko system doszed≈Ç do rozwiƒÖzania"""
        if not iterations:
            return 0.0

        scores = [it.get("critic", {}).get("score", 0) for it in iterations]
        if len(scores) < 2:
            return 1.0

        # ≈öredni przyrost score na iteracjƒô
        improvements = [scores[i + 1] - scores[i] for i in range(len(scores) - 1)]
        avg_improvement = sum(improvements) / len(improvements) if improvements else 0

        # Normalizuj do 0-1 (im wy≈ºszy przyrost, tym lepsza convergence)
        return min(
            avg_improvement / 20, 1.0
        )  # 20 punkt√≥w na iteracjƒô = max convergence

    def analyze_temporal_patterns(self) -> Dict[str, Any]:
        """Analizuje wzorce czasowe w performance systemu"""
        from datetime import datetime

        patterns = {"by_weekday": {}, "by_hour": {}, "by_day_hour": {}}

        if not self.full_mission_records:
            return patterns

        # Analiza per dzie≈Ñ tygodnia
        for record in self.full_mission_records:
            timestamp = datetime.fromisoformat(record["timestamp"])
            weekday = timestamp.strftime("%A")
            hour = timestamp.hour
            day_hour = f"{weekday}_{hour:02d}h"

            # Per weekday
            if weekday not in patterns["by_weekday"]:
                patterns["by_weekday"][weekday] = {
                    "missions": [],
                    "avg_score": 0,
                    "avg_iterations": 0,
                    "common_issues": [],
                }

            patterns["by_weekday"][weekday]["missions"].append(record["memory_id"])

            # Per hour
            if hour not in patterns["by_hour"]:
                patterns["by_hour"][hour] = {
                    "missions": [],
                    "avg_score": 0,
                    "avg_iterations": 0,
                }

            patterns["by_hour"][hour]["missions"].append(record["memory_id"])

            # Per day+hour combo
            if day_hour not in patterns["by_day_hour"]:
                patterns["by_day_hour"][day_hour] = {"missions": [], "scores": []}

            patterns["by_day_hour"][day_hour]["missions"].append(record["memory_id"])
            patterns["by_day_hour"][day_hour]["scores"].append(
                record.get("final_score", 0)
            )

        # Oblicz ≈õrednie
        for weekday, data in patterns["by_weekday"].items():
            if data["missions"]:
                scores = [
                    r["final_score"]
                    for r in self.full_mission_records
                    if r["memory_id"] in data["missions"]
                ]
                data["avg_score"] = sum(scores) / len(scores) if scores else 0

        return patterns

    def get_current_context_hints(self) -> str:
        """Zwraca wskaz√≥wki kontekstowe na podstawie aktualnego czasu"""
        from datetime import datetime

        now = datetime.now()
        patterns = self.analyze_temporal_patterns()

        hints = []

        # Sprawd≈∫ wzorce dla aktualnego dnia
        weekday = now.strftime("%A")
        if weekday in patterns["by_weekday"]:
            weekday_data = patterns["by_weekday"][weekday]
            if weekday_data["avg_score"] < 90:
                hints.append(
                    f"Uwaga: {weekday} historycznie majƒÖ ni≈ºsze score ({weekday_data['avg_score']:.1f})"
                )

        # Sprawd≈∫ wzorce dla aktualnej godziny
        hour = now.hour
        if hour in patterns["by_hour"]:
            hour_data = patterns["by_hour"][hour]
            if len(hour_data["missions"]) > 2:  # Je≈õli mamy wystarczajƒÖco danych
                hints.append(
                    f"O godzinie {hour}:00 zazwyczaj wykonywane sƒÖ misje tego typu"
                )

        return " | ".join(hints) if hints else ""



--- FILE: moa_prompts.py ---

"""
Zaawansowane prompty dla systemu MOA z technikami Chain-of-Thought i Self-Consistency
"""

from typing import Dict, Any, List
from config.models_config import AgentRole


class MOAPrompts:
    """Centralna biblioteka prompt√≥w dla systemu MOA"""

    # Uniwersalne zasady dla wszystkich agent√≥w
    UNIVERSAL_PRINCIPLES = """
## UNIVERSAL REASONING & OUTPUT POLICY

1) Deterministic, Structured Reasoning
- Decompose the mission into atomic steps; make dependencies explicit.
- Prefer DAG-like flows with clear success/failure transitions.

2) Output Contract (STRICT)
- Final output MUST be a single valid JSON object (no prose, no code fences, no comments).
- Keys and schema names are in English; user-facing strings are in Polish.
- If you risk exceeding token limits, compress explanations but keep structure intact.

3) Memory & Retrieval Discipline
- When WRITING memory: always store concise English bullet points or JSON objects
  (normalized nouns, present tense, ‚â§200 tokens per write).
- When READING memory: query only what is needed for the current decision.
- Never copy large memory chunks into the output; summarize instead.

4) Robustness by Design
- For each critical step, state the expected preconditions and postconditions.
- Include failure transitions (on_failure) and remediation (retry, rollback, notify).

5) Metrics & Confidence
- Quantify uncertainty (0‚Äì1). Justify with observable signals (e.g., data_quality).
- Prefer measurable thresholds over vague conditions.

6) Tooling Constraints
- Use ONLY nodes present in the node library (exact implementation names).
- Allowed edge.condition values: on_success, on_failure, retry, validated, partial_success,
  needs_optimization, else (as a last-resort catch-all).
"""

    @staticmethod
    def get_proposer_prompt(role: AgentRole, mission: str, node_library: Dict) -> str:
        """English prompt for Proposers; user-facing strings must be Polish."""
        style_mod = {
            "analytical": "Be precise and data-driven; justify every decision with observable signals.",
            "creative": "Explore non-obvious combinations and alternative paths; propose at least one novel twist.",
            "critical": "Stress-test assumptions and highlight edge cases and single points of failure.",
            "systematic": "Aim for holistic, end-to-end coherence with explicit interfaces between steps.",
        }

        expertise = f"""
    # ROLE: {role.role_name}

    ## YOUR EXPERTISE
    You specialize in: {', '.join(role.expertise_areas)}

    ## THINKING STYLE
    {style_mod.get(role.thinking_style, "Default to clarity and rigor.")}

    {MOAPrompts.UNIVERSAL_PRINCIPLES}

    ## ROLE-SPECIFIC TECHNIQUES
    """
        rl = role.role_name.lower()
        if "causal" in rl:
            expertise += """
    - Causal Reasoning:
      * Identify variables and likely causal relations (confounders, mediators).
      * Prefer testable interventions; annotate assumptions explicitly.
    """
        elif "strategic" in rl:
            expertise += """
    - Strategic Planning:
      * SWOT per component; map critical dependencies and critical path.
      * Prepare 1‚Äì2 realistic what-if branches with measurable triggers.
    """
        elif "creative" in rl:
            expertise += """
    - Creative Expansion:
      * Apply SCAMPER to at least two nodes.
      * Propose 3 alternative micro-approaches and pick one with rationale.
    """
        elif "risk" in rl or "quality" in rl:
            expertise += """
    - Risk/Quality:
      * FMEA table in your head; identify top 3 failure modes and mitigations.
      * Add explicit rollback/notify paths for irrecoverable states.
    """

        return f"""
    {expertise}

    ## MISSION
    {mission}

    ## AVAILABLE NODE LIBRARY
    {MOAPrompts._format_node_library(node_library)}

    ## OUTPUT CONTRACT (ONLY JSON, NO PROSE)
    - Keys in English; user-facing strings in Polish.
    - Use ONLY implementations from the node library.
    - Ensure failure paths exist for critical steps.
    - Keep "thought_process" and justifications concise in Polish.

    Expected JSON structure:
    {{
      "thought_process": ["Krok 1: ...", "Krok 2: ...", "Krok 3: ..."],
      "plan": {{
        "entry_point": "Start_Node_Name",
        "nodes": [
          {{"name": "Load_Data", "implementation": "load_data"}},
          {{"name": "Clean_Data", "implementation": "clean_data"}},
          {{"name": "Validate_Data", "implementation": "validate_data"}}
        ],
        "edges": [
          {{"from": "Load_Data", "to": "Clean_Data", "condition": "on_success"}},
          {{"from": "Load_Data", "to": "Error_Handler", "condition": "on_failure"}}
        ]
      }},
      "confidence": 0.80,
      "key_innovations": ["Innowacja 1", "Innowacja 2"],
      "risk_mitigation": {{"Ryzyko A": "Mitigacja A", "Ryzyko B": "Mitigacja B"}}
    }}
    - Do NOT include code fences or comments.
    - When you write ANY memory (outside this output), save it in concise EN.
"""

    @staticmethod
    def get_aggregator_prompt() -> str:
        """English prompt for the Master Aggregator; output JSON only; user-facing text Polish."""
        return """
    # ROLE: MASTER AGGREGATOR ‚Äî SYNTHESIS & GOVERNANCE

    You merge multiple proposals into a single, coherent, executable plan with strong
    robustness and measurable gates. You remove duplication, resolve conflicts, and
    preserve the best ideas.

    {UNIVERSAL_POLICY}

    ## SYNTHESIS PROTOCOL
    1) Score each proposal on: logical soundness, feasibility, innovation, robustness.
    2) Extract the best subcomponents and compose them (component interfaces must align).
    3) Resolve conflicts by explicit trade-offs; document rationale concisely (Polish).
    4) Guarantee failure paths (on_failure/rollback/notify) for critical nodes.
    5) Prefer measurable conditions (e.g., data_quality > 0.9) where applicable.

    ## META-LEARNING HOOKS
    - If prior successful patterns are known, prefer them; otherwise, annotate assumptions.

    ## OUTPUT CONTRACT (ONLY JSON, NO PROSE)
    - Keys in English; user-facing strings in Polish.
    - Provide a final executable DAG under `final_plan`.
    - Include a brief Polish synthesis rationale and confidence score in [0,1].

    Expected JSON structure:
    {
      "thought_process": ["≈ÅƒÖczƒô elementy X i Y...", "Ujednolicam warunki..."],
      "final_plan": {
        "entry_point": "Load_Data",
        "nodes": [
          {"name": "Load_Data", "implementation": "load_data"},
          {"name": "Clean_Data", "implementation": "clean_data"},
          {"name": "Validate_Data", "implementation": "validate_data"},
          {"name": "Error_Handler", "implementation": "error_handler"},
          {"name": "Rollback_Changes", "implementation": "rollback"},
          {"name": "Generate_Report", "implementation": "generate_report"}
        ],
        "edges": [
          {"from": "Load_Data", "to": "Clean_Data", "condition": "on_success"},
          {"from": "Load_Data", "to": "Error_Handler", "condition": "on_failure"},
          {"from": "Clean_Data", "to": "Validate_Data", "condition": "on_success"}
        ]
      },
      "synthesis_reasoning": "Kr√≥tko po polsku: dlaczego taki uk≈Çad jest najlepszy.",
      "component_sources": {"Causal Analyst": ["Validate_Data"], "Creative Planner": ["Generate_Report"]},
      "confidence_score": 0.90
    }
    - Do NOT include code fences or comments.
    - Any memory writes you perform must be saved in concise English.
    """.replace(
            "{UNIVERSAL_POLICY}", MOAPrompts.UNIVERSAL_PRINCIPLES
        )

    @staticmethod
    def get_critic_prompt() -> str:
        return """
# ROLE: QUALITY CRITIC ‚Äî ADVERSARIAL VALIDATOR

You are the final gate. Stress-test structure, semantics, robustness and compliance
with the mission. If and only if the plan passes, approve it.

{UNIVERSAL_POLICY}

## VALIDATION CHECKLIST
- Structural: valid JSON; required fields present; node names & implementations align with library.
- Semantic: mission alignment; logical flow; dependencies satisfied; measurable conditions preferred.
- Robustness: explicit error paths; rollback and notify; identify SPOFs and mitigations.
- Metrics: compute concise quality metrics; justify scores briefly in Polish.

## DECISION RULE
- APPROVE only if Overall Quality >= threshold you deem reasonable and no critical gaps remain.
- When you APPROVE, set `critique_summary.verdict` to "ZATWIERDZONY" (Polish, uppercase).
- Also include a short Polish justification.

## OUTPUT CONTRACT (ONLY JSON, NO PROSE)
- Keys in English; user-facing strings in Polish.
- If approved, include a complete `final_synthesized_plan` (same schema as proposer/aggregator).
- Optionally include `decision_marker`: "PLAN_ZATWIERDZONY" to facilitate orchestration.

Expected JSON structure:
{
  "critique_summary": {
    "verdict": "ZATWIERDZONY",
    "statement": "Kr√≥tki pow√≥d po polsku.",
    "key_strengths": ["Mocna strona 1", "Mocna strona 2"],
    "identified_weaknesses": [
      {"weakness": "S≈Çabo≈õƒá X", "severity": "Medium", "description": "Dlaczego to problem"}
    ]
  },
  "quality_metrics": {
    "Complexity_Score_C": 3.1,
    "Robustness_Score_R": 50,
    "Innovation_Score_I": 100,
    "Completeness_Score": 100,
    "Overall_Quality_Q": 84.07
  },
  "final_synthesized_plan": {
    "entry_point": "Load_Data",
    "nodes": [
      {"name": "Load_Data", "implementation": "load_data"},
      {"name": "Clean_Data", "implementation": "clean_data"}
    ],
    "edges": [
      {"from": "Load_Data", "to": "Clean_Data", "condition": "on_success"}
    ]
  },
  "decision_marker": "PLAN_ZATWIERDZONY"
}
- Do NOT include code fences or comments.
-In the final response, end with a line containing only PLAN_ZATWIERDZONY.
- Any memory writes you perform must be saved in concise English.
""".replace(
            "{UNIVERSAL_POLICY}", MOAPrompts.UNIVERSAL_PRINCIPLES
        )

    
    
    @staticmethod
    def get_memory_analyst_prompt() -> str:
        """English prompt for the Memory Analyst; output JSON only; internal text can be Polish for notes."""
        return """
# ROLE: MEMORY ANALYST ‚Äî RETRIEVAL & CONDENSATION

You produce a single, concise, strictly-structured summary of relevant prior missions.
You DO NOT design or modify plans. You DO NOT debate. Tool-first mindset.

{UNIVERSAL_POLICY}

## OPERATING MODE (TOOL-FIRST)
1) Query mission memory:
   - Local episodic memory (if available)
   - External Vertex/Discovery index (if configured)
2) Optionally fetch up to 3 representative example plans (IDs + URIs).
3) Deduplicate ideas; compress to short, actionable bullets.
4) Do not copy large chunks; never paste full plans. Summarize only.

## QUALITY BAR
- Be factual; avoid speculation.
- Prefer patterns with proven success/approval signals (if present in memory).
- Include pitfalls only if evidenced across multiple prior cases or clearly applicable.
- Keep it short; this is a pre-brief for other agents, not a plan.

## OUTPUT CONTRACT (ONLY JSON, NO PROSE)
- Keys in English; short, user-facing strings can be Polish.
- Strict schema (no extra keys, no code fences):

{
  "recommended_strategies": [  // up to 6 short bullets, Polish allowed
    "W≈ÇƒÖcz retry z backoff i DLQ",
    "Dodaj rollback dla operacji nieodwracalnych"
  ],
  "common_pitfalls": [         // up to 6 short bullets
    "Brak walidacji schematu (schema drift)",
    "Brak warunk√≥w pomiarowych przy decydowaniu o retrainingu"
  ],
  "examples": [                // up to 3 items
    { "mission_id": "mission_2024_09_01_ab12cd", "plan_uri": "gs://bucket/missions/.../plan.json" }
  ],
  "notes": "‚â§ 50 s≈Ç√≥w: kiedy powy≈ºsze stosowaƒá / granice wa≈ºno≈õci"
}

### RULES
- Return ONLY the JSON object above.
- If no relevant memory is found: return empty arrays and notes="".
- Never propose nodes/edges; never approve/reject plans here.
- Do not include code fences or comments.
""".replace(
            "{UNIVERSAL_POLICY}", MOAPrompts.UNIVERSAL_PRINCIPLES
        )
    
    
    
    @staticmethod
    def _format_node_library(node_library: Dict) -> str:
        """Formatuje bibliotekƒô wƒôz≈Ç√≥w dla promptu"""
        formatted = []
        for name, details in node_library.items():
            formatted.append(f"- {name}: {details.get('description', 'Brak opisu')}")
        return "\n".join(formatted)



--- FILE: models_config.py ---

"""
Definicje struktur danych u≈ºywanych do opisu r√≥l agent√≥w.
"""

from dataclasses import dataclass
from typing import List


@dataclass
class AgentRole:
    """
    Klasa opisujƒÖca rolƒô agenta w systemie multi‚Äëagentowym.

    :param role_name: Nazwa roli (np. "Causal Analyst", "Creative Planner").
    :param expertise_areas: Lista dziedzin, w kt√≥rych agent siƒô specjalizuje.
    :param thinking_style: Styl my≈õlenia ("analytical", "creative", "critical", "systematic" itp.).
    """

    role_name: str
    expertise_areas: List[str]
    thinking_style: str



--- FILE: process_logger.py ---

"""
Prosty logger procesu generowania planu i rozm√≥w miƒôdzy agentami.
Wszystkie komunikaty sƒÖ dopisywane do pliku tekstowego z sygnaturƒÖ czasu.
"""

import sys
from datetime import datetime  # zamiast: import datetime

LOG_FILE = "process_log.txt"
STREAM_STDOUT = True



def log_exception(msg: str, exc: BaseException):
    import traceback
    tb = traceback.TracebackException.from_exception(exc)
    last = tb.stack[-1] if tb and tb.stack else None
    where = f"{last.filename}:{last.lineno} in {last.name}" if last else "unknown location"
    log(f"{msg}: {type(exc).__name__}: {exc} @ {where}")
    # pe≈Çny traceback w kolejnej linii:
    log("".join(tb.format()))



def log(msg: str):
    ts = datetime.now().strftime(
        "%Y-%m-%d %H:%M:%S"
    )  # zamiast: datetime.datetime.now()
    line = f"[{ts}] {msg}"
    try:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(line + "\n")
    except Exception:
        pass
    if STREAM_STDOUT:
        print(line, file=sys.stdout, flush=True)



--- FILE: response_parser.py ---

"""
Inteligentny parser odpowiedzi agent√≥w z auto-korekcjƒÖ
"""

import json
import re
from typing import Dict, Any, Optional
import ast

# Lokalny logger procesu
from process_logger import log as process_log


class ResponseParser:
    """
    Zaawansowany parser kt√≥ry radzi sobie z r√≥≈ºnymi formatami odpowiedzi
    """

    def parse_agent_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        Parsuje odpowied≈∫ agenta pr√≥bujƒÖc r√≥≈ºnych strategii
        """
        if not response:
            return None
        # Zaloguj otrzymanƒÖ odpowied≈∫ (obcinamy do 200 znak√≥w, aby log nie r√≥s≈Ç nadmiernie)
        process_log(f"Received response: {response[:200]}")

        # Strategia 1: Czysty JSON
        parsed = self._try_pure_json(response)
        if parsed:
            process_log(f"Parsed using pure JSON: {parsed}")
            return parsed

        # Strategia 2: JSON z dodatkami (markdown, komentarze)
        parsed = self._try_extract_json(response)
        if parsed:
            process_log(f"Parsed using extract JSON: {parsed}")
            return parsed

        # Strategia 3: Python dict jako string (bez wykonywania kodu)
        parsed = self._try_python_dict(response)
        if parsed:
            process_log(f"Parsed using python-like dict: {parsed}")
            return parsed

        # Strategia 4: Strukturalna ekstrakcja
        parsed = self._try_structural_extraction(response)
        if parsed:
            process_log(f"Parsed using structural extraction: {parsed}")
            return parsed

        # Strategia 5: AI-based repair (u≈ºywa regex i heurystyk)
        parsed = self._try_ai_repair(response)
        if parsed:
            process_log(f"Parsed using AI repair: {parsed}")
            return parsed

        process_log(f"Parse failed: {response[:200]}")
        print(f"‚ö† Nie uda≈Ço siƒô sparsowaƒá odpowiedzi: {response[:100]}...")
        return None

    def _try_pure_json(self, response: str) -> Optional[Dict]:
        """Pr√≥buje parsowaƒá jako czysty JSON"""
        try:
            return json.loads(response.strip())
        except:
            return None

    def _try_extract_json(self, response: str) -> Optional[Dict]:
        """Ekstraktuje JSON z tekstu"""
        # Szukamy JSON w blokach kodu
        json_pattern = r"```(?:json)?\s*(\{.*?\})\s*```"
        match = re.search(json_pattern, response, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass

        # Szukamy pierwszego { i ostatniego }
        start = response.find("{")
        end = response.rfind("}")

        if start != -1 and end != -1 and end > start:
            try:
                return json.loads(response[start : end + 1])
            except:
                pass

        return None

    def _try_python_dict(self, response: str) -> Optional[Dict]:
        """
        Pr√≥buje sparsowaƒá s≈Çownik zapisany w notacji Pythona bez u≈ºycia eval. Wyszukuje
        pierwszƒÖ strukturƒô w nawiasach klamrowych, nastƒôpnie zamienia pojedyncze cudzys≈Çowy
        na podw√≥jne i dodaje cudzys≈Çowy do kluczy, aby u≈ºyƒá json.loads. Je≈õli napotka b≈ÇƒÖd,
        zwraca None.
        """
        try:
            # Wyszukaj fragment przypominajƒÖcy s≈Çownik
            dict_pattern = r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}"
            match = re.search(dict_pattern, response)
            if not match:
                return None
            obj_str = match.group(0)
            # Zamie≈Ñ pojedyncze cudzys≈Çowy na podw√≥jne
            json_like = obj_str.replace("'", '"')
            # Dodaj cudzys≈Çowy do kluczy, je≈õli ich brakuje
            json_like = re.sub(
                r"(?<!\")\b([A-Za-z_][A-Za-z0-9_]*)\b\s*:", r'"\1":', json_like
            )
            return json.loads(json_like)
        except Exception:
            return None

    def _try_structural_extraction(self, response: str) -> Optional[Dict]:
        """Ekstraktuje strukturƒô na podstawie kluczowych s≈Ç√≥w"""
        result = {}

        # Szukamy kluczowych sekcji
        patterns = {
            "thought_process": r"(?:thought_process|thinking|reasoning)[:\s]+([^\n]+(?:\n(?!\w+:)[^\n]+)*)",
            "entry_point": r'(?:entry_point|start)[:\s]+["\']?(\w+)["\']?',
            "confidence": r"(?:confidence|certainty)[:\s]+(\d*\.?\d+)",
            "nodes": r"nodes[:\s]+\[(.*?)\]",
            "edges": r"edges[:\s]+\[(.*?)\]",
        }

        for key, pattern in patterns.items():
            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)
            if match:
                value = match.group(1).strip()

                if key == "confidence":
                    try:
                        result[key] = float(value)
                    except:
                        result[key] = 0.5
                elif key in ["nodes", "edges"]:
                    # Pr√≥buj sparsowaƒá jako listƒô
                    try:
                        result[key] = ast.literal_eval(f"[{value}]")
                    except:
                        result[key] = []
                elif key == "thought_process":
                    # Podziel na kroki
                    steps = [s.strip() for s in value.split("\n") if s.strip()]
                    result[key] = steps
                else:
                    result[key] = value

        return result if result else None

    def _try_ai_repair(self, response: str) -> Optional[Dict]:
        """Pr√≥buje naprawiƒá JSON u≈ºywajƒÖc heurystyk"""
        # Usu≈Ñ komentarze
        response = re.sub(r"//.*?\n", "", response)
        response = re.sub(r"/\*.*?\*/", "", response, flags=re.DOTALL)

        # Napraw typowe b≈Çƒôdy
        repairs = [
            (r",\s*}", "}"),  # Usu≈Ñ trailing commas
            (r",\s*]", "]"),
            (r'"\s*:\s*"([^"]*)"(?=[,}])', r'": "\1"'),  # Napraw cudzys≈Çowy
            (r"(\w+)(?=\s*:)", r'"\1"'),  # Dodaj cudzys≈Çowy do kluczy
            (r':\s*([^",\[\{}\]]+)(?=[,}])', r': "\1"'),  # Dodaj cudzys≈Çowy do warto≈õci
        ]

        for pattern, replacement in repairs:
            response = re.sub(pattern, replacement, response)

        # Spr√≥buj ponownie
        return self._try_pure_json(response)



--- FILE: run_debate.ipynb ---

from autogen_orchestrator import AutoGenMOAOrchestrator
import config_api


import vertexai
vertexai.init(project="dark-data-discovery", location="us-central1")

# Biblioteka wƒôz≈Ç√≥w u≈ºywana do generowania plan√≥w
NODE_LIBRARY = {
    'load_data': {'description': 'Wczytuje dane z r√≥≈ºnych ≈∫r√≥de≈Ç'},
    'clean_data': {'description': 'Czy≈õci dane'},
    'validate_data': {'description': 'Waliduje dane'},
    'discover_causality': {'description': 'Odkrywa relacje przyczynowe (mo≈ºe zawie≈õƒá)'},
    'error_handler': {'description': 'Obs≈Çuguje b≈Çƒôdy'},
    'rollback': {'description': 'Cofa zmiany'},
    'generate_report': {'description': 'Generuje raport'},
    'validate_model': {'description': 'Waliduje model'},
    'optimize_performance': {'description': 'Optymalizuje wydajno≈õƒá'},
    'train_model': {'description': 'Uczy model'},
    'notify_user': {'description': 'Powiadamia u≈ºytkownika'}
}

# Mo≈ºesz podaƒá misjƒô na sta≈Çe albo poprosiƒá u≈ºytkownika o wpisanie
mission = input("Podaj opis misji: ").strip()
if not mission:
    mission = "Stw√≥rz prosty pipeline do analizy danych CSV"

# Inicjalizacja orchestratora z definicjƒÖ misji i ≈õcie≈ºkƒÖ do konfiguracji agent√≥w
orchestrator = AutoGenMOAOrchestrator(
    mission=mission,
    node_library=NODE_LIBRARY,
    config_file="agents_config.json"
)

# Uruchom pe≈ÇnƒÖ debatƒô; wynik to s≈Çownik z finalnym planem lub None
final_plan = orchestrator.run_full_debate_cycle()
orchestrator.reset()
# Wy≈õwietl wynik w czytelnej formie
if final_plan:
    import json
    print("\n‚úÖ Zatwierdzony plan:")
    print(json.dumps(final_plan, indent=2, ensure_ascii=False))
else:
    print("\n‚ùå Nie uda≈Ço siƒô uzyskaƒá zatwierdzonego planu.")
# --- Koniec kom√≥rki ---



--- FILE: structured_response_parser.py ---

"""
Structured parser oparty na Pydantic.  Zamiast heurystycznych pr√≥b parsowania
rƒôcznego, wykorzystuje schematy Pydantic do walidacji odpowiedzi LLM.  Ten
modu≈Ç zastƒôpuje dotychczasowy `response_parser` w nowej konfiguracji.

Model `ProposerResponse` definiuje minimalnƒÖ strukturƒô planu wygenerowanego
przez agent√≥w‚Äëproposer√≥w.  Model `AggregatorResponse` rozszerza go o pole
`final_plan` oraz metadane u≈ºywane przez agregatora.  Model `CriticResponse`
zawiera ocenƒô, listƒô mocnych i s≈Çabych stron oraz ewentualne sugestie
poprawek, zgodnie z za≈Ço≈ºonym formatem JSON.

Je≈õli odpowied≈∫ nie jest poprawnym JSON‚Äëem (np. zawiera `````markdown````
fences) lub nie spe≈Çnia schematu, parser zwraca `None`.
"""

from __future__ import annotations

import json
import re
from typing import List, Optional, Dict, Any
from process_logger import log as process_log
from pydantic import BaseModel, ValidationError, Field


class ProposerPlan(BaseModel):
    """Reprezentuje plan proponowany przez agenta‚Äêproposera."""

    entry_point: str = Field(..., description="Nazwa pierwszego wƒôz≈Ça w planie")
    nodes: List[Dict[str, Any]] = Field(..., description="Lista wƒôz≈Ç√≥w planu")
    edges: List[Dict[str, Any]] = Field(..., description="Lista krawƒôdzi planu")


class ProposerResponse(BaseModel):
    """Struktura odpowiedzi agenta proponujƒÖcego."""

    thought_process: List[str] = Field(..., description="Opis krok√≥w rozumowania")
    plan: ProposerPlan = Field(..., description="Plan w formacie grafu")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Pewno≈õƒá (0‚Äì1)")
    key_innovations: Optional[List[str]] = Field(default_factory=list)
    risk_mitigation: Optional[Dict[str, Any]] = Field(default_factory=dict)


class AggregatorResponse(BaseModel):
    """Struktura odpowiedzi agregatora.  Rozszerza odpowied≈∫ proponera o finalny plan."""

    thought_process: List[str]
    final_plan: ProposerPlan
    synthesis_reasoning: Optional[str]
    component_sources: Optional[Dict[str, Any]]
    confidence_score: Optional[float]
    improvements: Optional[List[str]] = Field(default_factory=list)


class CriticResponse(BaseModel):
    """Struktura odpowiedzi krytyka."""

    approved: bool
    score: float = Field(..., ge=0.0, le=100.0)
    strengths: List[str] = Field(default_factory=list)
    weaknesses: List[str] = Field(default_factory=list)
    feedback: Optional[str]
    improvements: Optional[List[str]] = Field(default_factory=list)


class StructuredResponseParser:
    """
    Parser, kt√≥ry wykorzystuje modele Pydantic do walidacji i konwersji odpowiedzi
    na s≈Çowniki.  Oczekuje, ≈ºe agent zwraca poprawny JSON zgodny z jednym z
    powy≈ºszych schemat√≥w.  Mo≈ºna ≈Çatwo rozszerzyƒá o kolejne typy odpowiedzi.
    """

    def __init__(self) -> None:
        pass

    def _strip_code_fences(self, response: str) -> str:
        """Usuwa bloki kodu (```json ... ```) z odpowiedzi."""
        # Usu≈Ñ bloki ```json ... ``` lub ``` ... ```
        pattern = r"```(?:json)?\s*(\{.*?\})\s*```"
        match = re.search(pattern, response, re.DOTALL)
        if match:
            return match.group(1)
        return response

    def parse_agent_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        Przetwarza odpowied≈∫ agenta i pr√≥buje jƒÖ zmapowaƒá na jeden z
        zdefiniowanych modeli.  Zwraca zserializowanƒÖ postaƒá s≈ÇownikowƒÖ,
        lub None, je≈õli nie mo≈ºna sparsowaƒá.
        """
        if not response:
            return None

        # Usu≈Ñ otaczajƒÖce bloki kodu
        cleaned = self._strip_code_fences(response.strip())

        # Spr√≥buj sparsowaƒá jako JSON
        try:
            data = json.loads(cleaned)
        except Exception:
            return None

        # Kolejno pr√≥buj dopasowaƒá do modeli
        for model_cls in (ProposerResponse, AggregatorResponse, CriticResponse):
            try:
                obj = model_cls.parse_obj(data)
                return obj.dict()
            except ValidationError:
                continue

        # Je≈õli nic nie pasuje, zwr√≥ƒá oryginalne dane
        return data

    def parse_critic_response(self, text: str):
        """
        Parsuje odpowied≈∫ krytyka - zwraca CA≈ÅY JSON
        """
        import json
        import re

        if not text:
            return None

        
        #poprawka
        if isinstance(text, dict):
            process_log("[PARSER] Otrzymano dict - zwracam bez zmian")
            return text
        
        #koniec poprawki
        
        try:
            # Usu≈Ñ markdown code blocks
            clean_text = text.strip()

            # Usu≈Ñ ```json i ```
            clean_text = re.sub(r"```json\s*", "", clean_text)
            clean_text = re.sub(r"```\s*", "", clean_text)

            # Usu≈Ñ PLAN_ZATWIERDZONY z ko≈Ñca
            if "PLAN_ZATWIERDZONY" in clean_text:
                # Znajd≈∫ ostatnie wystƒÖpienie i usu≈Ñ wszystko po nim
                parts = clean_text.rsplit("PLAN_ZATWIERDZONY", 1)
                clean_text = parts[0].strip()

            # Teraz po prostu sparsuj JSON
            result = json.loads(clean_text)

            # Debug - wypisz co znalaz≈Çe≈õ
            process_log(f"[PARSER] Znaleziono klucze: {list(result.keys())}")

            return result

        except json.JSONDecodeError as e:
            process_log(f"[PARSER] JSON decode error: {e}")

            # Plan B - znajd≈∫ JSON manualnie
            try:
                # Znajd≈∫ od pierwszego { do ostatniego }
                start = text.find("{")
                end = text.rfind("}")

                if start >= 0 and end > start:
                    json_str = text[start : end + 1]
                    return json.loads(json_str)
            except:
                pass

        return None



--- FILE: config/models_config.py ---

"""
Definicje struktur danych u≈ºywanych do opisu r√≥l agent√≥w.
"""

from dataclasses import dataclass
from typing import List


@dataclass
class AgentRole:
    """
    Klasa opisujƒÖca rolƒô agenta w systemie multi‚Äëagentowym.

    :param role_name: Nazwa roli (np. "Causal Analyst", "Creative Planner").
    :param expertise_areas: Lista dziedzin, w kt√≥rych agent siƒô specjalizuje.
    :param thinking_style: Styl my≈õlenia ("analytical", "creative", "critical", "systematic" itp.).
    """

    role_name: str
    expertise_areas: List[str]
    thinking_style: str



