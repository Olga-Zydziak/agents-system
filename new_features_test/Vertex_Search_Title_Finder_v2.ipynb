{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b2964d",
   "metadata": {},
   "source": [
    "# 🔎 Vertex AI Search — Title Finder (v2, aware of your data layout)\n",
    "\n",
    "Ta wersja:\n",
    "- Rozumie **strukturę Twojego indeksu** (wg `metadata_20250829.ndjson`).\n",
    "- Potrafi **zmapować linki `gs://...`** z indeksu na **lokalne pliki** rozpakowane w `missions.zip`.\n",
    "- Daje **fuzzy wyszukiwanie po tytule/haśle** (RapidFuzz) + re-ranking, a także szybki podgląd planu/transkryptu/metryk jeśli są dostępne lokalnie.\n",
    "\n",
    "> Jeśli uruchamiasz u siebie: ustaw `GOOGLE_APPLICATION_CREDENTIALS` i podaj `serving_config`, aby faktycznie pytać **Vertex Search**. Gdy chcesz tylko przeglądać lokalne dane, też zadziała (patrz sekcja „Tryb lokalny”).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9497d596",
   "metadata": {},
   "source": [
    "## 0) Instalacja pakietów (jeśli potrzeba)\n",
    "Uruchom jeśli nie masz pakietów:\n",
    "\n",
    "```\n",
    "%pip install -q google-cloud-discoveryengine rapidfuzz pandas ipywidgets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38078b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q google-cloud-discoveryengine rapidfuzz pandas ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0686d42",
   "metadata": {},
   "source": [
    "## 1) Importy + konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee294b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolumny metadanych: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re, json, sys, os\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Opcjonalny fuzzy matcher\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "    _FUZZY = True\n",
    "except Exception:\n",
    "    _FUZZY = False\n",
    "    def _simple_ratio(a: str, b: str) -> float:\n",
    "        a = (a or '').lower(); b = (b or '').lower()\n",
    "        if not a or not b: return 0.0\n",
    "        if a in b or b in a: return 100.0 * min(len(a), len(b))/max(len(a), len(b))\n",
    "        return 0.0\n",
    "    class fuzz:\n",
    "        @staticmethod\n",
    "        def partial_ratio(a,b): return _simple_ratio(a,b)\n",
    "        @staticmethod\n",
    "        def token_set_ratio(a,b): return _simple_ratio(a,b)\n",
    "\n",
    "# Klient Discovery Engine (Vertex Search)\n",
    "try:\n",
    "    from google.cloud import discoveryengine_v1 as de\n",
    "    from google.api_core.client_options import ClientOptions\n",
    "except Exception as e:\n",
    "    de = None\n",
    "    ClientOptions = None\n",
    "    print(\"ℹ️  google-cloud-discoveryengine niedostępny — tryb lokalny nadal działa.\")\n",
    "\n",
    "# Ścieżki do dostarczonych danych\n",
    "META_PATH = Path('/mnt/data/metadata_20250829.ndjson')   # NDJSON z metadanymi\n",
    "MISSIONS_DIR = Path('/mnt/data/missions_extracted')      # Rozpakowany missions.zip\n",
    "\n",
    "# Rozpakuj missions.zip jeśli potrzeba\n",
    "ZIP_PATH = Path('/mnt/data/missions.zip')\n",
    "if not MISSIONS_DIR.exists() and ZIP_PATH.exists():\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
    "        z.extractall(MISSIONS_DIR)\n",
    "    print(f\"📦 Rozpakowano: {ZIP_PATH} -> {MISSIONS_DIR}\")\n",
    "\n",
    "# Wczytaj metadane (lokalnie)\n",
    "if META_PATH.exists():\n",
    "    meta_rows = [json.loads(line) for line in META_PATH.open() if line.strip()]\n",
    "    df_meta = pd.json_normalize(meta_rows)\n",
    "else:\n",
    "    df_meta = pd.DataFrame()\n",
    "\n",
    "print('Kolumny metadanych:', list(df_meta.columns))\n",
    "\n",
    "# Mapowanie gs://... -> lokalny plik (po basename)\n",
    "_local_index = {}\n",
    "if MISSIONS_DIR.exists():\n",
    "    for p in MISSIONS_DIR.rglob('*'):\n",
    "        if p.is_file():\n",
    "            _local_index[p.name] = p\n",
    "\n",
    "def resolve_local_from_uri(uri: Optional[str]) -> Optional[Path]:\n",
    "    if not uri:\n",
    "        return None\n",
    "    base = os.path.basename(uri)\n",
    "    return _local_index.get(base)\n",
    "\n",
    "def infer_api_endpoint(serving_config: str) -> str:\n",
    "    m = re.search(r\"/locations/([^/]+)/\", serving_config)\n",
    "    loc = m.group(1) if m else \"global\"\n",
    "    return f\"{loc}-discoveryengine.googleapis.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a55a8",
   "metadata": {},
   "source": [
    "## 2) Funkcje: Vertex Search + fuzzy re-ranking + lokalne podglądy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "202d7cd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def search_vertex(\n",
    "    serving_config: Optional[str],\n",
    "    query: str,\n",
    "    page_size: int = 30,\n",
    "    api_endpoint: Optional[str] = None,\n",
    "    filter_expression: Optional[str] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Jeśli podasz serving_config i masz klienta DE, pyta Vertex Search.\n",
    "    W przeciwnym razie — tryb lokalny filtrujący df_meta po prostym dopasowaniu query.\"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    if serving_config and de is not None:\n",
    "        endpoint = api_endpoint or infer_api_endpoint(serving_config)\n",
    "        client = de.SearchServiceClient(client_options=ClientOptions(api_endpoint=endpoint))\n",
    "\n",
    "        req = de.SearchRequest(serving_config=serving_config, query=query, page_size=page_size)\n",
    "        # Rozszerzanie zapytań i korekta pisowni\n",
    "        try:\n",
    "            req.query_expansion_spec = de.SearchRequest.QueryExpansionSpec(\n",
    "                condition=de.SearchRequest.QueryExpansionSpec.Condition.AUTO\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            req.spell_correction_spec = de.SearchRequest.SpellCorrectionSpec(\n",
    "                mode=de.SearchRequest.SpellCorrectionSpec.Mode.AUTO\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "        if filter_expression:\n",
    "            req.filter = filter_expression\n",
    "\n",
    "        for r in client.search(request=req):\n",
    "            doc = r.document\n",
    "            struct = dict(doc.struct_data) if doc.struct_data is not None else {}\n",
    "            links = struct.get('links') if isinstance(struct.get('links'), dict) else {}\n",
    "            tags = struct.get('tags', [])\n",
    "            if isinstance(tags, dict): tags = list(tags.values())\n",
    "            elif isinstance(tags, str): tags = [tags]\n",
    "\n",
    "            rows.append({\n",
    "                'engine_score': getattr(r, 'score', None),\n",
    "                'mission_id': struct.get('mission_id'),\n",
    "                'display_id': struct.get('display_id'),\n",
    "                'mission_type': struct.get('mission_type'),\n",
    "                'outcome': struct.get('outcome'),\n",
    "                'tags': tags,\n",
    "                'approved': struct.get('approved'),\n",
    "                'final_score': struct.get('final_score'),\n",
    "                'lang': struct.get('lang'),\n",
    "                'nodes_count': struct.get('nodes_count'),\n",
    "                'edges_count': struct.get('edges_count'),\n",
    "                'plan_uri': links.get('plan_uri'),\n",
    "                'transcript_uri': links.get('transcript_uri'),\n",
    "                'metrics_uri': links.get('metrics_uri'),\n",
    "                'txt_uri': links.get('txt_uri'),\n",
    "                '_struct': struct,\n",
    "            })\n",
    "        return rows\n",
    "\n",
    "    # --- Tryb lokalny: jeśli nie łączymy się z Vertex Search ---\n",
    "    if df_meta.empty:\n",
    "        return rows\n",
    "\n",
    "    # Pola tekstowe do szukania (display_id / mission_id / outcome)\n",
    "    cols = [c for c in df_meta.columns if c in (\n",
    "        'structData.display_id', 'structData.mission_id', 'structData.outcome'\n",
    "    )]\n",
    "    if not cols:\n",
    "        cols = [c for c in df_meta.columns if c.startswith('structData.')]\n",
    "\n",
    "    q = (query or '').strip().lower()\n",
    "    for _, rec in df_meta.iterrows():\n",
    "        text = ' '.join(str(rec.get(c, '')) for c in cols).lower()\n",
    "        if not q or any(tok in text for tok in q.split()):\n",
    "            rows.append({\n",
    "                'engine_score': None,\n",
    "                'mission_id': rec.get('structData.mission_id'),\n",
    "                'display_id': rec.get('structData.display_id'),\n",
    "                'mission_type': rec.get('structData.mission_type'),\n",
    "                'outcome': rec.get('structData.outcome'),\n",
    "                'tags': rec.get('structData.tags'),\n",
    "                'approved': rec.get('structData.approved'),\n",
    "                'final_score': rec.get('structData.final_score'),\n",
    "                'lang': rec.get('structData.lang'),\n",
    "                'nodes_count': rec.get('structData.nodes_count'),\n",
    "                'edges_count': rec.get('structData.edges_count'),\n",
    "                'plan_uri': rec.get('structData.links.plan_uri'),\n",
    "                'transcript_uri': rec.get('structData.links.transcript_uri'),\n",
    "                'metrics_uri': rec.get('structData.links.metrics_uri'),\n",
    "                'txt_uri': rec.get('structData.links.txt_uri'),\n",
    "                '_struct': rec.to_dict(),\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def fuzzy_score(query: str, row: Dict[str, Any]) -> float:\n",
    "    fields = [row.get('display_id') or '', row.get('mission_id') or '', row.get('outcome') or '']\n",
    "    scores = [\n",
    "        fuzz.token_set_ratio(query, fields[0]),\n",
    "        fuzz.partial_ratio(query, fields[0]),\n",
    "        fuzz.token_set_ratio(query, fields[1]),\n",
    "        fuzz.partial_ratio(query, fields[1]),\n",
    "        fuzz.token_set_ratio(query, fields[2]),\n",
    "        fuzz.partial_ratio(query, fields[2]),\n",
    "    ]\n",
    "    return max(float(s or 0.0) for s in scores)\n",
    "\n",
    "def rerank_by_title(query: str, rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    for r in rows:\n",
    "        r['fuzzy_score'] = fuzzy_score(query, r)\n",
    "        es = r.get('engine_score') or 0.0\n",
    "        try: es = float(es)\n",
    "        except Exception: es = 0.0\n",
    "        r['combined_score'] = (0.85 * r['fuzzy_score']) + (0.15 * es)\n",
    "    rows.sort(key=lambda x: (x.get('combined_score', 0.0), x.get('engine_score') or 0.0), reverse=True)\n",
    "    return rows\n",
    "\n",
    "def as_dataframe(rows: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    cols = [\n",
    "        'combined_score','fuzzy_score','engine_score',\n",
    "        'display_id','mission_id','mission_type','outcome',\n",
    "        'tags','approved','final_score','lang',\n",
    "        'nodes_count','edges_count',\n",
    "        'plan_uri','transcript_uri','metrics_uri','txt_uri'\n",
    "    ]\n",
    "    return pd.DataFrame([{k: r.get(k) for k in cols} for r in rows])\n",
    "\n",
    "def load_local_json_from_uri(uri: Optional[str]) -> Optional[Dict[str, Any]]:\n",
    "    p = resolve_local_from_uri(uri)\n",
    "    if not p or not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(p.read_text(encoding='utf-8'))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_local_text_from_uri(uri: Optional[str]) -> Optional[str]:\n",
    "    p = resolve_local_from_uri(uri)\n",
    "    if not p or not p.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return p.read_text(encoding='utf-8')\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782a2f7d-59ab-40aa-9496-f29897f2aa74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 3) Uruchom wyszukiwanie\n",
    "\n",
    "\n",
    "# Parametry:\n",
    "serving_config = \"projects/815755318672/locations/us/collections/default_collection/dataStores/external-memory-connector_1756845276280_gcs_store/servingConfigs/default_config\"  # zostaw puste, by działać lokalnie (na metadata_20250829.ndjson)\n",
    "api_endpoint = None\n",
    "query = \"zbuduj adaptacyjny system ml continuous learning\"  # wpisz przybliżony tytuł/hasło\n",
    "page_size = 30\n",
    "filter_expression = None  # np. 'approved = true AND lang = \"pl\"'\n",
    "\n",
    "rows = search_vertex(serving_config, query, page_size, api_endpoint, filter_expression)\n",
    "if not rows:\n",
    "    print(\"Brak wyników.\")\n",
    "else:\n",
    "    rows = rerank_by_title(query, rows)\n",
    "    df = as_dataframe(rows)\n",
    "    display(df)\n",
    "\n",
    "    # Podgląd best-hita + lokalnych artefaktów\n",
    "    best = rows[0]\n",
    "    print(\"\\n== Najlepsze trafienie ==\")\n",
    "    print(json.dumps({\n",
    "        k: best.get(k) for k in [\n",
    "            \"display_id\",\"mission_id\",\"mission_type\",\"outcome\",\n",
    "            \"tags\",\"final_score\",\"approved\",\"lang\",\n",
    "            \"plan_uri\",\"transcript_uri\",\"metrics_uri\",\"txt_uri\"\n",
    "        ]\n",
    "    }, ensure_ascii=False, indent=2))\n",
    "\n",
    "    plan = load_local_json_from_uri(best.get('plan_uri'))\n",
    "    transcript = load_local_json_from_uri(best.get('transcript_uri'))\n",
    "    metrics = load_local_json_from_uri(best.get('metrics_uri'))\n",
    "    txt = load_local_text_from_uri(best.get('txt_uri'))\n",
    "\n",
    "    if plan:\n",
    "        print(\"\\n== plan.json (local mirror) ==\")\n",
    "        keys = list(plan.keys())\n",
    "        print(f\"Klucze: {keys[:10]} ...  (total: {len(keys)})\")\n",
    "        nn = len(plan.get('nodes', [])) if isinstance(plan.get('nodes'), list) else None\n",
    "        ee = len(plan.get('edges', [])) if isinstance(plan.get('edges'), list) else None\n",
    "        if nn is not None or ee is not None:\n",
    "            print(f\"Nodes: {nn}, Edges: {ee}\")\n",
    "    else:\n",
    "        print(\"\\n(plan.json niedostępny lokalnie lub nieczytelny)\")\n",
    "\n",
    "    if transcript:\n",
    "        print(\"\\n== transcript.json (local mirror) ==\")\n",
    "        tkeys = list(transcript.keys())\n",
    "        print(f\"Klucze: {tkeys[:10]} ... (total: {len(tkeys)})\")\n",
    "    else:\n",
    "        print(\"\\n(transcript.json niedostępny lokalnie lub nieczytelny)\")\n",
    "\n",
    "    if metrics:\n",
    "        print(\"\\n== metrics.json (local mirror) ==\")\n",
    "        mkeys = list(metrics.keys())\n",
    "        print(f\"Klucze: {mkeys[:10]} ... (total: {len(mkeys)})\")\n",
    "    else:\n",
    "        print(\"\\n(metrics.json niedostępny lokalnie lub nieczytelny)\")\n",
    "\n",
    "    if txt:\n",
    "        print(\"\\n== txt (local mirror) ==\")\n",
    "        print(txt[:500] + ('...' if len(txt) > 500 else ''))\n",
    "    else:\n",
    "        print(\"\\n(txt niedostępny lokalnie)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31de7ff",
   "metadata": {},
   "source": [
    "## 4) Eksport wyników (opcjonalnie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39996854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_csv = \"/mnt/data/search_results_v2.csv\"\n",
    "export_jsonl = \"/mnt/data/search_results_v2.jsonl\"\n",
    "\n",
    "if 'df' in globals():\n",
    "    df.to_csv(export_csv, index=False)\n",
    "    with open(export_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "    print(\"Zapisano:\", export_csv, \"i\", export_jsonl)\n",
    "else:\n",
    "    print(\"Najpierw uruchom wyszukiwanie.\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "agents_with_memory_p11",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Agents with memory (Python 3.11)",
   "language": "python",
   "name": "agents_with_memory_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
